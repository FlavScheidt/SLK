Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604112213_1790972638.txt
OK
Time taken: 4.578 seconds
OK
Time taken: 0.08 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.058 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q10_returned_item. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604112213_185913655.txt
OK
Time taken: 2.953 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.392 seconds
OK
Time taken: 0.046 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112213_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112213_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112213_0001
2016-04-11 22:13:41,546 Stage-1 map = 0%,  reduce = 0%
2016-04-11 22:13:50,614 Stage-1 map = 14%,  reduce = 0%
2016-04-11 22:13:53,649 Stage-1 map = 27%,  reduce = 0%
2016-04-11 22:13:55,674 Stage-1 map = 33%,  reduce = 0%
2016-04-11 22:13:56,690 Stage-1 map = 37%,  reduce = 0%
2016-04-11 22:13:58,717 Stage-1 map = 46%,  reduce = 0%
2016-04-11 22:14:01,750 Stage-1 map = 59%,  reduce = 3%
2016-04-11 22:14:04,786 Stage-1 map = 73%,  reduce = 7%
2016-04-11 22:14:07,819 Stage-1 map = 88%,  reduce = 7%
2016-04-11 22:14:10,852 Stage-1 map = 100%,  reduce = 7%
2016-04-11 22:14:13,886 Stage-1 map = 100%,  reduce = 10%
2016-04-11 22:14:16,920 Stage-1 map = 100%,  reduce = 20%
2016-04-11 22:14:19,954 Stage-1 map = 100%,  reduce = 47%
2016-04-11 22:14:22,992 Stage-1 map = 100%,  reduce = 84%
2016-04-11 22:14:26,029 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604112213_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112213_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112213_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112213_0002
2016-04-11 22:14:35,585 Stage-2 map = 0%,  reduce = 0%
2016-04-11 22:14:38,606 Stage-2 map = 33%,  reduce = 0%
2016-04-11 22:14:41,635 Stage-2 map = 67%,  reduce = 0%
2016-04-11 22:14:44,663 Stage-2 map = 100%,  reduce = 0%
2016-04-11 22:14:47,691 Stage-2 map = 100%,  reduce = 33%
2016-04-11 22:14:50,718 Stage-2 map = 100%,  reduce = 76%
2016-04-11 22:14:53,748 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604112213_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112213_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112213_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112213_0003
2016-04-11 22:15:02,429 Stage-3 map = 0%,  reduce = 0%
2016-04-11 22:15:11,481 Stage-3 map = 6%,  reduce = 0%
2016-04-11 22:15:14,501 Stage-3 map = 12%,  reduce = 0%
2016-04-11 22:15:17,521 Stage-3 map = 18%,  reduce = 0%
2016-04-11 22:15:20,559 Stage-3 map = 20%,  reduce = 0%
2016-04-11 22:15:23,587 Stage-3 map = 22%,  reduce = 0%
2016-04-11 22:15:26,616 Stage-3 map = 24%,  reduce = 0%
2016-04-11 22:15:29,645 Stage-3 map = 26%,  reduce = 1%
2016-04-11 22:15:32,677 Stage-3 map = 28%,  reduce = 2%
2016-04-11 22:15:35,708 Stage-3 map = 30%,  reduce = 2%
2016-04-11 22:15:38,739 Stage-3 map = 32%,  reduce = 3%
2016-04-11 22:15:41,773 Stage-3 map = 35%,  reduce = 4%
2016-04-11 22:15:44,800 Stage-3 map = 39%,  reduce = 4%
2016-04-11 22:15:47,830 Stage-3 map = 42%,  reduce = 4%
2016-04-11 22:15:49,847 Stage-3 map = 45%,  reduce = 4%
2016-04-11 22:15:52,874 Stage-3 map = 48%,  reduce = 4%
2016-04-11 22:15:55,897 Stage-3 map = 50%,  reduce = 4%
2016-04-11 22:15:58,921 Stage-3 map = 51%,  reduce = 5%
2016-04-11 22:16:01,945 Stage-3 map = 54%,  reduce = 7%
2016-04-11 22:16:04,968 Stage-3 map = 55%,  reduce = 7%
2016-04-11 22:16:07,991 Stage-3 map = 58%,  reduce = 8%
2016-04-11 22:16:11,014 Stage-3 map = 62%,  reduce = 8%
2016-04-11 22:16:14,038 Stage-3 map = 66%,  reduce = 8%
2016-04-11 22:16:17,060 Stage-3 map = 69%,  reduce = 8%
2016-04-11 22:16:20,085 Stage-3 map = 73%,  reduce = 8%
2016-04-11 22:16:23,106 Stage-3 map = 75%,  reduce = 9%
2016-04-11 22:16:26,128 Stage-3 map = 76%,  reduce = 9%
2016-04-11 22:16:29,149 Stage-3 map = 77%,  reduce = 11%
2016-04-11 22:16:32,170 Stage-3 map = 79%,  reduce = 12%
2016-04-11 22:16:35,192 Stage-3 map = 83%,  reduce = 12%
2016-04-11 22:16:38,213 Stage-3 map = 86%,  reduce = 12%
2016-04-11 22:16:41,234 Stage-3 map = 91%,  reduce = 12%
2016-04-11 22:16:44,255 Stage-3 map = 94%,  reduce = 12%
2016-04-11 22:16:47,276 Stage-3 map = 97%,  reduce = 13%
2016-04-11 22:16:50,298 Stage-3 map = 100%,  reduce = 13%
2016-04-11 22:16:53,320 Stage-3 map = 100%,  reduce = 14%
2016-04-11 22:17:02,380 Stage-3 map = 100%,  reduce = 26%
2016-04-11 22:17:05,401 Stage-3 map = 100%,  reduce = 40%
2016-04-11 22:17:08,423 Stage-3 map = 100%,  reduce = 47%
2016-04-11 22:17:11,445 Stage-3 map = 100%,  reduce = 50%
2016-04-11 22:17:17,486 Stage-3 map = 100%,  reduce = 58%
2016-04-11 22:17:20,509 Stage-3 map = 100%,  reduce = 75%
2016-04-11 22:17:23,530 Stage-3 map = 100%,  reduce = 77%
2016-04-11 22:17:26,553 Stage-3 map = 100%,  reduce = 92%
2016-04-11 22:17:29,577 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604112213_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112213_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112213_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112213_0004
2016-04-11 22:17:38,045 Stage-4 map = 0%,  reduce = 0%
2016-04-11 22:17:47,090 Stage-4 map = 50%,  reduce = 0%
2016-04-11 22:17:56,140 Stage-4 map = 100%,  reduce = 17%
2016-04-11 22:18:05,190 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604112213_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112213_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112213_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112213_0005
2016-04-11 22:18:14,624 Stage-5 map = 0%,  reduce = 0%
2016-04-11 22:18:20,654 Stage-5 map = 100%,  reduce = 0%
2016-04-11 22:18:29,703 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604112213_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112213_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112213_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112213_0006
2016-04-11 22:18:38,188 Stage-6 map = 0%,  reduce = 0%
2016-04-11 22:18:41,204 Stage-6 map = 100%,  reduce = 0%
2016-04-11 22:18:50,256 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604112213_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 320.664 seconds
