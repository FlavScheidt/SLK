Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604112330_920204395.txt
OK
Time taken: 3.001 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.42 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.042 seconds
FAILED: Hive Internal Error: java.lang.RuntimeException(org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /home/hadoop/tmp/hive-hadoop/hive_2016-04-11_23-30-18_350_1826629208516296976. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:1761)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:1735)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.mkdirs(NameNode.java:542)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /home/hadoop/tmp/hive-hadoop/hive_2016-04-11_23-30-18_350_1826629208516296976. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:1761)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:1735)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.mkdirs(NameNode.java:542)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)

	at org.apache.hadoop.hive.ql.Context.getExternalScratchDir(Context.java:195)
	at org.apache.hadoop.hive.ql.Context.getExternalTmpFileURI(Context.java:312)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:3239)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:5077)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:5547)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:6100)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:125)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:304)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:379)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:272)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /home/hadoop/tmp/hive-hadoop/hive_2016-04-11_23-30-18_350_1826629208516296976. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:1761)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:1735)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.mkdirs(NameNode.java:542)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)

	at org.apache.hadoop.ipc.Client.call(Client.java:740)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at com.sun.proxy.$Proxy2.mkdirs(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
	at com.sun.proxy.$Proxy2.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:912)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:262)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1120)
	at org.apache.hadoop.hive.ql.Context.makeExternalScratchDir(Context.java:155)
	at org.apache.hadoop.hive.ql.Context.getExternalScratchDir(Context.java:190)
	... 17 more

Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604112330_1788561706.txt
OK
Time taken: 3.681 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.147 seconds
OK
Time taken: 0.356 seconds
OK
Time taken: 0.082 seconds
OK
Time taken: 0.061 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.049 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112330_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112330_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112330_0001
2016-04-11 23:30:29,882 Stage-1 map = 0%,  reduce = 0%
2016-04-11 23:30:38,948 Stage-1 map = 7%,  reduce = 0%
2016-04-11 23:30:41,990 Stage-1 map = 26%,  reduce = 0%
2016-04-11 23:30:45,025 Stage-1 map = 45%,  reduce = 0%
2016-04-11 23:30:48,058 Stage-1 map = 53%,  reduce = 0%
2016-04-11 23:30:51,091 Stage-1 map = 62%,  reduce = 0%
2016-04-11 23:30:54,124 Stage-1 map = 73%,  reduce = 7%
2016-04-11 23:30:57,158 Stage-1 map = 82%,  reduce = 7%
2016-04-11 23:31:00,191 Stage-1 map = 90%,  reduce = 7%
2016-04-11 23:31:03,225 Stage-1 map = 97%,  reduce = 7%
2016-04-11 23:31:06,260 Stage-1 map = 100%,  reduce = 7%
2016-04-11 23:31:09,293 Stage-1 map = 100%,  reduce = 20%
2016-04-11 23:31:15,357 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604112330_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112330_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112330_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112330_0002
2016-04-11 23:31:23,919 Stage-2 map = 0%,  reduce = 0%
2016-04-11 23:31:26,943 Stage-2 map = 33%,  reduce = 0%
2016-04-11 23:31:29,972 Stage-2 map = 67%,  reduce = 0%
2016-04-11 23:31:33,000 Stage-2 map = 100%,  reduce = 0%
2016-04-11 23:31:36,028 Stage-2 map = 100%,  reduce = 33%
2016-04-11 23:31:39,056 Stage-2 map = 100%,  reduce = 78%
2016-04-11 23:31:42,086 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604112330_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112330_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112330_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112330_0003
2016-04-11 23:31:50,790 Stage-3 map = 0%,  reduce = 0%
2016-04-11 23:31:59,842 Stage-3 map = 6%,  reduce = 0%
2016-04-11 23:32:02,861 Stage-3 map = 11%,  reduce = 0%
2016-04-11 23:32:05,880 Stage-3 map = 17%,  reduce = 0%
2016-04-11 23:32:08,902 Stage-3 map = 20%,  reduce = 0%
2016-04-11 23:32:11,931 Stage-3 map = 22%,  reduce = 0%
2016-04-11 23:32:14,959 Stage-3 map = 23%,  reduce = 0%
2016-04-11 23:32:17,989 Stage-3 map = 26%,  reduce = 1%
2016-04-11 23:32:21,018 Stage-3 map = 28%,  reduce = 2%
2016-04-11 23:32:24,046 Stage-3 map = 30%,  reduce = 2%
2016-04-11 23:32:27,075 Stage-3 map = 32%,  reduce = 3%
2016-04-11 23:32:30,105 Stage-3 map = 36%,  reduce = 4%
2016-04-11 23:32:33,133 Stage-3 map = 39%,  reduce = 4%
2016-04-11 23:32:36,160 Stage-3 map = 43%,  reduce = 4%
2016-04-11 23:32:39,187 Stage-3 map = 45%,  reduce = 4%
2016-04-11 23:32:42,211 Stage-3 map = 48%,  reduce = 4%
2016-04-11 23:32:45,235 Stage-3 map = 49%,  reduce = 5%
2016-04-11 23:32:48,260 Stage-3 map = 52%,  reduce = 5%
2016-04-11 23:32:51,284 Stage-3 map = 54%,  reduce = 7%
2016-04-11 23:32:54,307 Stage-3 map = 57%,  reduce = 7%
2016-04-11 23:32:57,331 Stage-3 map = 59%,  reduce = 7%
2016-04-11 23:33:00,352 Stage-3 map = 63%,  reduce = 8%
2016-04-11 23:33:03,374 Stage-3 map = 67%,  reduce = 8%
2016-04-11 23:33:06,395 Stage-3 map = 70%,  reduce = 8%
2016-04-11 23:33:09,416 Stage-3 map = 73%,  reduce = 8%
2016-04-11 23:33:12,437 Stage-3 map = 74%,  reduce = 9%
2016-04-11 23:33:15,457 Stage-3 map = 77%,  reduce = 10%
2016-04-11 23:33:18,477 Stage-3 map = 79%,  reduce = 10%
2016-04-11 23:33:21,498 Stage-3 map = 82%,  reduce = 12%
2016-04-11 23:33:24,519 Stage-3 map = 84%,  reduce = 12%
2016-04-11 23:33:27,543 Stage-3 map = 88%,  reduce = 12%
2016-04-11 23:33:30,563 Stage-3 map = 92%,  reduce = 12%
2016-04-11 23:33:33,583 Stage-3 map = 95%,  reduce = 12%
2016-04-11 23:33:36,605 Stage-3 map = 98%,  reduce = 14%
2016-04-11 23:33:39,625 Stage-3 map = 100%,  reduce = 15%
2016-04-11 23:33:48,680 Stage-3 map = 100%,  reduce = 24%
2016-04-11 23:33:51,701 Stage-3 map = 100%,  reduce = 37%
2016-04-11 23:33:54,723 Stage-3 map = 100%,  reduce = 44%
2016-04-11 23:33:57,744 Stage-3 map = 100%,  reduce = 50%
2016-04-11 23:34:05,797 Stage-3 map = 100%,  reduce = 58%
2016-04-11 23:34:08,821 Stage-3 map = 100%,  reduce = 74%
2016-04-11 23:34:11,843 Stage-3 map = 100%,  reduce = 83%
2016-04-11 23:34:14,865 Stage-3 map = 100%,  reduce = 93%
2016-04-11 23:34:17,888 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604112330_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112330_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112330_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112330_0004
2016-04-11 23:34:27,348 Stage-4 map = 0%,  reduce = 0%
2016-04-11 23:34:36,390 Stage-4 map = 50%,  reduce = 0%
2016-04-11 23:34:45,438 Stage-4 map = 100%,  reduce = 17%
2016-04-11 23:34:54,488 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604112330_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112330_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112330_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112330_0005
2016-04-11 23:35:02,942 Stage-5 map = 0%,  reduce = 0%
2016-04-11 23:35:08,982 Stage-5 map = 100%,  reduce = 0%
2016-04-11 23:35:18,029 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604112330_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604112330_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604112330_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604112330_0006
2016-04-11 23:35:27,900 Stage-6 map = 0%,  reduce = 0%
2016-04-11 23:35:29,913 Stage-6 map = 100%,  reduce = 0%
2016-04-11 23:35:38,963 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604112330_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 317.595 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604112335_845012121.txt
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604112335_1717622716.txt
