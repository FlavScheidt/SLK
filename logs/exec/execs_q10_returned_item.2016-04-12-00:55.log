Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604120043_894635246.txt
OK
Time taken: 3.417 seconds
OK
Time taken: 0.089 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.067 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q10_returned_item. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604120043_1366875642.txt
OK
Time taken: 3.018 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.435 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120043_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120043_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120043_0001
2016-04-12 00:43:58,938 Stage-1 map = 0%,  reduce = 0%
2016-04-12 00:44:06,997 Stage-1 map = 6%,  reduce = 0%
2016-04-12 00:44:10,022 Stage-1 map = 13%,  reduce = 0%
2016-04-12 00:44:13,047 Stage-1 map = 31%,  reduce = 0%
2016-04-12 00:44:16,082 Stage-1 map = 54%,  reduce = 0%
2016-04-12 00:44:19,127 Stage-1 map = 65%,  reduce = 0%
2016-04-12 00:44:22,162 Stage-1 map = 74%,  reduce = 0%
2016-04-12 00:44:25,195 Stage-1 map = 77%,  reduce = 0%
2016-04-12 00:44:28,228 Stage-1 map = 83%,  reduce = 20%
2016-04-12 00:44:31,261 Stage-1 map = 87%,  reduce = 20%
2016-04-12 00:44:34,297 Stage-1 map = 95%,  reduce = 20%
2016-04-12 00:44:37,332 Stage-1 map = 100%,  reduce = 20%
2016-04-12 00:44:43,393 Stage-1 map = 100%,  reduce = 27%
2016-04-12 00:44:49,454 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604120043_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120043_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120043_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120043_0002
2016-04-12 00:44:58,204 Stage-2 map = 0%,  reduce = 0%
2016-04-12 00:45:01,228 Stage-2 map = 33%,  reduce = 0%
2016-04-12 00:45:07,277 Stage-2 map = 96%,  reduce = 0%
2016-04-12 00:45:10,305 Stage-2 map = 100%,  reduce = 11%
2016-04-12 00:45:19,378 Stage-2 map = 100%,  reduce = 75%
2016-04-12 00:45:22,406 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604120043_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120043_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120043_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120043_0003
2016-04-12 00:45:32,060 Stage-3 map = 0%,  reduce = 0%
2016-04-12 00:45:40,106 Stage-3 map = 2%,  reduce = 0%
2016-04-12 00:45:41,115 Stage-3 map = 7%,  reduce = 0%
2016-04-12 00:45:43,129 Stage-3 map = 8%,  reduce = 0%
2016-04-12 00:45:44,152 Stage-3 map = 12%,  reduce = 0%
2016-04-12 00:45:46,166 Stage-3 map = 17%,  reduce = 0%
2016-04-12 00:45:49,189 Stage-3 map = 20%,  reduce = 0%
2016-04-12 00:45:52,216 Stage-3 map = 22%,  reduce = 0%
2016-04-12 00:45:55,246 Stage-3 map = 24%,  reduce = 0%
2016-04-12 00:45:58,276 Stage-3 map = 25%,  reduce = 0%
2016-04-12 00:46:01,306 Stage-3 map = 26%,  reduce = 1%
2016-04-12 00:46:04,337 Stage-3 map = 27%,  reduce = 3%
2016-04-12 00:46:07,366 Stage-3 map = 29%,  reduce = 3%
2016-04-12 00:46:10,390 Stage-3 map = 32%,  reduce = 4%
2016-04-12 00:46:13,413 Stage-3 map = 36%,  reduce = 4%
2016-04-12 00:46:16,437 Stage-3 map = 39%,  reduce = 4%
2016-04-12 00:46:19,460 Stage-3 map = 43%,  reduce = 4%
2016-04-12 00:46:22,483 Stage-3 map = 46%,  reduce = 4%
2016-04-12 00:46:25,506 Stage-3 map = 48%,  reduce = 4%
2016-04-12 00:46:28,530 Stage-3 map = 50%,  reduce = 5%
2016-04-12 00:46:34,572 Stage-3 map = 51%,  reduce = 7%
2016-04-12 00:46:37,593 Stage-3 map = 53%,  reduce = 7%
2016-04-12 00:46:40,616 Stage-3 map = 56%,  reduce = 8%
2016-04-12 00:46:43,637 Stage-3 map = 60%,  reduce = 8%
2016-04-12 00:46:46,658 Stage-3 map = 64%,  reduce = 8%
2016-04-12 00:46:49,679 Stage-3 map = 67%,  reduce = 8%
2016-04-12 00:46:52,701 Stage-3 map = 71%,  reduce = 8%
2016-04-12 00:46:55,723 Stage-3 map = 73%,  reduce = 8%
2016-04-12 00:46:58,744 Stage-3 map = 75%,  reduce = 9%
2016-04-12 00:47:04,783 Stage-3 map = 77%,  reduce = 10%
2016-04-12 00:47:07,804 Stage-3 map = 79%,  reduce = 11%
2016-04-12 00:47:10,825 Stage-3 map = 82%,  reduce = 12%
2016-04-12 00:47:13,845 Stage-3 map = 85%,  reduce = 13%
2016-04-12 00:47:16,865 Stage-3 map = 89%,  reduce = 13%
2016-04-12 00:47:19,885 Stage-3 map = 94%,  reduce = 13%
2016-04-12 00:47:22,905 Stage-3 map = 97%,  reduce = 13%
2016-04-12 00:47:25,926 Stage-3 map = 99%,  reduce = 13%
2016-04-12 00:47:28,946 Stage-3 map = 100%,  reduce = 13%
2016-04-12 00:47:31,966 Stage-3 map = 100%,  reduce = 14%
2016-04-12 00:47:34,986 Stage-3 map = 100%,  reduce = 15%
2016-04-12 00:47:41,022 Stage-3 map = 100%,  reduce = 26%
2016-04-12 00:47:44,043 Stage-3 map = 100%,  reduce = 34%
2016-04-12 00:47:47,063 Stage-3 map = 100%,  reduce = 45%
2016-04-12 00:47:50,085 Stage-3 map = 100%,  reduce = 50%
2016-04-12 00:47:56,125 Stage-3 map = 100%,  reduce = 58%
2016-04-12 00:47:59,146 Stage-3 map = 100%,  reduce = 71%
2016-04-12 00:48:02,167 Stage-3 map = 100%,  reduce = 77%
2016-04-12 00:48:05,189 Stage-3 map = 100%,  reduce = 91%
2016-04-12 00:48:08,210 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604120043_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120043_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120043_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120043_0004
2016-04-12 00:48:16,679 Stage-4 map = 0%,  reduce = 0%
2016-04-12 00:48:25,722 Stage-4 map = 50%,  reduce = 0%
2016-04-12 00:48:34,775 Stage-4 map = 100%,  reduce = 17%
2016-04-12 00:48:43,825 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604120043_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120043_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120043_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120043_0005
2016-04-12 00:48:53,246 Stage-5 map = 0%,  reduce = 0%
2016-04-12 00:48:59,285 Stage-5 map = 100%,  reduce = 0%
2016-04-12 00:49:08,333 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604120043_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120043_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120043_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120043_0006
2016-04-12 00:49:16,779 Stage-6 map = 0%,  reduce = 0%
2016-04-12 00:49:19,795 Stage-6 map = 100%,  reduce = 0%
2016-04-12 00:49:28,845 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604120043_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 339.854 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604120049_2013136863.txt
OK
Time taken: 3.515 seconds
OK
Time taken: 0.096 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.066 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q10_returned_item. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604120049_1067541165.txt
OK
Time taken: 2.877 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.436 seconds
OK
Time taken: 0.056 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120049_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120049_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120049_0001
2016-04-12 00:50:08,304 Stage-1 map = 0%,  reduce = 0%
2016-04-12 00:50:17,378 Stage-1 map = 15%,  reduce = 0%
2016-04-12 00:50:20,417 Stage-1 map = 28%,  reduce = 0%
2016-04-12 00:50:23,453 Stage-1 map = 36%,  reduce = 0%
2016-04-12 00:50:26,487 Stage-1 map = 46%,  reduce = 0%
2016-04-12 00:50:29,519 Stage-1 map = 55%,  reduce = 0%
2016-04-12 00:50:32,553 Stage-1 map = 66%,  reduce = 7%
2016-04-12 00:50:35,586 Stage-1 map = 75%,  reduce = 7%
2016-04-12 00:50:38,619 Stage-1 map = 88%,  reduce = 7%
2016-04-12 00:50:41,652 Stage-1 map = 93%,  reduce = 7%
2016-04-12 00:50:44,685 Stage-1 map = 100%,  reduce = 7%
2016-04-12 00:50:47,717 Stage-1 map = 100%,  reduce = 17%
2016-04-12 00:50:53,783 Stage-1 map = 100%,  reduce = 84%
2016-04-12 00:50:56,818 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604120049_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120049_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120049_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120049_0002
2016-04-12 00:51:05,383 Stage-2 map = 0%,  reduce = 0%
2016-04-12 00:51:08,407 Stage-2 map = 33%,  reduce = 0%
2016-04-12 00:51:14,456 Stage-2 map = 96%,  reduce = 0%
2016-04-12 00:51:17,483 Stage-2 map = 100%,  reduce = 11%
2016-04-12 00:51:26,556 Stage-2 map = 100%,  reduce = 76%
2016-04-12 00:51:29,584 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604120049_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120049_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120049_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120049_0003
2016-04-12 00:51:38,252 Stage-3 map = 0%,  reduce = 0%
2016-04-12 00:51:47,303 Stage-3 map = 7%,  reduce = 0%
2016-04-12 00:51:50,323 Stage-3 map = 12%,  reduce = 0%
2016-04-12 00:51:53,343 Stage-3 map = 17%,  reduce = 0%
2016-04-12 00:51:56,380 Stage-3 map = 19%,  reduce = 0%
2016-04-12 00:51:59,407 Stage-3 map = 22%,  reduce = 0%
2016-04-12 00:52:02,435 Stage-3 map = 24%,  reduce = 0%
2016-04-12 00:52:05,466 Stage-3 map = 25%,  reduce = 0%
2016-04-12 00:52:08,496 Stage-3 map = 26%,  reduce = 1%
2016-04-12 00:52:11,527 Stage-3 map = 28%,  reduce = 3%
2016-04-12 00:52:14,560 Stage-3 map = 29%,  reduce = 3%
2016-04-12 00:52:17,583 Stage-3 map = 33%,  reduce = 4%
2016-04-12 00:52:20,606 Stage-3 map = 36%,  reduce = 4%
2016-04-12 00:52:23,629 Stage-3 map = 39%,  reduce = 4%
2016-04-12 00:52:26,652 Stage-3 map = 44%,  reduce = 4%
2016-04-12 00:52:29,675 Stage-3 map = 46%,  reduce = 4%
2016-04-12 00:52:32,698 Stage-3 map = 48%,  reduce = 4%
2016-04-12 00:52:35,721 Stage-3 map = 50%,  reduce = 4%
2016-04-12 00:52:38,744 Stage-3 map = 50%,  reduce = 6%
2016-04-12 00:52:41,765 Stage-3 map = 52%,  reduce = 7%
2016-04-12 00:52:44,787 Stage-3 map = 53%,  reduce = 7%
2016-04-12 00:52:47,809 Stage-3 map = 56%,  reduce = 8%
2016-04-12 00:52:50,830 Stage-3 map = 60%,  reduce = 8%
2016-04-12 00:52:53,851 Stage-3 map = 64%,  reduce = 8%
2016-04-12 00:52:56,871 Stage-3 map = 67%,  reduce = 8%
2016-04-12 00:52:59,892 Stage-3 map = 71%,  reduce = 8%
2016-04-12 00:53:02,914 Stage-3 map = 73%,  reduce = 8%
2016-04-12 00:53:05,936 Stage-3 map = 75%,  reduce = 8%
2016-04-12 00:53:08,956 Stage-3 map = 75%,  reduce = 9%
2016-04-12 00:53:11,978 Stage-3 map = 76%,  reduce = 11%
2016-04-12 00:53:14,999 Stage-3 map = 77%,  reduce = 11%
2016-04-12 00:53:18,019 Stage-3 map = 80%,  reduce = 13%
2016-04-12 00:53:21,038 Stage-3 map = 84%,  reduce = 13%
2016-04-12 00:53:24,058 Stage-3 map = 88%,  reduce = 13%
2016-04-12 00:53:27,077 Stage-3 map = 92%,  reduce = 13%
2016-04-12 00:53:30,097 Stage-3 map = 96%,  reduce = 13%
2016-04-12 00:53:33,117 Stage-3 map = 98%,  reduce = 13%
2016-04-12 00:53:36,137 Stage-3 map = 100%,  reduce = 13%
2016-04-12 00:53:45,190 Stage-3 map = 100%,  reduce = 21%
2016-04-12 00:53:48,211 Stage-3 map = 100%,  reduce = 31%
2016-04-12 00:53:50,226 Stage-3 map = 100%,  reduce = 34%
2016-04-12 00:53:51,236 Stage-3 map = 100%,  reduce = 40%
2016-04-12 00:53:53,250 Stage-3 map = 100%,  reduce = 43%
2016-04-12 00:53:54,261 Stage-3 map = 100%,  reduce = 48%
2016-04-12 00:53:57,283 Stage-3 map = 100%,  reduce = 50%
2016-04-12 00:53:59,298 Stage-3 map = 100%,  reduce = 54%
2016-04-12 00:54:02,318 Stage-3 map = 100%,  reduce = 59%
2016-04-12 00:54:03,327 Stage-3 map = 100%,  reduce = 62%
2016-04-12 00:54:05,343 Stage-3 map = 100%,  reduce = 70%
2016-04-12 00:54:06,353 Stage-3 map = 100%,  reduce = 75%
2016-04-12 00:54:08,369 Stage-3 map = 100%,  reduce = 79%
2016-04-12 00:54:09,379 Stage-3 map = 100%,  reduce = 88%
2016-04-12 00:54:11,394 Stage-3 map = 100%,  reduce = 91%
2016-04-12 00:54:12,404 Stage-3 map = 100%,  reduce = 94%
2016-04-12 00:54:14,419 Stage-3 map = 100%,  reduce = 95%
2016-04-12 00:54:15,429 Stage-3 map = 100%,  reduce = 99%
2016-04-12 00:54:18,450 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604120049_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120049_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120049_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120049_0004
2016-04-12 00:54:26,916 Stage-4 map = 0%,  reduce = 0%
2016-04-12 00:54:32,946 Stage-4 map = 13%,  reduce = 0%
2016-04-12 00:54:35,965 Stage-4 map = 50%,  reduce = 0%
2016-04-12 00:54:38,983 Stage-4 map = 63%,  reduce = 0%
2016-04-12 00:54:42,001 Stage-4 map = 75%,  reduce = 17%
2016-04-12 00:54:45,019 Stage-4 map = 100%,  reduce = 17%
2016-04-12 00:54:51,052 Stage-4 map = 100%,  reduce = 25%
2016-04-12 00:54:57,087 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604120049_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120049_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120049_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120049_0005
2016-04-12 00:55:05,544 Stage-5 map = 0%,  reduce = 0%
2016-04-12 00:55:11,573 Stage-5 map = 100%,  reduce = 0%
2016-04-12 00:55:20,621 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604120049_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120049_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120049_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120049_0006
2016-04-12 00:55:30,037 Stage-6 map = 0%,  reduce = 0%
2016-04-12 00:55:33,053 Stage-6 map = 100%,  reduce = 0%
2016-04-12 00:55:42,103 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604120049_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 343.319 seconds
