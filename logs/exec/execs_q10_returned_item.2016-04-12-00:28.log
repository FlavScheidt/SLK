Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604120022_156431579.txt
OK
Time taken: 3.733 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.132 seconds
OK
Time taken: 0.169 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.035 seconds
OK
Time taken: 0.041 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/hive/warehouse/q10_returned_item. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 20 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:1761)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:1735)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.mkdirs(NameNode.java:542)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604120022_630299964.txt
OK
Time taken: 3.501 seconds
OK
Time taken: 0.105 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.074 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.232 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120022_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120022_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120022_0001
2016-04-12 00:22:32,376 Stage-1 map = 0%,  reduce = 0%
2016-04-12 00:22:41,441 Stage-1 map = 6%,  reduce = 0%
2016-04-12 00:22:44,466 Stage-1 map = 13%,  reduce = 0%
2016-04-12 00:22:47,491 Stage-1 map = 32%,  reduce = 0%
2016-04-12 00:22:50,528 Stage-1 map = 50%,  reduce = 0%
2016-04-12 00:22:53,560 Stage-1 map = 61%,  reduce = 0%
2016-04-12 00:22:56,595 Stage-1 map = 70%,  reduce = 0%
2016-04-12 00:22:59,629 Stage-1 map = 78%,  reduce = 3%
2016-04-12 00:23:02,663 Stage-1 map = 80%,  reduce = 3%
2016-04-12 00:23:08,725 Stage-1 map = 82%,  reduce = 13%
2016-04-12 00:23:11,758 Stage-1 map = 88%,  reduce = 13%
2016-04-12 00:23:13,782 Stage-1 map = 88%,  reduce = 23%
2016-04-12 00:23:14,797 Stage-1 map = 92%,  reduce = 23%
2016-04-12 00:23:17,831 Stage-1 map = 100%,  reduce = 27%
2016-04-12 00:23:19,854 Stage-1 map = 100%,  reduce = 30%
2016-04-12 00:23:22,886 Stage-1 map = 100%,  reduce = 50%
2016-04-12 00:23:25,923 Stage-1 map = 100%,  reduce = 63%
2016-04-12 00:23:28,959 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604120022_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120022_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120022_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120022_0002
2016-04-12 00:23:37,787 Stage-2 map = 0%,  reduce = 0%
2016-04-12 00:23:41,817 Stage-2 map = 33%,  reduce = 0%
2016-04-12 00:23:46,858 Stage-2 map = 95%,  reduce = 0%
2016-04-12 00:23:49,887 Stage-2 map = 100%,  reduce = 0%
2016-04-12 00:23:50,900 Stage-2 map = 100%,  reduce = 11%
2016-04-12 00:23:59,980 Stage-2 map = 100%,  reduce = 76%
2016-04-12 00:24:02,003 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604120022_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120022_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120022_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120022_0003
2016-04-12 00:24:11,138 Stage-3 map = 0%,  reduce = 0%
2016-04-12 00:24:20,191 Stage-3 map = 7%,  reduce = 0%
2016-04-12 00:24:23,210 Stage-3 map = 12%,  reduce = 0%
2016-04-12 00:24:26,228 Stage-3 map = 17%,  reduce = 0%
2016-04-12 00:24:29,251 Stage-3 map = 20%,  reduce = 0%
2016-04-12 00:24:32,278 Stage-3 map = 22%,  reduce = 0%
2016-04-12 00:24:35,304 Stage-3 map = 24%,  reduce = 0%
2016-04-12 00:24:38,331 Stage-3 map = 25%,  reduce = 0%
2016-04-12 00:24:41,358 Stage-3 map = 26%,  reduce = 1%
2016-04-12 00:24:44,382 Stage-3 map = 28%,  reduce = 3%
2016-04-12 00:24:47,405 Stage-3 map = 29%,  reduce = 3%
2016-04-12 00:24:50,433 Stage-3 map = 33%,  reduce = 4%
2016-04-12 00:24:53,457 Stage-3 map = 36%,  reduce = 4%
2016-04-12 00:24:56,480 Stage-3 map = 39%,  reduce = 4%
2016-04-12 00:24:59,504 Stage-3 map = 43%,  reduce = 4%
2016-04-12 00:25:02,528 Stage-3 map = 46%,  reduce = 4%
2016-04-12 00:25:05,549 Stage-3 map = 48%,  reduce = 4%
2016-04-12 00:25:08,570 Stage-3 map = 50%,  reduce = 5%
2016-04-12 00:25:11,592 Stage-3 map = 50%,  reduce = 6%
2016-04-12 00:25:14,613 Stage-3 map = 50%,  reduce = 7%
2016-04-12 00:25:17,635 Stage-3 map = 51%,  reduce = 7%
2016-04-12 00:25:20,657 Stage-3 map = 54%,  reduce = 8%
2016-04-12 00:25:23,679 Stage-3 map = 57%,  reduce = 8%
2016-04-12 00:25:26,700 Stage-3 map = 61%,  reduce = 8%
2016-04-12 00:25:29,721 Stage-3 map = 65%,  reduce = 8%
2016-04-12 00:25:32,741 Stage-3 map = 68%,  reduce = 8%
2016-04-12 00:25:35,762 Stage-3 map = 73%,  reduce = 8%
2016-04-12 00:25:38,782 Stage-3 map = 75%,  reduce = 8%
2016-04-12 00:25:41,802 Stage-3 map = 75%,  reduce = 9%
2016-04-12 00:25:44,823 Stage-3 map = 77%,  reduce = 10%
2016-04-12 00:25:47,844 Stage-3 map = 78%,  reduce = 11%
2016-04-12 00:25:50,865 Stage-3 map = 81%,  reduce = 12%
2016-04-12 00:25:53,885 Stage-3 map = 85%,  reduce = 13%
2016-04-12 00:25:56,906 Stage-3 map = 89%,  reduce = 13%
2016-04-12 00:25:59,926 Stage-3 map = 93%,  reduce = 13%
2016-04-12 00:26:01,941 Stage-3 map = 95%,  reduce = 13%
2016-04-12 00:26:02,975 Stage-3 map = 96%,  reduce = 13%
2016-04-12 00:26:04,990 Stage-3 map = 98%,  reduce = 13%
2016-04-12 00:26:08,011 Stage-3 map = 100%,  reduce = 13%
2016-04-12 00:26:17,065 Stage-3 map = 100%,  reduce = 14%
2016-04-12 00:26:18,074 Stage-3 map = 100%,  reduce = 15%
2016-04-12 00:26:20,089 Stage-3 map = 100%,  reduce = 21%
2016-04-12 00:26:21,103 Stage-3 map = 100%,  reduce = 26%
2016-04-12 00:26:23,120 Stage-3 map = 100%,  reduce = 33%
2016-04-12 00:26:24,129 Stage-3 map = 100%,  reduce = 35%
2016-04-12 00:26:26,144 Stage-3 map = 100%,  reduce = 39%
2016-04-12 00:26:27,154 Stage-3 map = 100%,  reduce = 46%
2016-04-12 00:26:29,170 Stage-3 map = 100%,  reduce = 50%
2016-04-12 00:26:35,208 Stage-3 map = 100%,  reduce = 58%
2016-04-12 00:26:38,229 Stage-3 map = 100%,  reduce = 74%
2016-04-12 00:26:41,249 Stage-3 map = 100%,  reduce = 78%
2016-04-12 00:26:44,270 Stage-3 map = 100%,  reduce = 91%
2016-04-12 00:26:47,291 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604120022_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120022_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120022_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120022_0004
2016-04-12 00:26:56,746 Stage-4 map = 0%,  reduce = 0%
2016-04-12 00:27:05,788 Stage-4 map = 50%,  reduce = 0%
2016-04-12 00:27:14,836 Stage-4 map = 100%,  reduce = 17%
2016-04-12 00:27:23,884 Stage-4 map = 100%,  reduce = 94%
2016-04-12 00:27:26,902 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604120022_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120022_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120022_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120022_0005
2016-04-12 00:27:35,353 Stage-5 map = 0%,  reduce = 0%
2016-04-12 00:27:41,382 Stage-5 map = 100%,  reduce = 0%
2016-04-12 00:27:50,428 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604120022_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604120022_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604120022_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604120022_0006
2016-04-12 00:27:59,815 Stage-6 map = 0%,  reduce = 0%
2016-04-12 00:28:02,831 Stage-6 map = 100%,  reduce = 0%
2016-04-12 00:28:11,879 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604120022_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 348.188 seconds
