Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121437_65877207.txt
OK
Time taken: 3.604 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.074 seconds
OK
Time taken: 0.135 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.074 seconds
OK
Time taken: 0.158 seconds
OK
Time taken: 0.025 seconds
OK
Time taken: 0.025 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/hive/warehouse/q11_important_stock. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:1761)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:1735)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.mkdirs(NameNode.java:542)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121437_443704134.txt
OK
Time taken: 3.553 seconds
OK
Time taken: 0.089 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.014 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.247 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.075 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121437_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121437_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121437_0001
2016-04-12 14:37:28,165 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:37:31,200 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:37:34,239 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:37:40,341 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121437_0001
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121437_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121437_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121437_0002
2016-04-12 14:37:49,096 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:37:58,148 Stage-2 map = 15%,  reduce = 0%
2016-04-12 14:38:01,172 Stage-2 map = 21%,  reduce = 0%
2016-04-12 14:38:10,249 Stage-2 map = 32%,  reduce = 7%
2016-04-12 14:38:13,278 Stage-2 map = 40%,  reduce = 7%
2016-04-12 14:38:16,308 Stage-2 map = 42%,  reduce = 7%
2016-04-12 14:38:25,392 Stage-2 map = 58%,  reduce = 14%
2016-04-12 14:38:27,416 Stage-2 map = 60%,  reduce = 14%
2016-04-12 14:38:28,430 Stage-2 map = 63%,  reduce = 14%
2016-04-12 14:38:30,455 Stage-2 map = 63%,  reduce = 21%
2016-04-12 14:38:36,516 Stage-2 map = 79%,  reduce = 21%
2016-04-12 14:38:39,545 Stage-2 map = 84%,  reduce = 21%
2016-04-12 14:38:42,574 Stage-2 map = 89%,  reduce = 21%
2016-04-12 14:38:45,602 Stage-2 map = 89%,  reduce = 26%
2016-04-12 14:38:48,631 Stage-2 map = 97%,  reduce = 30%
2016-04-12 14:38:51,657 Stage-2 map = 100%,  reduce = 30%
2016-04-12 14:39:06,773 Stage-2 map = 100%,  reduce = 89%
2016-04-12 14:39:09,802 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121437_0002
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121437_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121437_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121437_0003
2016-04-12 14:39:19,354 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:39:25,390 Stage-3 map = 100%,  reduce = 0%
2016-04-12 14:39:34,459 Stage-3 map = 100%,  reduce = 33%
2016-04-12 14:39:37,484 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121437_0003
Loading data to table q11_part_tmp
304774 Rows loaded to q11_part_tmp
OK
Time taken: 140.123 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121437_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121437_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121437_0004
2016-04-12 14:39:46,126 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:39:49,143 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:39:58,199 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121437_0004
Loading data to table q11_sum_tmp
1 Rows loaded to q11_sum_tmp
OK
Time taken: 21.776 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121437_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121437_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121437_0005
2016-04-12 14:40:06,918 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:40:09,935 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:40:12,955 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:40:18,993 Stage-1 map = 100%,  reduce = 33%
2016-04-12 14:40:22,014 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121437_0005
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121437_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121437_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121437_0006
2016-04-12 14:40:31,485 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:40:34,501 Stage-2 map = 100%,  reduce = 0%
2016-04-12 14:40:43,551 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121437_0006
Loading data to table q11_important_stock
OK
Time taken: 45.28 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121441_1769825485.txt
OK
Time taken: 3.456 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.05 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q11_important_stock. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121441_1177636842.txt
OK
Time taken: 2.882 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.604 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.174 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.066 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121441_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121441_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121441_0001
2016-04-12 14:41:22,704 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:41:25,737 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:41:28,778 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:41:34,849 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121441_0001
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121441_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121441_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121441_0002
2016-04-12 14:41:43,965 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:41:53,019 Stage-2 map = 15%,  reduce = 0%
2016-04-12 14:41:56,043 Stage-2 map = 21%,  reduce = 0%
2016-04-12 14:42:05,122 Stage-2 map = 29%,  reduce = 7%
2016-04-12 14:42:08,154 Stage-2 map = 42%,  reduce = 7%
2016-04-12 14:42:17,236 Stage-2 map = 57%,  reduce = 7%
2016-04-12 14:42:20,268 Stage-2 map = 63%,  reduce = 14%
2016-04-12 14:42:26,329 Stage-2 map = 63%,  reduce = 21%
2016-04-12 14:42:29,362 Stage-2 map = 79%,  reduce = 21%
2016-04-12 14:42:32,397 Stage-2 map = 84%,  reduce = 21%
2016-04-12 14:42:35,426 Stage-2 map = 89%,  reduce = 21%
2016-04-12 14:42:41,478 Stage-2 map = 96%,  reduce = 28%
2016-04-12 14:42:44,506 Stage-2 map = 100%,  reduce = 29%
2016-04-12 14:42:47,534 Stage-2 map = 100%,  reduce = 30%
2016-04-12 14:42:56,610 Stage-2 map = 100%,  reduce = 87%
2016-04-12 14:42:59,637 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121441_0002
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121441_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121441_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121441_0003
2016-04-12 14:43:08,184 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:43:14,219 Stage-3 map = 100%,  reduce = 0%
2016-04-12 14:43:23,285 Stage-3 map = 100%,  reduce = 33%
2016-04-12 14:43:26,314 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121441_0003
Loading data to table q11_part_tmp
304774 Rows loaded to q11_part_tmp
OK
Time taken: 132.522 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121441_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121441_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121441_0004
2016-04-12 14:43:34,920 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:43:37,937 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:43:46,991 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121441_0004
Loading data to table q11_sum_tmp
1 Rows loaded to q11_sum_tmp
OK
Time taken: 20.668 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121441_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121441_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121441_0005
2016-04-12 14:43:56,678 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:43:59,694 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:44:02,714 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:44:08,753 Stage-1 map = 100%,  reduce = 33%
2016-04-12 14:44:11,775 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121441_0005
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121441_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121441_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121441_0006
2016-04-12 14:44:20,249 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:44:23,265 Stage-2 map = 100%,  reduce = 0%
2016-04-12 14:44:32,314 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121441_0006
Loading data to table q11_important_stock
OK
Time taken: 45.347 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121444_1785415500.txt
OK
Time taken: 3.476 seconds
OK
Time taken: 0.087 seconds
OK
Time taken: 0.066 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q11_important_stock. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121445_48592402.txt
OK
Time taken: 2.807 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.611 seconds
OK
Time taken: 0.073 seconds
OK
Time taken: 0.167 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.058 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121444_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121444_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121444_0001
2016-04-12 14:45:11,699 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:45:14,732 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:45:17,771 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:45:23,844 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121444_0001
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121444_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121444_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121444_0002
2016-04-12 14:45:33,552 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:45:42,607 Stage-2 map = 15%,  reduce = 0%
2016-04-12 14:45:45,631 Stage-2 map = 21%,  reduce = 0%
2016-04-12 14:45:53,698 Stage-2 map = 25%,  reduce = 4%
2016-04-12 14:45:54,711 Stage-2 map = 32%,  reduce = 7%
2016-04-12 14:45:56,739 Stage-2 map = 39%,  reduce = 7%
2016-04-12 14:45:57,753 Stage-2 map = 42%,  reduce = 7%
2016-04-12 14:46:02,798 Stage-2 map = 42%,  reduce = 12%
2016-04-12 14:46:05,829 Stage-2 map = 58%,  reduce = 12%
2016-04-12 14:46:08,860 Stage-2 map = 63%,  reduce = 14%
2016-04-12 14:46:14,921 Stage-2 map = 68%,  reduce = 21%
2016-04-12 14:46:17,955 Stage-2 map = 81%,  reduce = 21%
2016-04-12 14:46:20,986 Stage-2 map = 84%,  reduce = 21%
2016-04-12 14:46:24,015 Stage-2 map = 89%,  reduce = 23%
2016-04-12 14:46:30,065 Stage-2 map = 96%,  reduce = 27%
2016-04-12 14:46:33,093 Stage-2 map = 100%,  reduce = 28%
2016-04-12 14:46:36,121 Stage-2 map = 100%,  reduce = 30%
2016-04-12 14:46:45,193 Stage-2 map = 100%,  reduce = 86%
2016-04-12 14:46:48,221 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121444_0002
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121444_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121444_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121444_0003
2016-04-12 14:46:57,798 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:47:03,834 Stage-3 map = 100%,  reduce = 0%
2016-04-12 14:47:11,894 Stage-3 map = 100%,  reduce = 33%
2016-04-12 14:47:14,925 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121444_0003
Loading data to table q11_part_tmp
304774 Rows loaded to q11_part_tmp
OK
Time taken: 132.13 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121444_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121444_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121444_0004
2016-04-12 14:47:24,656 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:47:27,673 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:47:36,729 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121444_0004
Loading data to table q11_sum_tmp
1 Rows loaded to q11_sum_tmp
OK
Time taken: 21.81 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121444_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121444_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121444_0005
2016-04-12 14:47:45,317 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:47:48,333 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:47:51,353 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:47:57,392 Stage-1 map = 100%,  reduce = 33%
2016-04-12 14:48:00,411 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121444_0005
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121444_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121444_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121444_0006
2016-04-12 14:48:09,875 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:48:12,890 Stage-2 map = 100%,  reduce = 0%
2016-04-12 14:48:21,937 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121444_0006
Loading data to table q11_important_stock
OK
Time taken: 44.238 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121448_1741818841.txt
OK
Time taken: 3.515 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.058 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q11_important_stock. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121448_359049458.txt
OK
Time taken: 2.942 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.621 seconds
OK
Time taken: 0.106 seconds
OK
Time taken: 0.276 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.067 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121448_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121448_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121448_0001
2016-04-12 14:49:00,330 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:49:03,364 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:49:06,403 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:49:12,475 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121448_0001
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121448_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121448_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121448_0002
2016-04-12 14:49:21,595 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:49:30,647 Stage-2 map = 15%,  reduce = 0%
2016-04-12 14:49:33,671 Stage-2 map = 21%,  reduce = 0%
2016-04-12 14:49:42,747 Stage-2 map = 32%,  reduce = 7%
2016-04-12 14:49:45,776 Stage-2 map = 40%,  reduce = 7%
2016-04-12 14:49:48,806 Stage-2 map = 42%,  reduce = 7%
2016-04-12 14:49:54,861 Stage-2 map = 50%,  reduce = 7%
2016-04-12 14:49:57,892 Stage-2 map = 60%,  reduce = 14%
2016-04-12 14:50:00,926 Stage-2 map = 63%,  reduce = 14%
2016-04-12 14:50:03,959 Stage-2 map = 63%,  reduce = 21%
2016-04-12 14:50:06,989 Stage-2 map = 71%,  reduce = 21%
2016-04-12 14:50:10,020 Stage-2 map = 81%,  reduce = 21%
2016-04-12 14:50:13,063 Stage-2 map = 84%,  reduce = 21%
2016-04-12 14:50:15,083 Stage-2 map = 89%,  reduce = 21%
2016-04-12 14:50:18,110 Stage-2 map = 96%,  reduce = 25%
2016-04-12 14:50:21,138 Stage-2 map = 100%,  reduce = 30%
2016-04-12 14:50:33,233 Stage-2 map = 100%,  reduce = 84%
2016-04-12 14:50:36,262 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121448_0002
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121448_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121448_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121448_0003
2016-04-12 14:50:45,348 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:50:51,381 Stage-3 map = 100%,  reduce = 0%
2016-04-12 14:51:00,446 Stage-3 map = 100%,  reduce = 33%
2016-04-12 14:51:03,472 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121448_0003
Loading data to table q11_part_tmp
304774 Rows loaded to q11_part_tmp
OK
Time taken: 131.787 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121448_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121448_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121448_0004
2016-04-12 14:51:13,084 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:51:16,102 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:51:19,122 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:51:24,156 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121448_0004
Loading data to table q11_sum_tmp
1 Rows loaded to q11_sum_tmp
OK
Time taken: 20.675 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121448_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121448_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121448_0005
2016-04-12 14:51:33,813 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:51:36,831 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:51:39,851 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:51:45,888 Stage-1 map = 100%,  reduce = 33%
2016-04-12 14:51:48,914 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121448_0005
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121448_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121448_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121448_0006
2016-04-12 14:51:57,381 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:52:00,398 Stage-2 map = 100%,  reduce = 0%
2016-04-12 14:52:09,446 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121448_0006
Loading data to table q11_important_stock
OK
Time taken: 45.338 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121452_103260884.txt
OK
Time taken: 3.468 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.059 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q11_important_stock. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121452_14434027.txt
OK
Time taken: 2.946 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.013 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.575 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.165 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.066 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121452_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121452_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121452_0001
2016-04-12 14:52:49,587 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:52:52,622 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:52:55,660 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:53:01,733 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121452_0001
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121452_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121452_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121452_0002
2016-04-12 14:53:10,536 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:53:19,589 Stage-2 map = 15%,  reduce = 0%
2016-04-12 14:53:22,614 Stage-2 map = 21%,  reduce = 0%
2016-04-12 14:53:31,691 Stage-2 map = 28%,  reduce = 7%
2016-04-12 14:53:34,719 Stage-2 map = 39%,  reduce = 7%
2016-04-12 14:53:37,748 Stage-2 map = 42%,  reduce = 7%
2016-04-12 14:53:46,831 Stage-2 map = 58%,  reduce = 14%
2016-04-12 14:53:49,863 Stage-2 map = 63%,  reduce = 14%
2016-04-12 14:53:52,898 Stage-2 map = 63%,  reduce = 18%
2016-04-12 14:53:57,949 Stage-2 map = 71%,  reduce = 18%
2016-04-12 14:53:58,963 Stage-2 map = 78%,  reduce = 18%
2016-04-12 14:54:00,983 Stage-2 map = 84%,  reduce = 21%
2016-04-12 14:54:04,012 Stage-2 map = 89%,  reduce = 21%
2016-04-12 14:54:07,041 Stage-2 map = 89%,  reduce = 27%
2016-04-12 14:54:10,069 Stage-2 map = 97%,  reduce = 30%
2016-04-12 14:54:13,097 Stage-2 map = 100%,  reduce = 30%
2016-04-12 14:54:19,147 Stage-2 map = 100%,  reduce = 32%
2016-04-12 14:54:22,174 Stage-2 map = 100%,  reduce = 84%
2016-04-12 14:54:25,200 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121452_0002
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121452_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121452_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121452_0003
2016-04-12 14:54:34,773 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:54:40,809 Stage-3 map = 100%,  reduce = 0%
2016-04-12 14:54:49,877 Stage-3 map = 100%,  reduce = 33%
2016-04-12 14:54:52,909 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121452_0003
Loading data to table q11_part_tmp
304774 Rows loaded to q11_part_tmp
OK
Time taken: 132.728 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121452_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121452_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121452_0004
2016-04-12 14:55:01,593 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:55:04,610 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:55:13,668 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121452_0004
Loading data to table q11_sum_tmp
1 Rows loaded to q11_sum_tmp
OK
Time taken: 20.742 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121452_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121452_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121452_0005
2016-04-12 14:55:22,270 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:55:25,288 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:55:28,309 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:55:34,346 Stage-1 map = 100%,  reduce = 33%
2016-04-12 14:55:37,366 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121452_0005
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121452_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121452_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121452_0006
2016-04-12 14:55:46,817 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:55:49,834 Stage-2 map = 100%,  reduce = 0%
2016-04-12 14:55:58,882 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121452_0006
Loading data to table q11_important_stock
OK
Time taken: 45.222 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121456_1212684644.txt
OK
Time taken: 3.59 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.058 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q11_important_stock. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121456_1359094820.txt
OK
Time taken: 2.866 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.633 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.168 seconds
OK
Time taken: 0.03 seconds
OK
Time taken: 0.026 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.074 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121456_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121456_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121456_0001
2016-04-12 14:56:38,991 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:56:42,025 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:56:45,066 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:56:51,140 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121456_0001
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121456_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121456_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121456_0002
2016-04-12 14:57:00,739 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:57:08,788 Stage-2 map = 8%,  reduce = 0%
2016-04-12 14:57:09,797 Stage-2 map = 15%,  reduce = 0%
2016-04-12 14:57:11,816 Stage-2 map = 21%,  reduce = 0%
2016-04-12 14:57:20,893 Stage-2 map = 36%,  reduce = 7%
2016-04-12 14:57:23,924 Stage-2 map = 42%,  reduce = 7%
2016-04-12 14:57:33,005 Stage-2 map = 50%,  reduce = 7%
2016-04-12 14:57:36,038 Stage-2 map = 60%,  reduce = 14%
2016-04-12 14:57:39,071 Stage-2 map = 63%,  reduce = 14%
2016-04-12 14:57:42,105 Stage-2 map = 63%,  reduce = 21%
2016-04-12 14:57:45,138 Stage-2 map = 76%,  reduce = 21%
2016-04-12 14:57:48,172 Stage-2 map = 84%,  reduce = 21%
2016-04-12 14:57:54,223 Stage-2 map = 89%,  reduce = 21%
2016-04-12 14:57:57,251 Stage-2 map = 96%,  reduce = 26%
2016-04-12 14:58:00,279 Stage-2 map = 100%,  reduce = 30%
2016-04-12 14:58:12,377 Stage-2 map = 100%,  reduce = 86%
2016-04-12 14:58:15,405 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121456_0002
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121456_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121456_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121456_0003
2016-04-12 14:58:23,967 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:58:30,003 Stage-3 map = 50%,  reduce = 0%
2016-04-12 14:58:31,015 Stage-3 map = 100%,  reduce = 0%
2016-04-12 14:58:39,075 Stage-3 map = 100%,  reduce = 33%
2016-04-12 14:58:42,105 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121456_0003
Loading data to table q11_part_tmp
304774 Rows loaded to q11_part_tmp
OK
Time taken: 132.06 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121456_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121456_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121456_0004
2016-04-12 14:58:51,728 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:58:54,746 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:59:03,801 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121456_0004
Loading data to table q11_sum_tmp
1 Rows loaded to q11_sum_tmp
OK
Time taken: 21.684 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121456_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121456_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121456_0005
2016-04-12 14:59:12,457 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:59:15,475 Stage-1 map = 50%,  reduce = 0%
2016-04-12 14:59:18,496 Stage-1 map = 100%,  reduce = 0%
2016-04-12 14:59:24,535 Stage-1 map = 100%,  reduce = 33%
2016-04-12 14:59:27,560 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121456_0005
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121456_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121456_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121456_0006
2016-04-12 14:59:36,031 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:59:39,048 Stage-2 map = 100%,  reduce = 0%
2016-04-12 14:59:48,095 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121456_0006
Loading data to table q11_important_stock
OK
Time taken: 44.272 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121500_475063033.txt
OK
Time taken: 3.525 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.059 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q11_important_stock. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121500_127118739.txt
OK
Time taken: 2.861 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.597 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.149 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.025 seconds
OK
Time taken: 0.026 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.076 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121500_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121500_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121500_0001
2016-04-12 15:00:27,111 Stage-1 map = 0%,  reduce = 0%
2016-04-12 15:00:30,142 Stage-1 map = 50%,  reduce = 0%
2016-04-12 15:00:33,182 Stage-1 map = 100%,  reduce = 0%
2016-04-12 15:00:39,253 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121500_0001
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121500_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121500_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121500_0002
2016-04-12 15:00:48,393 Stage-2 map = 0%,  reduce = 0%
2016-04-12 15:00:57,444 Stage-2 map = 15%,  reduce = 0%
2016-04-12 15:01:00,467 Stage-2 map = 21%,  reduce = 0%
2016-04-12 15:01:09,541 Stage-2 map = 29%,  reduce = 7%
2016-04-12 15:01:12,570 Stage-2 map = 42%,  reduce = 7%
2016-04-12 15:01:18,623 Stage-2 map = 42%,  reduce = 10%
2016-04-12 15:01:21,652 Stage-2 map = 55%,  reduce = 10%
2016-04-12 15:01:24,692 Stage-2 map = 63%,  reduce = 14%
2016-04-12 15:01:30,753 Stage-2 map = 68%,  reduce = 20%
2016-04-12 15:01:33,785 Stage-2 map = 81%,  reduce = 20%
2016-04-12 15:01:36,817 Stage-2 map = 84%,  reduce = 20%
2016-04-12 15:01:39,845 Stage-2 map = 89%,  reduce = 23%
2016-04-12 15:01:42,872 Stage-2 map = 94%,  reduce = 28%
2016-04-12 15:01:45,901 Stage-2 map = 100%,  reduce = 28%
2016-04-12 15:01:48,927 Stage-2 map = 100%,  reduce = 29%
2016-04-12 15:01:51,955 Stage-2 map = 100%,  reduce = 32%
2016-04-12 15:01:54,981 Stage-2 map = 100%,  reduce = 79%
2016-04-12 15:01:58,010 Stage-2 map = 100%,  reduce = 99%
2016-04-12 15:02:01,039 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121500_0002
Launching Job 3 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121500_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121500_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121500_0003
2016-04-12 15:02:09,563 Stage-3 map = 0%,  reduce = 0%
2016-04-12 15:02:15,598 Stage-3 map = 100%,  reduce = 0%
2016-04-12 15:02:24,661 Stage-3 map = 100%,  reduce = 33%
2016-04-12 15:02:27,689 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121500_0003
Loading data to table q11_part_tmp
304774 Rows loaded to q11_part_tmp
OK
Time taken: 129.443 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121500_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121500_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121500_0004
2016-04-12 15:02:36,319 Stage-1 map = 0%,  reduce = 0%
2016-04-12 15:02:39,337 Stage-1 map = 100%,  reduce = 0%
2016-04-12 15:02:48,392 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121500_0004
Loading data to table q11_sum_tmp
1 Rows loaded to q11_sum_tmp
OK
Time taken: 20.692 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121500_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121500_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121500_0005
2016-04-12 15:02:58,032 Stage-1 map = 0%,  reduce = 0%
2016-04-12 15:03:01,049 Stage-1 map = 50%,  reduce = 0%
2016-04-12 15:03:04,068 Stage-1 map = 100%,  reduce = 0%
2016-04-12 15:03:10,105 Stage-1 map = 100%,  reduce = 33%
2016-04-12 15:03:13,125 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121500_0005
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121500_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121500_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121500_0006
2016-04-12 15:03:21,586 Stage-2 map = 0%,  reduce = 0%
2016-04-12 15:03:24,603 Stage-2 map = 100%,  reduce = 0%
2016-04-12 15:03:33,649 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121500_0006
Loading data to table q11_important_stock
OK
Time taken: 45.33 seconds
