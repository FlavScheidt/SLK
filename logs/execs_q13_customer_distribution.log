Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121540_2086691025.txt
OK
Time taken: 3.4 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.257 seconds
OK
Time taken: 0.058 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/hive/warehouse/q13_customer_distribution. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:1761)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:1735)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.mkdirs(NameNode.java:542)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121540_2005434485.txt
OK
Time taken: 3.636 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.24 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.048 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121540_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121540_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121540_0001
2016-04-12 15:40:45,039 Stage-1 map = 0%,  reduce = 0%
2016-04-12 15:40:54,111 Stage-1 map = 3%,  reduce = 0%
2016-04-12 15:40:57,136 Stage-1 map = 4%,  reduce = 0%
2016-04-12 15:41:00,173 Stage-1 map = 10%,  reduce = 0%
2016-04-12 15:41:03,210 Stage-1 map = 11%,  reduce = 0%
2016-04-12 15:41:06,245 Stage-1 map = 13%,  reduce = 0%
2016-04-12 15:41:09,281 Stage-1 map = 15%,  reduce = 0%
2016-04-12 15:41:12,316 Stage-1 map = 17%,  reduce = 2%
2016-04-12 15:41:15,351 Stage-1 map = 20%,  reduce = 4%
2016-04-12 15:41:18,385 Stage-1 map = 23%,  reduce = 4%
2016-04-12 15:41:21,420 Stage-1 map = 25%,  reduce = 4%
2016-04-12 15:41:24,455 Stage-1 map = 29%,  reduce = 5%
2016-04-12 15:41:27,489 Stage-1 map = 30%,  reduce = 7%
2016-04-12 15:41:30,523 Stage-1 map = 32%,  reduce = 9%
2016-04-12 15:41:33,557 Stage-1 map = 36%,  reduce = 9%
2016-04-12 15:41:36,592 Stage-1 map = 37%,  reduce = 9%
2016-04-12 15:41:39,624 Stage-1 map = 42%,  reduce = 9%
2016-04-12 15:41:42,657 Stage-1 map = 43%,  reduce = 10%
2016-04-12 15:41:45,688 Stage-1 map = 45%,  reduce = 11%
2016-04-12 15:41:48,718 Stage-1 map = 47%,  reduce = 13%
2016-04-12 15:41:51,748 Stage-1 map = 50%,  reduce = 14%
2016-04-12 15:41:54,782 Stage-1 map = 53%,  reduce = 14%
2016-04-12 15:41:57,812 Stage-1 map = 57%,  reduce = 14%
2016-04-12 15:42:00,841 Stage-1 map = 59%,  reduce = 15%
2016-04-12 15:42:03,873 Stage-1 map = 60%,  reduce = 17%
2016-04-12 15:42:06,908 Stage-1 map = 60%,  reduce = 19%
2016-04-12 15:42:08,930 Stage-1 map = 63%,  reduce = 19%
2016-04-12 15:42:11,959 Stage-1 map = 65%,  reduce = 19%
2016-04-12 15:42:14,988 Stage-1 map = 69%,  reduce = 20%
2016-04-12 15:42:18,017 Stage-1 map = 72%,  reduce = 20%
2016-04-12 15:42:21,046 Stage-1 map = 73%,  reduce = 20%
2016-04-12 15:42:27,101 Stage-1 map = 77%,  reduce = 23%
2016-04-12 15:42:30,128 Stage-1 map = 83%,  reduce = 24%
2016-04-12 15:42:33,154 Stage-1 map = 87%,  reduce = 24%
2016-04-12 15:42:39,209 Stage-1 map = 87%,  reduce = 27%
2016-04-12 15:42:42,234 Stage-1 map = 90%,  reduce = 29%
2016-04-12 15:42:45,257 Stage-1 map = 92%,  reduce = 29%
2016-04-12 15:42:48,280 Stage-1 map = 100%,  reduce = 29%
2016-04-12 15:42:54,322 Stage-1 map = 100%,  reduce = 32%
2016-04-12 15:42:57,346 Stage-1 map = 100%,  reduce = 69%
2016-04-12 15:43:00,368 Stage-1 map = 100%,  reduce = 78%
2016-04-12 15:43:03,389 Stage-1 map = 100%,  reduce = 84%
2016-04-12 15:43:06,410 Stage-1 map = 100%,  reduce = 93%
2016-04-12 15:43:09,435 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121540_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121540_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121540_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121540_0002
2016-04-12 15:43:18,909 Stage-2 map = 0%,  reduce = 0%
2016-04-12 15:43:27,957 Stage-2 map = 100%,  reduce = 0%
2016-04-12 15:43:37,014 Stage-2 map = 100%,  reduce = 33%
2016-04-12 15:43:40,035 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121540_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121540_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121540_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121540_0003
2016-04-12 15:43:49,058 Stage-3 map = 0%,  reduce = 0%
2016-04-12 15:43:52,080 Stage-3 map = 100%,  reduce = 0%
2016-04-12 15:44:00,129 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121540_0003
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121540_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121540_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121540_0004
2016-04-12 15:44:09,609 Stage-4 map = 0%,  reduce = 0%
2016-04-12 15:44:12,627 Stage-4 map = 100%,  reduce = 0%
2016-04-12 15:44:21,683 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121540_0004
Loading data to table q13_customer_distribution
46 Rows loaded to q13_customer_distribution
OK
Time taken: 225.563 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121544_322708609.txt
OK
Time taken: 3.433 seconds
OK
Time taken: 0.065 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q13_customer_distribution. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121544_1857731152.txt
OK
Time taken: 2.948 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.487 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121544_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121544_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121544_0001
2016-04-12 15:45:03,680 Stage-1 map = 0%,  reduce = 0%
2016-04-12 15:45:12,746 Stage-1 map = 3%,  reduce = 0%
2016-04-12 15:45:15,769 Stage-1 map = 4%,  reduce = 0%
2016-04-12 15:45:18,792 Stage-1 map = 10%,  reduce = 0%
2016-04-12 15:45:21,831 Stage-1 map = 11%,  reduce = 0%
2016-04-12 15:45:24,865 Stage-1 map = 13%,  reduce = 0%
2016-04-12 15:45:30,929 Stage-1 map = 16%,  reduce = 4%
2016-04-12 15:45:33,963 Stage-1 map = 20%,  reduce = 4%
2016-04-12 15:45:36,994 Stage-1 map = 23%,  reduce = 4%
2016-04-12 15:45:40,029 Stage-1 map = 27%,  reduce = 4%
2016-04-12 15:45:46,092 Stage-1 map = 27%,  reduce = 9%
2016-04-12 15:45:49,125 Stage-1 map = 29%,  reduce = 9%
2016-04-12 15:45:52,159 Stage-1 map = 34%,  reduce = 9%
2016-04-12 15:45:55,191 Stage-1 map = 38%,  reduce = 9%
2016-04-12 15:45:58,225 Stage-1 map = 40%,  reduce = 9%
2016-04-12 15:46:03,282 Stage-1 map = 42%,  reduce = 10%
2016-04-12 15:46:06,312 Stage-1 map = 45%,  reduce = 13%
2016-04-12 15:46:09,343 Stage-1 map = 48%,  reduce = 13%
2016-04-12 15:46:12,374 Stage-1 map = 51%,  reduce = 13%
2016-04-12 15:46:15,403 Stage-1 map = 55%,  reduce = 14%
2016-04-12 15:46:18,433 Stage-1 map = 57%,  reduce = 14%
2016-04-12 15:46:21,462 Stage-1 map = 57%,  reduce = 18%
2016-04-12 15:46:24,492 Stage-1 map = 59%,  reduce = 18%
2016-04-12 15:46:27,521 Stage-1 map = 60%,  reduce = 19%
2016-04-12 15:46:30,548 Stage-1 map = 62%,  reduce = 19%
2016-04-12 15:46:33,576 Stage-1 map = 66%,  reduce = 19%
2016-04-12 15:46:36,604 Stage-1 map = 70%,  reduce = 20%
2016-04-12 15:46:39,633 Stage-1 map = 72%,  reduce = 20%
2016-04-12 15:46:42,661 Stage-1 map = 73%,  reduce = 20%
2016-04-12 15:46:48,713 Stage-1 map = 78%,  reduce = 20%
2016-04-12 15:46:51,745 Stage-1 map = 81%,  reduce = 23%
2016-04-12 15:46:54,769 Stage-1 map = 87%,  reduce = 23%
2016-04-12 15:46:57,792 Stage-1 map = 87%,  reduce = 26%
2016-04-12 15:47:03,835 Stage-1 map = 93%,  reduce = 26%
2016-04-12 15:47:06,858 Stage-1 map = 98%,  reduce = 29%
2016-04-12 15:47:09,880 Stage-1 map = 100%,  reduce = 29%
2016-04-12 15:47:12,902 Stage-1 map = 100%,  reduce = 32%
2016-04-12 15:47:21,962 Stage-1 map = 100%,  reduce = 68%
2016-04-12 15:47:24,983 Stage-1 map = 100%,  reduce = 80%
2016-04-12 15:47:28,005 Stage-1 map = 100%,  reduce = 87%
2016-04-12 15:47:31,027 Stage-1 map = 100%,  reduce = 92%
2016-04-12 15:47:34,051 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121544_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121544_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121544_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121544_0002
2016-04-12 15:47:42,568 Stage-2 map = 0%,  reduce = 0%
2016-04-12 15:47:51,613 Stage-2 map = 100%,  reduce = 0%
2016-04-12 15:48:00,666 Stage-2 map = 100%,  reduce = 33%
2016-04-12 15:48:03,687 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121544_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121544_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121544_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121544_0003
2016-04-12 15:48:12,649 Stage-3 map = 0%,  reduce = 0%
2016-04-12 15:48:15,666 Stage-3 map = 100%,  reduce = 0%
2016-04-12 15:48:24,719 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121544_0003
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121544_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121544_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121544_0004
2016-04-12 15:48:34,251 Stage-4 map = 0%,  reduce = 0%
2016-04-12 15:48:37,269 Stage-4 map = 100%,  reduce = 0%
2016-04-12 15:48:46,324 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121544_0004
Loading data to table q13_customer_distribution
46 Rows loaded to q13_customer_distribution
OK
Time taken: 232.661 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121549_1521930936.txt
OK
Time taken: 3.45 seconds
OK
Time taken: 0.065 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q13_customer_distribution. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121549_1309907911.txt
OK
Time taken: 2.876 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.469 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121549_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121549_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121549_0001
2016-04-12 15:49:28,510 Stage-1 map = 0%,  reduce = 0%
2016-04-12 15:49:37,576 Stage-1 map = 4%,  reduce = 0%
2016-04-12 15:49:40,599 Stage-1 map = 8%,  reduce = 0%
2016-04-12 15:49:43,634 Stage-1 map = 12%,  reduce = 0%
2016-04-12 15:49:46,672 Stage-1 map = 13%,  reduce = 0%
2016-04-12 15:49:52,733 Stage-1 map = 15%,  reduce = 0%
2016-04-12 15:49:55,768 Stage-1 map = 19%,  reduce = 4%
2016-04-12 15:49:58,801 Stage-1 map = 23%,  reduce = 4%
2016-04-12 15:50:01,836 Stage-1 map = 27%,  reduce = 4%
2016-04-12 15:50:07,900 Stage-1 map = 29%,  reduce = 4%
2016-04-12 15:50:10,934 Stage-1 map = 30%,  reduce = 8%
2016-04-12 15:50:13,970 Stage-1 map = 34%,  reduce = 9%
2016-04-12 15:50:17,002 Stage-1 map = 37%,  reduce = 9%
2016-04-12 15:50:20,035 Stage-1 map = 40%,  reduce = 9%
2016-04-12 15:50:23,068 Stage-1 map = 42%,  reduce = 9%
2016-04-12 15:50:26,099 Stage-1 map = 43%,  reduce = 10%
2016-04-12 15:50:29,129 Stage-1 map = 45%,  reduce = 10%
2016-04-12 15:50:31,151 Stage-1 map = 49%,  reduce = 14%
2016-04-12 15:50:34,181 Stage-1 map = 51%,  reduce = 14%
2016-04-12 15:50:37,211 Stage-1 map = 55%,  reduce = 14%
2016-04-12 15:50:40,242 Stage-1 map = 56%,  reduce = 14%
2016-04-12 15:50:43,271 Stage-1 map = 58%,  reduce = 14%
2016-04-12 15:50:46,299 Stage-1 map = 60%,  reduce = 18%
2016-04-12 15:50:49,330 Stage-1 map = 63%,  reduce = 19%
2016-04-12 15:50:52,358 Stage-1 map = 66%,  reduce = 19%
2016-04-12 15:50:55,384 Stage-1 map = 69%,  reduce = 19%
2016-04-12 15:50:58,412 Stage-1 map = 70%,  reduce = 19%
2016-04-12 15:51:01,441 Stage-1 map = 72%,  reduce = 20%
2016-04-12 15:51:04,467 Stage-1 map = 76%,  reduce = 20%
2016-04-12 15:51:07,495 Stage-1 map = 78%,  reduce = 23%
2016-04-12 15:51:10,521 Stage-1 map = 82%,  reduce = 23%
2016-04-12 15:51:13,548 Stage-1 map = 83%,  reduce = 23%
2016-04-12 15:51:16,573 Stage-1 map = 87%,  reduce = 24%
2016-04-12 15:51:22,616 Stage-1 map = 91%,  reduce = 28%
2016-04-12 15:51:25,641 Stage-1 map = 99%,  reduce = 29%
2016-04-12 15:51:28,672 Stage-1 map = 100%,  reduce = 29%
2016-04-12 15:51:37,732 Stage-1 map = 100%,  reduce = 69%
2016-04-12 15:51:40,755 Stage-1 map = 100%,  reduce = 79%
2016-04-12 15:51:43,779 Stage-1 map = 100%,  reduce = 83%
2016-04-12 15:51:46,804 Stage-1 map = 100%,  reduce = 94%
2016-04-12 15:51:49,828 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121549_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121549_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121549_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121549_0002
2016-04-12 15:51:58,362 Stage-2 map = 0%,  reduce = 0%
2016-04-12 15:52:07,408 Stage-2 map = 100%,  reduce = 0%
2016-04-12 15:52:16,464 Stage-2 map = 100%,  reduce = 33%
2016-04-12 15:52:19,485 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121549_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121549_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121549_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121549_0003
2016-04-12 15:52:28,437 Stage-3 map = 0%,  reduce = 0%
2016-04-12 15:52:31,455 Stage-3 map = 100%,  reduce = 0%
2016-04-12 15:52:40,509 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121549_0003
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121549_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121549_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121549_0004
2016-04-12 15:52:50,015 Stage-4 map = 0%,  reduce = 0%
2016-04-12 15:52:53,033 Stage-4 map = 100%,  reduce = 0%
2016-04-12 15:53:02,089 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121549_0004
Loading data to table q13_customer_distribution
46 Rows loaded to q13_customer_distribution
OK
Time taken: 223.464 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121553_1517315908.txt
OK
Time taken: 3.379 seconds
OK
Time taken: 0.064 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q13_customer_distribution. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121553_830754097.txt
OK
Time taken: 2.921 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.433 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.024 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121553_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121553_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121553_0001
2016-04-12 15:53:43,563 Stage-1 map = 0%,  reduce = 0%
2016-04-12 15:53:52,633 Stage-1 map = 6%,  reduce = 0%
2016-04-12 15:53:55,656 Stage-1 map = 9%,  reduce = 0%
2016-04-12 15:53:58,679 Stage-1 map = 12%,  reduce = 0%
2016-04-12 15:54:01,713 Stage-1 map = 13%,  reduce = 0%
2016-04-12 15:54:10,801 Stage-1 map = 17%,  reduce = 4%
2016-04-12 15:54:13,837 Stage-1 map = 20%,  reduce = 4%
2016-04-12 15:54:16,869 Stage-1 map = 25%,  reduce = 4%
2016-04-12 15:54:19,903 Stage-1 map = 27%,  reduce = 4%
2016-04-12 15:54:25,966 Stage-1 map = 27%,  reduce = 7%
2016-04-12 15:54:29,000 Stage-1 map = 32%,  reduce = 9%
2016-04-12 15:54:32,037 Stage-1 map = 35%,  reduce = 9%
2016-04-12 15:54:35,070 Stage-1 map = 40%,  reduce = 9%
2016-04-12 15:54:47,175 Stage-1 map = 45%,  reduce = 13%
2016-04-12 15:54:50,204 Stage-1 map = 49%,  reduce = 13%
2016-04-12 15:54:53,234 Stage-1 map = 53%,  reduce = 13%
2016-04-12 15:55:02,313 Stage-1 map = 56%,  reduce = 17%
2016-04-12 15:55:05,342 Stage-1 map = 59%,  reduce = 18%
2016-04-12 15:55:08,370 Stage-1 map = 63%,  reduce = 18%
2016-04-12 15:55:11,399 Stage-1 map = 65%,  reduce = 18%
2016-04-12 15:55:14,429 Stage-1 map = 67%,  reduce = 18%
2016-04-12 15:55:20,482 Stage-1 map = 69%,  reduce = 20%
2016-04-12 15:55:23,511 Stage-1 map = 73%,  reduce = 22%
2016-04-12 15:55:26,540 Stage-1 map = 77%,  reduce = 22%
2016-04-12 15:55:28,560 Stage-1 map = 80%,  reduce = 22%
2016-04-12 15:55:37,637 Stage-1 map = 80%,  reduce = 24%
2016-04-12 15:55:38,649 Stage-1 map = 84%,  reduce = 27%
2016-04-12 15:55:40,667 Stage-1 map = 89%,  reduce = 27%
2016-04-12 15:55:43,691 Stage-1 map = 93%,  reduce = 27%
2016-04-12 15:55:49,735 Stage-1 map = 97%,  reduce = 27%
2016-04-12 15:55:52,758 Stage-1 map = 100%,  reduce = 29%
2016-04-12 15:55:55,780 Stage-1 map = 100%,  reduce = 31%
2016-04-12 15:55:58,803 Stage-1 map = 100%,  reduce = 67%
2016-04-12 15:56:01,826 Stage-1 map = 100%,  reduce = 74%
2016-04-12 15:56:04,847 Stage-1 map = 100%,  reduce = 85%
2016-04-12 15:56:07,873 Stage-1 map = 100%,  reduce = 92%
2016-04-12 15:56:10,894 Stage-1 map = 100%,  reduce = 95%
2016-04-12 15:56:13,917 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121553_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121553_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121553_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121553_0002
2016-04-12 15:56:23,488 Stage-2 map = 0%,  reduce = 0%
2016-04-12 15:56:32,533 Stage-2 map = 100%,  reduce = 0%
2016-04-12 15:56:41,587 Stage-2 map = 100%,  reduce = 33%
2016-04-12 15:56:44,608 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121553_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121553_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121553_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121553_0003
2016-04-12 15:56:53,159 Stage-3 map = 0%,  reduce = 0%
2016-04-12 15:56:56,176 Stage-3 map = 100%,  reduce = 0%
2016-04-12 15:57:05,231 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121553_0003
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121553_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121553_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121553_0004
2016-04-12 15:57:13,778 Stage-4 map = 0%,  reduce = 0%
2016-04-12 15:57:16,796 Stage-4 map = 100%,  reduce = 0%
2016-04-12 15:57:25,851 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121553_0004
Loading data to table q13_customer_distribution
46 Rows loaded to q13_customer_distribution
OK
Time taken: 231.781 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121557_1831499919.txt
OK
Time taken: 3.427 seconds
OK
Time taken: 0.065 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q13_customer_distribution. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121557_676972604.txt
OK
Time taken: 2.911 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.447 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.024 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121557_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121557_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121557_0001
2016-04-12 15:58:07,798 Stage-1 map = 0%,  reduce = 0%
2016-04-12 15:58:16,864 Stage-1 map = 3%,  reduce = 0%
2016-04-12 15:58:19,887 Stage-1 map = 4%,  reduce = 0%
2016-04-12 15:58:22,910 Stage-1 map = 9%,  reduce = 0%
2016-04-12 15:58:25,943 Stage-1 map = 11%,  reduce = 0%
2016-04-12 15:58:28,978 Stage-1 map = 13%,  reduce = 0%
2016-04-12 15:58:35,040 Stage-1 map = 16%,  reduce = 2%
2016-04-12 15:58:38,075 Stage-1 map = 18%,  reduce = 4%
2016-04-12 15:58:41,108 Stage-1 map = 23%,  reduce = 4%
2016-04-12 15:58:44,144 Stage-1 map = 25%,  reduce = 4%
2016-04-12 15:58:47,180 Stage-1 map = 27%,  reduce = 4%
2016-04-12 15:58:50,216 Stage-1 map = 27%,  reduce = 7%
2016-04-12 15:58:53,250 Stage-1 map = 29%,  reduce = 9%
2016-04-12 15:58:56,284 Stage-1 map = 34%,  reduce = 9%
2016-04-12 15:58:58,308 Stage-1 map = 35%,  reduce = 9%
2016-04-12 15:58:59,326 Stage-1 map = 38%,  reduce = 9%
2016-04-12 15:59:01,358 Stage-1 map = 40%,  reduce = 9%
2016-04-12 15:59:07,420 Stage-1 map = 40%,  reduce = 11%
2016-04-12 15:59:10,451 Stage-1 map = 43%,  reduce = 12%
2016-04-12 15:59:13,482 Stage-1 map = 47%,  reduce = 13%
2016-04-12 15:59:16,512 Stage-1 map = 51%,  reduce = 13%
2016-04-12 15:59:19,543 Stage-1 map = 53%,  reduce = 13%
2016-04-12 15:59:25,597 Stage-1 map = 55%,  reduce = 13%
2016-04-12 15:59:28,629 Stage-1 map = 57%,  reduce = 18%
2016-04-12 15:59:31,658 Stage-1 map = 60%,  reduce = 18%
2016-04-12 15:59:34,687 Stage-1 map = 63%,  reduce = 18%
2016-04-12 15:59:37,715 Stage-1 map = 65%,  reduce = 18%
2016-04-12 15:59:40,744 Stage-1 map = 69%,  reduce = 19%
2016-04-12 15:59:43,772 Stage-1 map = 70%,  reduce = 21%
2016-04-12 15:59:46,799 Stage-1 map = 72%,  reduce = 21%
2016-04-12 15:59:49,826 Stage-1 map = 75%,  reduce = 22%
2016-04-12 15:59:52,854 Stage-1 map = 78%,  reduce = 22%
2016-04-12 15:59:55,882 Stage-1 map = 82%,  reduce = 23%
2016-04-12 15:59:58,907 Stage-1 map = 83%,  reduce = 24%
2016-04-12 16:00:01,930 Stage-1 map = 87%,  reduce = 26%
2016-04-12 16:00:07,974 Stage-1 map = 94%,  reduce = 27%
2016-04-12 16:00:10,998 Stage-1 map = 96%,  reduce = 28%
2016-04-12 16:00:14,021 Stage-1 map = 100%,  reduce = 29%
2016-04-12 16:00:17,045 Stage-1 map = 100%,  reduce = 31%
2016-04-12 16:00:20,067 Stage-1 map = 100%,  reduce = 33%
2016-04-12 16:00:23,088 Stage-1 map = 100%,  reduce = 52%
2016-04-12 16:00:26,109 Stage-1 map = 100%,  reduce = 77%
2016-04-12 16:00:29,130 Stage-1 map = 100%,  reduce = 81%
2016-04-12 16:00:32,151 Stage-1 map = 100%,  reduce = 87%
2016-04-12 16:00:35,175 Stage-1 map = 100%,  reduce = 98%
2016-04-12 16:00:38,198 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121557_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121557_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121557_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121557_0002
2016-04-12 16:00:46,736 Stage-2 map = 0%,  reduce = 0%
2016-04-12 16:00:55,782 Stage-2 map = 100%,  reduce = 0%
2016-04-12 16:01:04,835 Stage-2 map = 100%,  reduce = 33%
2016-04-12 16:01:07,857 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121557_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121557_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121557_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121557_0003
2016-04-12 16:01:17,443 Stage-3 map = 0%,  reduce = 0%
2016-04-12 16:01:20,462 Stage-3 map = 100%,  reduce = 0%
2016-04-12 16:01:29,517 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121557_0003
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121557_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121557_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121557_0004
2016-04-12 16:01:38,024 Stage-4 map = 0%,  reduce = 0%
2016-04-12 16:01:41,041 Stage-4 map = 100%,  reduce = 0%
2016-04-12 16:01:50,095 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121557_0004
Loading data to table q13_customer_distribution
46 Rows loaded to q13_customer_distribution
OK
Time taken: 232.273 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121602_1301177051.txt
OK
Time taken: 3.435 seconds
OK
Time taken: 0.065 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q13_customer_distribution. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121602_1526548374.txt
OK
Time taken: 2.9 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.421 seconds
OK
Time taken: 0.063 seconds
OK
Time taken: 0.032 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121602_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121602_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121602_0001
2016-04-12 16:02:32,556 Stage-1 map = 0%,  reduce = 0%
2016-04-12 16:02:41,625 Stage-1 map = 3%,  reduce = 0%
2016-04-12 16:02:44,647 Stage-1 map = 4%,  reduce = 0%
2016-04-12 16:02:47,672 Stage-1 map = 9%,  reduce = 0%
2016-04-12 16:02:50,706 Stage-1 map = 11%,  reduce = 0%
2016-04-12 16:02:53,743 Stage-1 map = 13%,  reduce = 0%
2016-04-12 16:02:59,807 Stage-1 map = 16%,  reduce = 2%
2016-04-12 16:03:02,841 Stage-1 map = 18%,  reduce = 4%
2016-04-12 16:03:05,873 Stage-1 map = 22%,  reduce = 4%
2016-04-12 16:03:08,909 Stage-1 map = 25%,  reduce = 4%
2016-04-12 16:03:11,944 Stage-1 map = 27%,  reduce = 4%
2016-04-12 16:03:14,978 Stage-1 map = 27%,  reduce = 7%
2016-04-12 16:03:18,012 Stage-1 map = 29%,  reduce = 9%
2016-04-12 16:03:21,047 Stage-1 map = 33%,  reduce = 9%
2016-04-12 16:03:24,081 Stage-1 map = 37%,  reduce = 9%
2016-04-12 16:03:27,113 Stage-1 map = 40%,  reduce = 9%
2016-04-12 16:03:36,195 Stage-1 map = 43%,  reduce = 13%
2016-04-12 16:03:39,225 Stage-1 map = 47%,  reduce = 13%
2016-04-12 16:03:42,255 Stage-1 map = 51%,  reduce = 13%
2016-04-12 16:03:45,288 Stage-1 map = 53%,  reduce = 13%
2016-04-12 16:03:51,341 Stage-1 map = 55%,  reduce = 16%
2016-04-12 16:03:54,373 Stage-1 map = 57%,  reduce = 18%
2016-04-12 16:03:57,403 Stage-1 map = 60%,  reduce = 18%
2016-04-12 16:04:00,431 Stage-1 map = 63%,  reduce = 18%
2016-04-12 16:04:03,459 Stage-1 map = 65%,  reduce = 18%
2016-04-12 16:04:06,489 Stage-1 map = 69%,  reduce = 19%
2016-04-12 16:04:08,510 Stage-1 map = 70%,  reduce = 19%
2016-04-12 16:04:09,523 Stage-1 map = 70%,  reduce = 21%
2016-04-12 16:04:11,543 Stage-1 map = 72%,  reduce = 21%
2016-04-12 16:04:14,571 Stage-1 map = 75%,  reduce = 22%
2016-04-12 16:04:17,599 Stage-1 map = 77%,  reduce = 22%
2016-04-12 16:04:20,630 Stage-1 map = 82%,  reduce = 23%
2016-04-12 16:04:23,655 Stage-1 map = 83%,  reduce = 23%
2016-04-12 16:04:26,679 Stage-1 map = 87%,  reduce = 25%
2016-04-12 16:04:29,704 Stage-1 map = 87%,  reduce = 27%
2016-04-12 16:04:32,728 Stage-1 map = 95%,  reduce = 27%
2016-04-12 16:04:35,751 Stage-1 map = 96%,  reduce = 28%
2016-04-12 16:04:38,774 Stage-1 map = 100%,  reduce = 28%
2016-04-12 16:04:41,797 Stage-1 map = 100%,  reduce = 31%
2016-04-12 16:04:44,821 Stage-1 map = 100%,  reduce = 50%
2016-04-12 16:04:47,844 Stage-1 map = 100%,  reduce = 68%
2016-04-12 16:04:50,866 Stage-1 map = 100%,  reduce = 72%
2016-04-12 16:04:53,887 Stage-1 map = 100%,  reduce = 82%
2016-04-12 16:04:56,909 Stage-1 map = 100%,  reduce = 92%
2016-04-12 16:04:59,932 Stage-1 map = 100%,  reduce = 97%
2016-04-12 16:05:02,955 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121602_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121602_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121602_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121602_0002
2016-04-12 16:05:12,459 Stage-2 map = 0%,  reduce = 0%
2016-04-12 16:05:21,505 Stage-2 map = 100%,  reduce = 0%
2016-04-12 16:05:30,562 Stage-2 map = 100%,  reduce = 33%
2016-04-12 16:05:33,584 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121602_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121602_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121602_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121602_0003
2016-04-12 16:05:42,167 Stage-3 map = 0%,  reduce = 0%
2016-04-12 16:05:45,185 Stage-3 map = 100%,  reduce = 0%
2016-04-12 16:05:54,240 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121602_0003
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121602_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121602_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121602_0004
2016-04-12 16:06:02,776 Stage-4 map = 0%,  reduce = 0%
2016-04-12 16:06:05,793 Stage-4 map = 100%,  reduce = 0%
2016-04-12 16:06:14,847 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121602_0004
Loading data to table q13_customer_distribution
46 Rows loaded to q13_customer_distribution
OK
Time taken: 231.772 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121606_1487180118.txt
OK
Time taken: 3.488 seconds
OK
Time taken: 0.065 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q13_customer_distribution. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121606_426792635.txt
OK
Time taken: 2.874 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.436 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.032 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121606_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121606_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121606_0001
2016-04-12 16:06:56,061 Stage-1 map = 0%,  reduce = 0%
2016-04-12 16:07:05,131 Stage-1 map = 3%,  reduce = 0%
2016-04-12 16:07:08,153 Stage-1 map = 4%,  reduce = 0%
2016-04-12 16:07:11,177 Stage-1 map = 10%,  reduce = 0%
2016-04-12 16:07:14,210 Stage-1 map = 11%,  reduce = 0%
2016-04-12 16:07:17,245 Stage-1 map = 13%,  reduce = 0%
2016-04-12 16:07:23,308 Stage-1 map = 14%,  reduce = 2%
2016-04-12 16:07:26,342 Stage-1 map = 18%,  reduce = 4%
2016-04-12 16:07:29,374 Stage-1 map = 22%,  reduce = 4%
2016-04-12 16:07:32,409 Stage-1 map = 25%,  reduce = 4%
2016-04-12 16:07:35,444 Stage-1 map = 27%,  reduce = 4%
2016-04-12 16:07:38,481 Stage-1 map = 27%,  reduce = 7%
2016-04-12 16:07:41,515 Stage-1 map = 29%,  reduce = 9%
2016-04-12 16:07:44,549 Stage-1 map = 34%,  reduce = 9%
2016-04-12 16:07:47,582 Stage-1 map = 38%,  reduce = 9%
2016-04-12 16:07:50,616 Stage-1 map = 40%,  reduce = 9%
2016-04-12 16:07:56,677 Stage-1 map = 40%,  reduce = 11%
2016-04-12 16:07:59,707 Stage-1 map = 43%,  reduce = 13%
2016-04-12 16:08:02,738 Stage-1 map = 47%,  reduce = 13%
2016-04-12 16:08:05,771 Stage-1 map = 51%,  reduce = 13%
2016-04-12 16:08:08,802 Stage-1 map = 53%,  reduce = 13%
2016-04-12 16:08:14,855 Stage-1 map = 53%,  reduce = 16%
2016-04-12 16:08:17,885 Stage-1 map = 56%,  reduce = 18%
2016-04-12 16:08:20,915 Stage-1 map = 60%,  reduce = 18%
2016-04-12 16:08:23,942 Stage-1 map = 63%,  reduce = 18%
2016-04-12 16:08:26,971 Stage-1 map = 67%,  reduce = 18%
2016-04-12 16:08:30,000 Stage-1 map = 67%,  reduce = 20%
2016-04-12 16:08:33,028 Stage-1 map = 67%,  reduce = 22%
2016-04-12 16:08:36,055 Stage-1 map = 72%,  reduce = 22%
2016-04-12 16:08:39,083 Stage-1 map = 75%,  reduce = 22%
2016-04-12 16:08:42,111 Stage-1 map = 80%,  reduce = 22%
2016-04-12 16:08:47,150 Stage-1 map = 80%,  reduce = 23%
2016-04-12 16:08:51,181 Stage-1 map = 80%,  reduce = 24%
2016-04-12 16:08:54,206 Stage-1 map = 89%,  reduce = 27%
2016-04-12 16:08:56,222 Stage-1 map = 91%,  reduce = 27%
2016-04-12 16:08:57,232 Stage-1 map = 93%,  reduce = 27%
2016-04-12 16:09:02,268 Stage-1 map = 97%,  reduce = 27%
2016-04-12 16:09:05,290 Stage-1 map = 100%,  reduce = 29%
2016-04-12 16:09:08,317 Stage-1 map = 100%,  reduce = 32%
2016-04-12 16:09:11,340 Stage-1 map = 100%,  reduce = 49%
2016-04-12 16:09:14,361 Stage-1 map = 100%,  reduce = 54%
2016-04-12 16:09:17,382 Stage-1 map = 100%,  reduce = 80%
2016-04-12 16:09:20,403 Stage-1 map = 100%,  reduce = 83%
2016-04-12 16:09:23,425 Stage-1 map = 100%,  reduce = 92%
2016-04-12 16:09:26,449 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121606_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121606_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121606_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121606_0002
2016-04-12 16:09:35,999 Stage-2 map = 0%,  reduce = 0%
2016-04-12 16:09:45,045 Stage-2 map = 100%,  reduce = 0%
2016-04-12 16:09:54,102 Stage-2 map = 100%,  reduce = 33%
2016-04-12 16:09:57,123 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121606_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121606_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121606_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121606_0003
2016-04-12 16:10:05,677 Stage-3 map = 0%,  reduce = 0%
2016-04-12 16:10:08,695 Stage-3 map = 100%,  reduce = 0%
2016-04-12 16:10:17,751 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121606_0003
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121606_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121606_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121606_0004
2016-04-12 16:10:26,363 Stage-4 map = 0%,  reduce = 0%
2016-04-12 16:10:29,381 Stage-4 map = 100%,  reduce = 0%
2016-04-12 16:10:38,434 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121606_0004
Loading data to table q13_customer_distribution
46 Rows loaded to q13_customer_distribution
OK
Time taken: 231.788 seconds
