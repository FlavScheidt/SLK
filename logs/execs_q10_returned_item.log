Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121353_441292975.txt
OK
Time taken: 3.469 seconds
OK
Time taken: 0.082 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.066 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q10_returned_item. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121353_782064008.txt
OK
Time taken: 2.828 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.422 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121353_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121353_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121353_0001
2016-04-12 13:54:05,712 Stage-1 map = 0%,  reduce = 0%
2016-04-12 13:54:14,778 Stage-1 map = 15%,  reduce = 0%
2016-04-12 13:54:17,817 Stage-1 map = 27%,  reduce = 0%
2016-04-12 13:54:20,854 Stage-1 map = 34%,  reduce = 0%
2016-04-12 13:54:23,887 Stage-1 map = 44%,  reduce = 0%
2016-04-12 13:54:26,920 Stage-1 map = 52%,  reduce = 0%
2016-04-12 13:54:29,953 Stage-1 map = 66%,  reduce = 7%
2016-04-12 13:54:31,978 Stage-1 map = 72%,  reduce = 7%
2016-04-12 13:54:32,993 Stage-1 map = 75%,  reduce = 7%
2016-04-12 13:54:35,018 Stage-1 map = 88%,  reduce = 7%
2016-04-12 13:54:38,052 Stage-1 map = 100%,  reduce = 7%
2016-04-12 13:54:44,111 Stage-1 map = 100%,  reduce = 23%
2016-04-12 13:54:47,145 Stage-1 map = 100%,  reduce = 47%
2016-04-12 13:54:50,185 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121353_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121353_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121353_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121353_0002
2016-04-12 13:54:59,767 Stage-2 map = 0%,  reduce = 0%
2016-04-12 13:55:02,790 Stage-2 map = 33%,  reduce = 0%
2016-04-12 13:55:05,817 Stage-2 map = 67%,  reduce = 0%
2016-04-12 13:55:08,845 Stage-2 map = 100%,  reduce = 0%
2016-04-12 13:55:11,873 Stage-2 map = 100%,  reduce = 33%
2016-04-12 13:55:14,900 Stage-2 map = 100%,  reduce = 77%
2016-04-12 13:55:17,928 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121353_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121353_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121353_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121353_0003
2016-04-12 13:55:26,572 Stage-3 map = 0%,  reduce = 0%
2016-04-12 13:55:35,623 Stage-3 map = 4%,  reduce = 0%
2016-04-12 13:55:38,643 Stage-3 map = 9%,  reduce = 0%
2016-04-12 13:55:41,662 Stage-3 map = 14%,  reduce = 0%
2016-04-12 13:55:44,684 Stage-3 map = 18%,  reduce = 0%
2016-04-12 13:55:47,711 Stage-3 map = 22%,  reduce = 0%
2016-04-12 13:55:50,739 Stage-3 map = 23%,  reduce = 0%
2016-04-12 13:55:53,768 Stage-3 map = 24%,  reduce = 0%
2016-04-12 13:55:56,811 Stage-3 map = 26%,  reduce = 2%
2016-04-12 13:55:59,839 Stage-3 map = 28%,  reduce = 2%
2016-04-12 13:56:02,870 Stage-3 map = 30%,  reduce = 2%
2016-04-12 13:56:05,902 Stage-3 map = 33%,  reduce = 3%
2016-04-12 13:56:08,928 Stage-3 map = 37%,  reduce = 4%
2016-04-12 13:56:11,956 Stage-3 map = 40%,  reduce = 4%
2016-04-12 13:56:14,981 Stage-3 map = 44%,  reduce = 4%
2016-04-12 13:56:18,005 Stage-3 map = 47%,  reduce = 4%
2016-04-12 13:56:21,030 Stage-3 map = 49%,  reduce = 4%
2016-04-12 13:56:24,055 Stage-3 map = 50%,  reduce = 5%
2016-04-12 13:56:27,078 Stage-3 map = 53%,  reduce = 6%
2016-04-12 13:56:30,102 Stage-3 map = 55%,  reduce = 7%
2016-04-12 13:56:33,125 Stage-3 map = 58%,  reduce = 8%
2016-04-12 13:56:36,148 Stage-3 map = 61%,  reduce = 8%
2016-04-12 13:56:39,172 Stage-3 map = 65%,  reduce = 8%
2016-04-12 13:56:42,193 Stage-3 map = 68%,  reduce = 8%
2016-04-12 13:56:44,210 Stage-3 map = 70%,  reduce = 9%
2016-04-12 13:56:45,220 Stage-3 map = 71%,  reduce = 9%
2016-04-12 13:56:47,235 Stage-3 map = 72%,  reduce = 9%
2016-04-12 13:56:48,245 Stage-3 map = 73%,  reduce = 9%
2016-04-12 13:56:50,260 Stage-3 map = 74%,  reduce = 10%
2016-04-12 13:56:53,281 Stage-3 map = 77%,  reduce = 10%
2016-04-12 13:56:56,302 Stage-3 map = 79%,  reduce = 11%
2016-04-12 13:56:59,322 Stage-3 map = 81%,  reduce = 11%
2016-04-12 13:57:02,344 Stage-3 map = 83%,  reduce = 12%
2016-04-12 13:57:05,365 Stage-3 map = 87%,  reduce = 13%
2016-04-12 13:57:08,386 Stage-3 map = 90%,  reduce = 13%
2016-04-12 13:57:11,407 Stage-3 map = 93%,  reduce = 13%
2016-04-12 13:57:14,428 Stage-3 map = 95%,  reduce = 13%
2016-04-12 13:57:17,449 Stage-3 map = 98%,  reduce = 13%
2016-04-12 13:57:20,470 Stage-3 map = 100%,  reduce = 14%
2016-04-12 13:57:26,507 Stage-3 map = 100%,  reduce = 15%
2016-04-12 13:57:29,528 Stage-3 map = 100%,  reduce = 20%
2016-04-12 13:57:32,548 Stage-3 map = 100%,  reduce = 36%
2016-04-12 13:57:35,569 Stage-3 map = 100%,  reduce = 44%
2016-04-12 13:57:38,591 Stage-3 map = 100%,  reduce = 50%
2016-04-12 13:57:44,629 Stage-3 map = 100%,  reduce = 54%
2016-04-12 13:57:47,650 Stage-3 map = 100%,  reduce = 66%
2016-04-12 13:57:50,671 Stage-3 map = 100%,  reduce = 76%
2016-04-12 13:57:53,692 Stage-3 map = 100%,  reduce = 85%
2016-04-12 13:57:56,713 Stage-3 map = 100%,  reduce = 96%
2016-04-12 13:57:59,734 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121353_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121353_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121353_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121353_0004
2016-04-12 13:58:09,224 Stage-4 map = 0%,  reduce = 0%
2016-04-12 13:58:15,254 Stage-4 map = 13%,  reduce = 0%
2016-04-12 13:58:18,272 Stage-4 map = 50%,  reduce = 0%
2016-04-12 13:58:21,291 Stage-4 map = 63%,  reduce = 0%
2016-04-12 13:58:24,309 Stage-4 map = 75%,  reduce = 17%
2016-04-12 13:58:27,327 Stage-4 map = 100%,  reduce = 17%
2016-04-12 13:58:33,361 Stage-4 map = 100%,  reduce = 25%
2016-04-12 13:58:38,389 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121353_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121353_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121353_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121353_0005
2016-04-12 13:58:47,823 Stage-5 map = 0%,  reduce = 0%
2016-04-12 13:58:56,866 Stage-5 map = 100%,  reduce = 0%
2016-04-12 13:59:05,915 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604121353_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121353_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121353_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121353_0006
2016-04-12 13:59:15,320 Stage-6 map = 0%,  reduce = 0%
2016-04-12 13:59:18,337 Stage-6 map = 100%,  reduce = 0%
2016-04-12 13:59:27,388 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604121353_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 331.649 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121359_1739386481.txt
OK
Time taken: 3.425 seconds
OK
Time taken: 0.097 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.058 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q10_returned_item. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121400_697264981.txt
OK
Time taken: 2.996 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.43 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.051 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121359_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121359_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121359_0001
2016-04-12 14:00:13,150 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:00:22,221 Stage-1 map = 6%,  reduce = 0%
2016-04-12 14:00:25,247 Stage-1 map = 14%,  reduce = 0%
2016-04-12 14:00:28,272 Stage-1 map = 32%,  reduce = 0%
2016-04-12 14:00:31,306 Stage-1 map = 56%,  reduce = 0%
2016-04-12 14:00:34,339 Stage-1 map = 66%,  reduce = 0%
2016-04-12 14:00:37,375 Stage-1 map = 74%,  reduce = 0%
2016-04-12 14:00:40,410 Stage-1 map = 78%,  reduce = 20%
2016-04-12 14:00:43,443 Stage-1 map = 87%,  reduce = 20%
2016-04-12 14:00:49,504 Stage-1 map = 100%,  reduce = 20%
2016-04-12 14:00:55,566 Stage-1 map = 100%,  reduce = 47%
2016-04-12 14:00:58,599 Stage-1 map = 100%,  reduce = 60%
2016-04-12 14:01:01,638 Stage-1 map = 100%,  reduce = 99%
2016-04-12 14:01:04,674 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121359_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121359_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121359_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121359_0002
2016-04-12 14:01:13,611 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:01:19,649 Stage-2 map = 67%,  reduce = 0%
2016-04-12 14:01:22,676 Stage-2 map = 100%,  reduce = 0%
2016-04-12 14:01:28,726 Stage-2 map = 100%,  reduce = 33%
2016-04-12 14:01:31,753 Stage-2 map = 100%,  reduce = 77%
2016-04-12 14:01:34,781 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121359_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121359_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121359_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121359_0003
2016-04-12 14:01:43,421 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:01:52,470 Stage-3 map = 6%,  reduce = 0%
2016-04-12 14:01:55,490 Stage-3 map = 12%,  reduce = 0%
2016-04-12 14:01:58,510 Stage-3 map = 17%,  reduce = 0%
2016-04-12 14:02:01,534 Stage-3 map = 20%,  reduce = 0%
2016-04-12 14:02:04,562 Stage-3 map = 22%,  reduce = 0%
2016-04-12 14:02:07,603 Stage-3 map = 24%,  reduce = 0%
2016-04-12 14:02:10,631 Stage-3 map = 26%,  reduce = 1%
2016-04-12 14:02:13,662 Stage-3 map = 27%,  reduce = 2%
2016-04-12 14:02:16,693 Stage-3 map = 30%,  reduce = 2%
2016-04-12 14:02:19,719 Stage-3 map = 32%,  reduce = 3%
2016-04-12 14:02:22,745 Stage-3 map = 36%,  reduce = 4%
2016-04-12 14:02:25,773 Stage-3 map = 39%,  reduce = 4%
2016-04-12 14:02:28,797 Stage-3 map = 43%,  reduce = 4%
2016-04-12 14:02:31,820 Stage-3 map = 45%,  reduce = 4%
2016-04-12 14:02:34,844 Stage-3 map = 48%,  reduce = 4%
2016-04-12 14:02:37,867 Stage-3 map = 50%,  reduce = 5%
2016-04-12 14:02:40,890 Stage-3 map = 52%,  reduce = 5%
2016-04-12 14:02:43,916 Stage-3 map = 54%,  reduce = 7%
2016-04-12 14:02:46,938 Stage-3 map = 56%,  reduce = 7%
2016-04-12 14:02:49,960 Stage-3 map = 59%,  reduce = 7%
2016-04-12 14:02:52,982 Stage-3 map = 63%,  reduce = 8%
2016-04-12 14:02:56,008 Stage-3 map = 67%,  reduce = 8%
2016-04-12 14:02:59,029 Stage-3 map = 70%,  reduce = 8%
2016-04-12 14:03:02,051 Stage-3 map = 72%,  reduce = 8%
2016-04-12 14:03:05,073 Stage-3 map = 74%,  reduce = 9%
2016-04-12 14:03:08,094 Stage-3 map = 77%,  reduce = 10%
2016-04-12 14:03:11,116 Stage-3 map = 79%,  reduce = 10%
2016-04-12 14:03:14,138 Stage-3 map = 81%,  reduce = 11%
2016-04-12 14:03:17,159 Stage-3 map = 83%,  reduce = 12%
2016-04-12 14:03:20,180 Stage-3 map = 87%,  reduce = 12%
2016-04-12 14:03:23,202 Stage-3 map = 91%,  reduce = 12%
2016-04-12 14:03:26,223 Stage-3 map = 94%,  reduce = 12%
2016-04-12 14:03:29,243 Stage-3 map = 97%,  reduce = 12%
2016-04-12 14:03:32,265 Stage-3 map = 100%,  reduce = 13%
2016-04-12 14:03:35,285 Stage-3 map = 100%,  reduce = 14%
2016-04-12 14:03:38,306 Stage-3 map = 100%,  reduce = 15%
2016-04-12 14:03:41,326 Stage-3 map = 100%,  reduce = 20%
2016-04-12 14:03:44,347 Stage-3 map = 100%,  reduce = 36%
2016-04-12 14:03:47,368 Stage-3 map = 100%,  reduce = 42%
2016-04-12 14:03:50,389 Stage-3 map = 100%,  reduce = 47%
2016-04-12 14:03:53,411 Stage-3 map = 100%,  reduce = 50%
2016-04-12 14:03:59,450 Stage-3 map = 100%,  reduce = 57%
2016-04-12 14:04:02,470 Stage-3 map = 100%,  reduce = 66%
2016-04-12 14:04:05,491 Stage-3 map = 100%,  reduce = 76%
2016-04-12 14:04:07,506 Stage-3 map = 100%,  reduce = 89%
2016-04-12 14:04:10,527 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121359_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121359_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121359_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121359_0004
2016-04-12 14:04:19,994 Stage-4 map = 0%,  reduce = 0%
2016-04-12 14:04:29,037 Stage-4 map = 50%,  reduce = 0%
2016-04-12 14:04:38,085 Stage-4 map = 100%,  reduce = 17%
2016-04-12 14:04:47,134 Stage-4 map = 100%,  reduce = 93%
2016-04-12 14:04:50,153 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121359_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121359_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121359_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121359_0005
2016-04-12 14:04:58,670 Stage-5 map = 0%,  reduce = 0%
2016-04-12 14:05:07,714 Stage-5 map = 100%,  reduce = 0%
2016-04-12 14:05:16,763 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604121359_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121359_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121359_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121359_0006
2016-04-12 14:05:26,203 Stage-6 map = 0%,  reduce = 0%
2016-04-12 14:05:29,219 Stage-6 map = 100%,  reduce = 0%
2016-04-12 14:05:38,270 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604121359_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 334.19 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121406_1337706331.txt
OK
Time taken: 3.477 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.058 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q10_returned_item. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121406_1805819025.txt
OK
Time taken: 2.981 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.013 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.447 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121406_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121406_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121406_0001
2016-04-12 14:06:25,849 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:06:34,914 Stage-1 map = 14%,  reduce = 0%
2016-04-12 14:06:37,952 Stage-1 map = 28%,  reduce = 0%
2016-04-12 14:06:39,978 Stage-1 map = 32%,  reduce = 0%
2016-04-12 14:06:40,994 Stage-1 map = 36%,  reduce = 0%
2016-04-12 14:06:43,027 Stage-1 map = 46%,  reduce = 0%
2016-04-12 14:06:46,061 Stage-1 map = 54%,  reduce = 0%
2016-04-12 14:06:49,094 Stage-1 map = 66%,  reduce = 7%
2016-04-12 14:06:52,128 Stage-1 map = 76%,  reduce = 7%
2016-04-12 14:06:55,161 Stage-1 map = 86%,  reduce = 7%
2016-04-12 14:06:58,196 Stage-1 map = 96%,  reduce = 7%
2016-04-12 14:07:01,229 Stage-1 map = 100%,  reduce = 7%
2016-04-12 14:07:04,263 Stage-1 map = 100%,  reduce = 20%
2016-04-12 14:07:10,331 Stage-1 map = 100%,  reduce = 67%
2016-04-12 14:07:13,369 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121406_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121406_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121406_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121406_0002
2016-04-12 14:07:22,883 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:07:25,906 Stage-2 map = 33%,  reduce = 0%
2016-04-12 14:07:28,934 Stage-2 map = 67%,  reduce = 0%
2016-04-12 14:07:31,962 Stage-2 map = 100%,  reduce = 0%
2016-04-12 14:07:34,989 Stage-2 map = 100%,  reduce = 33%
2016-04-12 14:07:38,016 Stage-2 map = 100%,  reduce = 78%
2016-04-12 14:07:41,046 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121406_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121406_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121406_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121406_0003
2016-04-12 14:07:49,700 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:07:58,750 Stage-3 map = 4%,  reduce = 0%
2016-04-12 14:08:01,769 Stage-3 map = 9%,  reduce = 0%
2016-04-12 14:08:04,801 Stage-3 map = 14%,  reduce = 0%
2016-04-12 14:08:07,823 Stage-3 map = 18%,  reduce = 0%
2016-04-12 14:08:10,850 Stage-3 map = 21%,  reduce = 0%
2016-04-12 14:08:13,879 Stage-3 map = 23%,  reduce = 0%
2016-04-12 14:08:16,906 Stage-3 map = 24%,  reduce = 0%
2016-04-12 14:08:19,933 Stage-3 map = 26%,  reduce = 1%
2016-04-12 14:08:22,962 Stage-3 map = 29%,  reduce = 2%
2016-04-12 14:08:25,993 Stage-3 map = 31%,  reduce = 2%
2016-04-12 14:08:29,021 Stage-3 map = 33%,  reduce = 3%
2016-04-12 14:08:32,047 Stage-3 map = 37%,  reduce = 4%
2016-04-12 14:08:35,072 Stage-3 map = 41%,  reduce = 4%
2016-04-12 14:08:38,096 Stage-3 map = 44%,  reduce = 4%
2016-04-12 14:08:41,120 Stage-3 map = 47%,  reduce = 4%
2016-04-12 14:08:44,145 Stage-3 map = 49%,  reduce = 4%
2016-04-12 14:08:47,173 Stage-3 map = 52%,  reduce = 5%
2016-04-12 14:08:49,189 Stage-3 map = 54%,  reduce = 6%
2016-04-12 14:08:52,213 Stage-3 map = 56%,  reduce = 7%
2016-04-12 14:08:55,236 Stage-3 map = 58%,  reduce = 8%
2016-04-12 14:08:58,257 Stage-3 map = 62%,  reduce = 8%
2016-04-12 14:09:01,278 Stage-3 map = 65%,  reduce = 8%
2016-04-12 14:09:04,300 Stage-3 map = 69%,  reduce = 8%
2016-04-12 14:09:07,321 Stage-3 map = 71%,  reduce = 9%
2016-04-12 14:09:10,342 Stage-3 map = 73%,  reduce = 9%
2016-04-12 14:09:13,363 Stage-3 map = 76%,  reduce = 9%
2016-04-12 14:09:16,384 Stage-3 map = 78%,  reduce = 10%
2016-04-12 14:09:19,406 Stage-3 map = 80%,  reduce = 10%
2016-04-12 14:09:22,427 Stage-3 map = 82%,  reduce = 10%
2016-04-12 14:09:25,448 Stage-3 map = 85%,  reduce = 12%
2016-04-12 14:09:28,469 Stage-3 map = 89%,  reduce = 12%
2016-04-12 14:09:31,489 Stage-3 map = 93%,  reduce = 12%
2016-04-12 14:09:34,511 Stage-3 map = 95%,  reduce = 12%
2016-04-12 14:09:37,531 Stage-3 map = 98%,  reduce = 12%
2016-04-12 14:09:40,551 Stage-3 map = 99%,  reduce = 14%
2016-04-12 14:09:43,572 Stage-3 map = 100%,  reduce = 15%
2016-04-12 14:09:52,627 Stage-3 map = 100%,  reduce = 16%
2016-04-12 14:09:55,648 Stage-3 map = 100%,  reduce = 22%
2016-04-12 14:09:58,668 Stage-3 map = 100%,  reduce = 31%
2016-04-12 14:10:01,689 Stage-3 map = 100%,  reduce = 43%
2016-04-12 14:10:04,711 Stage-3 map = 100%,  reduce = 50%
2016-04-12 14:10:10,749 Stage-3 map = 100%,  reduce = 54%
2016-04-12 14:10:13,774 Stage-3 map = 100%,  reduce = 66%
2016-04-12 14:10:16,795 Stage-3 map = 100%,  reduce = 76%
2016-04-12 14:10:19,816 Stage-3 map = 100%,  reduce = 84%
2016-04-12 14:10:22,837 Stage-3 map = 100%,  reduce = 96%
2016-04-12 14:10:25,858 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121406_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121406_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121406_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121406_0004
2016-04-12 14:10:34,325 Stage-4 map = 0%,  reduce = 0%
2016-04-12 14:10:43,367 Stage-4 map = 25%,  reduce = 0%
2016-04-12 14:10:44,376 Stage-4 map = 50%,  reduce = 0%
2016-04-12 14:10:52,419 Stage-4 map = 75%,  reduce = 17%
2016-04-12 14:10:53,427 Stage-4 map = 100%,  reduce = 17%
2016-04-12 14:11:01,470 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121406_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121406_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121406_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121406_0005
2016-04-12 14:11:10,943 Stage-5 map = 0%,  reduce = 0%
2016-04-12 14:11:19,984 Stage-5 map = 100%,  reduce = 0%
2016-04-12 14:11:29,031 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604121406_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121406_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121406_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121406_0006
2016-04-12 14:11:37,494 Stage-6 map = 0%,  reduce = 0%
2016-04-12 14:11:40,510 Stage-6 map = 100%,  reduce = 0%
2016-04-12 14:11:49,559 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604121406_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 333.615 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121412_2093570120.txt
OK
Time taken: 3.461 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.067 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q10_returned_item. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121412_170829834.txt
OK
Time taken: 2.868 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.436 seconds
OK
Time taken: 0.031 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121412_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121412_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121412_0001
2016-04-12 14:12:37,615 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:12:46,681 Stage-1 map = 6%,  reduce = 0%
2016-04-12 14:12:49,705 Stage-1 map = 14%,  reduce = 0%
2016-04-12 14:12:52,730 Stage-1 map = 33%,  reduce = 0%
2016-04-12 14:12:55,763 Stage-1 map = 57%,  reduce = 0%
2016-04-12 14:12:58,796 Stage-1 map = 68%,  reduce = 0%
2016-04-12 14:13:01,833 Stage-1 map = 75%,  reduce = 0%
2016-04-12 14:13:04,865 Stage-1 map = 78%,  reduce = 0%
2016-04-12 14:13:06,890 Stage-1 map = 78%,  reduce = 10%
2016-04-12 14:13:07,905 Stage-1 map = 83%,  reduce = 20%
2016-04-12 14:13:09,930 Stage-1 map = 88%,  reduce = 20%
2016-04-12 14:13:12,966 Stage-1 map = 100%,  reduce = 20%
2016-04-12 14:13:22,066 Stage-1 map = 100%,  reduce = 93%
2016-04-12 14:13:25,106 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121412_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121412_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121412_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121412_0002
2016-04-12 14:13:34,732 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:13:37,755 Stage-2 map = 33%,  reduce = 0%
2016-04-12 14:13:40,782 Stage-2 map = 67%,  reduce = 0%
2016-04-12 14:13:43,810 Stage-2 map = 100%,  reduce = 0%
2016-04-12 14:13:46,837 Stage-2 map = 100%,  reduce = 33%
2016-04-12 14:13:49,865 Stage-2 map = 100%,  reduce = 77%
2016-04-12 14:13:52,894 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121412_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121412_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121412_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121412_0003
2016-04-12 14:14:01,577 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:14:10,630 Stage-3 map = 6%,  reduce = 0%
2016-04-12 14:14:13,649 Stage-3 map = 12%,  reduce = 0%
2016-04-12 14:14:16,671 Stage-3 map = 17%,  reduce = 0%
2016-04-12 14:14:19,715 Stage-3 map = 19%,  reduce = 0%
2016-04-12 14:14:22,743 Stage-3 map = 21%,  reduce = 0%
2016-04-12 14:14:25,771 Stage-3 map = 23%,  reduce = 1%
2016-04-12 14:14:28,798 Stage-3 map = 26%,  reduce = 2%
2016-04-12 14:14:31,827 Stage-3 map = 29%,  reduce = 2%
2016-04-12 14:14:34,858 Stage-3 map = 31%,  reduce = 2%
2016-04-12 14:14:37,888 Stage-3 map = 33%,  reduce = 3%
2016-04-12 14:14:40,915 Stage-3 map = 36%,  reduce = 3%
2016-04-12 14:14:43,947 Stage-3 map = 39%,  reduce = 4%
2016-04-12 14:14:46,973 Stage-3 map = 42%,  reduce = 4%
2016-04-12 14:14:49,997 Stage-3 map = 44%,  reduce = 4%
2016-04-12 14:14:53,022 Stage-3 map = 46%,  reduce = 4%
2016-04-12 14:14:56,044 Stage-3 map = 48%,  reduce = 5%
2016-04-12 14:14:59,066 Stage-3 map = 52%,  reduce = 6%
2016-04-12 14:15:01,083 Stage-3 map = 53%,  reduce = 6%
2016-04-12 14:15:02,093 Stage-3 map = 54%,  reduce = 6%
2016-04-12 14:15:04,110 Stage-3 map = 55%,  reduce = 7%
2016-04-12 14:15:05,133 Stage-3 map = 56%,  reduce = 7%
2016-04-12 14:15:07,148 Stage-3 map = 59%,  reduce = 7%
2016-04-12 14:15:10,169 Stage-3 map = 62%,  reduce = 8%
2016-04-12 14:15:13,191 Stage-3 map = 64%,  reduce = 8%
2016-04-12 14:15:16,212 Stage-3 map = 65%,  reduce = 9%
2016-04-12 14:15:19,233 Stage-3 map = 68%,  reduce = 10%
2016-04-12 14:15:22,254 Stage-3 map = 70%,  reduce = 10%
2016-04-12 14:15:25,275 Stage-3 map = 74%,  reduce = 10%
2016-04-12 14:15:28,295 Stage-3 map = 77%,  reduce = 10%
2016-04-12 14:15:31,316 Stage-3 map = 80%,  reduce = 10%
2016-04-12 14:15:34,338 Stage-3 map = 83%,  reduce = 11%
2016-04-12 14:15:37,359 Stage-3 map = 86%,  reduce = 11%
2016-04-12 14:15:40,379 Stage-3 map = 88%,  reduce = 11%
2016-04-12 14:15:43,400 Stage-3 map = 90%,  reduce = 11%
2016-04-12 14:15:46,421 Stage-3 map = 92%,  reduce = 12%
2016-04-12 14:15:49,442 Stage-3 map = 95%,  reduce = 14%
2016-04-12 14:15:52,463 Stage-3 map = 96%,  reduce = 14%
2016-04-12 14:15:55,483 Stage-3 map = 97%,  reduce = 15%
2016-04-12 14:15:58,503 Stage-3 map = 99%,  reduce = 15%
2016-04-12 14:16:01,523 Stage-3 map = 100%,  reduce = 16%
2016-04-12 14:16:07,567 Stage-3 map = 100%,  reduce = 20%
2016-04-12 14:16:10,587 Stage-3 map = 100%,  reduce = 30%
2016-04-12 14:16:13,606 Stage-3 map = 100%,  reduce = 39%
2016-04-12 14:16:16,626 Stage-3 map = 100%,  reduce = 46%
2016-04-12 14:16:19,647 Stage-3 map = 100%,  reduce = 50%
2016-04-12 14:16:25,685 Stage-3 map = 100%,  reduce = 54%
2016-04-12 14:16:28,706 Stage-3 map = 100%,  reduce = 67%
2016-04-12 14:16:31,727 Stage-3 map = 100%,  reduce = 76%
2016-04-12 14:16:34,749 Stage-3 map = 100%,  reduce = 85%
2016-04-12 14:16:37,770 Stage-3 map = 100%,  reduce = 96%
2016-04-12 14:16:40,790 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121412_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121412_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121412_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121412_0004
2016-04-12 14:16:50,246 Stage-4 map = 0%,  reduce = 0%
2016-04-12 14:16:59,288 Stage-4 map = 50%,  reduce = 0%
2016-04-12 14:17:07,332 Stage-4 map = 100%,  reduce = 17%
2016-04-12 14:17:16,381 Stage-4 map = 100%,  reduce = 97%
2016-04-12 14:17:19,398 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121412_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121412_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121412_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121412_0005
2016-04-12 14:17:28,825 Stage-5 map = 0%,  reduce = 0%
2016-04-12 14:17:37,868 Stage-5 map = 100%,  reduce = 0%
2016-04-12 14:17:46,916 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604121412_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121412_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121412_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121412_0006
2016-04-12 14:17:55,370 Stage-6 map = 0%,  reduce = 0%
2016-04-12 14:17:58,392 Stage-6 map = 100%,  reduce = 0%
2016-04-12 14:18:07,441 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604121412_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 339.923 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121418_1639532921.txt
OK
Time taken: 3.376 seconds
OK
Time taken: 0.086 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.067 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q10_returned_item. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121418_1727835752.txt
OK
Time taken: 2.938 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.418 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121418_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121418_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121418_0001
2016-04-12 14:18:55,293 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:19:04,362 Stage-1 map = 6%,  reduce = 0%
2016-04-12 14:19:07,387 Stage-1 map = 14%,  reduce = 0%
2016-04-12 14:19:10,411 Stage-1 map = 31%,  reduce = 0%
2016-04-12 14:19:13,446 Stage-1 map = 57%,  reduce = 0%
2016-04-12 14:19:16,480 Stage-1 map = 67%,  reduce = 0%
2016-04-12 14:19:19,517 Stage-1 map = 74%,  reduce = 0%
2016-04-12 14:19:22,551 Stage-1 map = 78%,  reduce = 10%
2016-04-12 14:19:25,584 Stage-1 map = 83%,  reduce = 20%
2016-04-12 14:19:27,610 Stage-1 map = 87%,  reduce = 20%
2016-04-12 14:19:30,645 Stage-1 map = 100%,  reduce = 20%
2016-04-12 14:19:36,707 Stage-1 map = 100%,  reduce = 79%
2016-04-12 14:19:39,749 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121418_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121418_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121418_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121418_0002
2016-04-12 14:19:48,885 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:19:54,927 Stage-2 map = 67%,  reduce = 0%
2016-04-12 14:19:57,957 Stage-2 map = 100%,  reduce = 0%
2016-04-12 14:20:04,007 Stage-2 map = 100%,  reduce = 33%
2016-04-12 14:20:07,034 Stage-2 map = 100%,  reduce = 76%
2016-04-12 14:20:10,064 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121418_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121418_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121418_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121418_0003
2016-04-12 14:20:18,707 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:20:27,765 Stage-3 map = 4%,  reduce = 0%
2016-04-12 14:20:30,785 Stage-3 map = 9%,  reduce = 0%
2016-04-12 14:20:33,805 Stage-3 map = 14%,  reduce = 0%
2016-04-12 14:20:36,829 Stage-3 map = 18%,  reduce = 0%
2016-04-12 14:20:39,857 Stage-3 map = 22%,  reduce = 0%
2016-04-12 14:20:42,901 Stage-3 map = 23%,  reduce = 0%
2016-04-12 14:20:45,930 Stage-3 map = 25%,  reduce = 1%
2016-04-12 14:20:48,958 Stage-3 map = 27%,  reduce = 2%
2016-04-12 14:20:51,988 Stage-3 map = 28%,  reduce = 2%
2016-04-12 14:20:55,019 Stage-3 map = 31%,  reduce = 3%
2016-04-12 14:20:58,047 Stage-3 map = 34%,  reduce = 3%
2016-04-12 14:21:01,073 Stage-3 map = 37%,  reduce = 4%
2016-04-12 14:21:04,099 Stage-3 map = 41%,  reduce = 4%
2016-04-12 14:21:07,127 Stage-3 map = 46%,  reduce = 4%
2016-04-12 14:21:10,153 Stage-3 map = 48%,  reduce = 4%
2016-04-12 14:21:13,182 Stage-3 map = 49%,  reduce = 4%
2016-04-12 14:21:16,207 Stage-3 map = 50%,  reduce = 6%
2016-04-12 14:21:19,231 Stage-3 map = 53%,  reduce = 7%
2016-04-12 14:21:22,257 Stage-3 map = 56%,  reduce = 8%
2016-04-12 14:21:25,286 Stage-3 map = 58%,  reduce = 8%
2016-04-12 14:21:28,309 Stage-3 map = 62%,  reduce = 8%
2016-04-12 14:21:31,330 Stage-3 map = 65%,  reduce = 8%
2016-04-12 14:21:34,353 Stage-3 map = 69%,  reduce = 8%
2016-04-12 14:21:37,376 Stage-3 map = 73%,  reduce = 8%
2016-04-12 14:21:40,398 Stage-3 map = 74%,  reduce = 8%
2016-04-12 14:21:43,422 Stage-3 map = 75%,  reduce = 8%
2016-04-12 14:21:46,444 Stage-3 map = 76%,  reduce = 10%
2016-04-12 14:21:49,467 Stage-3 map = 77%,  reduce = 12%
2016-04-12 14:21:52,490 Stage-3 map = 79%,  reduce = 12%
2016-04-12 14:21:55,512 Stage-3 map = 84%,  reduce = 12%
2016-04-12 14:21:58,533 Stage-3 map = 87%,  reduce = 12%
2016-04-12 14:22:01,554 Stage-3 map = 90%,  reduce = 12%
2016-04-12 14:22:04,576 Stage-3 map = 93%,  reduce = 12%
2016-04-12 14:22:07,597 Stage-3 map = 97%,  reduce = 12%
2016-04-12 14:22:10,618 Stage-3 map = 99%,  reduce = 13%
2016-04-12 14:22:13,638 Stage-3 map = 100%,  reduce = 14%
2016-04-12 14:22:19,676 Stage-3 map = 100%,  reduce = 19%
2016-04-12 14:22:22,697 Stage-3 map = 100%,  reduce = 25%
2016-04-12 14:22:25,718 Stage-3 map = 100%,  reduce = 39%
2016-04-12 14:22:28,740 Stage-3 map = 100%,  reduce = 46%
2016-04-12 14:22:31,763 Stage-3 map = 100%,  reduce = 50%
2016-04-12 14:22:37,804 Stage-3 map = 100%,  reduce = 54%
2016-04-12 14:22:40,824 Stage-3 map = 100%,  reduce = 65%
2016-04-12 14:22:43,844 Stage-3 map = 100%,  reduce = 73%
2016-04-12 14:22:46,866 Stage-3 map = 100%,  reduce = 84%
2016-04-12 14:22:49,889 Stage-3 map = 100%,  reduce = 94%
2016-04-12 14:22:51,905 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121418_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121418_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121418_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121418_0004
2016-04-12 14:23:01,451 Stage-4 map = 0%,  reduce = 0%
2016-04-12 14:23:07,481 Stage-4 map = 13%,  reduce = 0%
2016-04-12 14:23:10,500 Stage-4 map = 50%,  reduce = 0%
2016-04-12 14:23:13,519 Stage-4 map = 63%,  reduce = 0%
2016-04-12 14:23:16,537 Stage-4 map = 75%,  reduce = 17%
2016-04-12 14:23:19,556 Stage-4 map = 100%,  reduce = 17%
2016-04-12 14:23:25,591 Stage-4 map = 100%,  reduce = 25%
2016-04-12 14:23:31,626 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121418_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121418_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121418_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121418_0005
2016-04-12 14:23:40,093 Stage-5 map = 0%,  reduce = 0%
2016-04-12 14:23:49,137 Stage-5 map = 100%,  reduce = 0%
2016-04-12 14:23:58,188 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604121418_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121418_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121418_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121418_0006
2016-04-12 14:24:07,659 Stage-6 map = 0%,  reduce = 0%
2016-04-12 14:24:10,676 Stage-6 map = 100%,  reduce = 0%
2016-04-12 14:24:19,726 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604121418_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 334.465 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121424_1049741410.txt
OK
Time taken: 3.473 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.066 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q10_returned_item. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121424_1371813199.txt
OK
Time taken: 2.847 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.427 seconds
OK
Time taken: 0.039 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121424_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121424_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121424_0001
2016-04-12 14:25:07,751 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:25:16,817 Stage-1 map = 6%,  reduce = 0%
2016-04-12 14:25:19,842 Stage-1 map = 14%,  reduce = 0%
2016-04-12 14:25:22,867 Stage-1 map = 31%,  reduce = 0%
2016-04-12 14:25:25,900 Stage-1 map = 57%,  reduce = 0%
2016-04-12 14:25:28,934 Stage-1 map = 66%,  reduce = 0%
2016-04-12 14:25:31,971 Stage-1 map = 75%,  reduce = 0%
2016-04-12 14:25:33,996 Stage-1 map = 78%,  reduce = 10%
2016-04-12 14:25:37,028 Stage-1 map = 83%,  reduce = 20%
2016-04-12 14:25:40,063 Stage-1 map = 87%,  reduce = 20%
2016-04-12 14:25:43,098 Stage-1 map = 100%,  reduce = 20%
2016-04-12 14:25:49,161 Stage-1 map = 100%,  reduce = 23%
2016-04-12 14:25:52,199 Stage-1 map = 100%,  reduce = 63%
2016-04-12 14:25:55,238 Stage-1 map = 100%,  reduce = 97%
2016-04-12 14:25:58,274 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121424_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121424_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121424_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121424_0002
2016-04-12 14:26:07,350 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:26:10,374 Stage-2 map = 33%,  reduce = 0%
2016-04-12 14:26:16,424 Stage-2 map = 96%,  reduce = 0%
2016-04-12 14:26:19,453 Stage-2 map = 100%,  reduce = 11%
2016-04-12 14:26:28,527 Stage-2 map = 100%,  reduce = 75%
2016-04-12 14:26:31,559 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121424_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121424_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121424_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121424_0003
2016-04-12 14:26:40,214 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:26:49,264 Stage-3 map = 6%,  reduce = 0%
2016-04-12 14:26:52,284 Stage-3 map = 11%,  reduce = 0%
2016-04-12 14:26:55,303 Stage-3 map = 17%,  reduce = 0%
2016-04-12 14:26:58,341 Stage-3 map = 20%,  reduce = 0%
2016-04-12 14:27:01,370 Stage-3 map = 22%,  reduce = 0%
2016-04-12 14:27:04,401 Stage-3 map = 23%,  reduce = 0%
2016-04-12 14:27:07,430 Stage-3 map = 26%,  reduce = 1%
2016-04-12 14:27:10,462 Stage-3 map = 27%,  reduce = 2%
2016-04-12 14:27:13,491 Stage-3 map = 30%,  reduce = 2%
2016-04-12 14:27:16,518 Stage-3 map = 32%,  reduce = 3%
2016-04-12 14:27:19,543 Stage-3 map = 35%,  reduce = 4%
2016-04-12 14:27:22,567 Stage-3 map = 39%,  reduce = 4%
2016-04-12 14:27:25,592 Stage-3 map = 43%,  reduce = 4%
2016-04-12 14:27:28,616 Stage-3 map = 45%,  reduce = 4%
2016-04-12 14:27:31,639 Stage-3 map = 47%,  reduce = 4%
2016-04-12 14:27:34,664 Stage-3 map = 49%,  reduce = 5%
2016-04-12 14:27:37,689 Stage-3 map = 52%,  reduce = 6%
2016-04-12 14:27:40,712 Stage-3 map = 54%,  reduce = 7%
2016-04-12 14:27:43,733 Stage-3 map = 56%,  reduce = 7%
2016-04-12 14:27:46,756 Stage-3 map = 59%,  reduce = 7%
2016-04-12 14:27:49,780 Stage-3 map = 62%,  reduce = 8%
2016-04-12 14:27:52,801 Stage-3 map = 66%,  reduce = 8%
2016-04-12 14:27:55,824 Stage-3 map = 69%,  reduce = 8%
2016-04-12 14:27:58,846 Stage-3 map = 71%,  reduce = 8%
2016-04-12 14:28:01,868 Stage-3 map = 74%,  reduce = 9%
2016-04-12 14:28:04,891 Stage-3 map = 75%,  reduce = 10%
2016-04-12 14:28:07,912 Stage-3 map = 78%,  reduce = 10%
2016-04-12 14:28:10,934 Stage-3 map = 81%,  reduce = 11%
2016-04-12 14:28:13,955 Stage-3 map = 83%,  reduce = 12%
2016-04-12 14:28:16,977 Stage-3 map = 87%,  reduce = 12%
2016-04-12 14:28:19,998 Stage-3 map = 91%,  reduce = 12%
2016-04-12 14:28:23,020 Stage-3 map = 94%,  reduce = 12%
2016-04-12 14:28:26,040 Stage-3 map = 98%,  reduce = 12%
2016-04-12 14:28:29,062 Stage-3 map = 99%,  reduce = 13%
2016-04-12 14:28:32,083 Stage-3 map = 100%,  reduce = 15%
2016-04-12 14:28:41,139 Stage-3 map = 100%,  reduce = 25%
2016-04-12 14:28:44,160 Stage-3 map = 100%,  reduce = 35%
2016-04-12 14:28:47,182 Stage-3 map = 100%,  reduce = 43%
2016-04-12 14:28:50,205 Stage-3 map = 100%,  reduce = 50%
2016-04-12 14:28:56,242 Stage-3 map = 100%,  reduce = 54%
2016-04-12 14:28:59,263 Stage-3 map = 100%,  reduce = 65%
2016-04-12 14:29:02,284 Stage-3 map = 100%,  reduce = 72%
2016-04-12 14:29:05,305 Stage-3 map = 100%,  reduce = 83%
2016-04-12 14:29:08,327 Stage-3 map = 100%,  reduce = 95%
2016-04-12 14:29:11,346 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121424_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121424_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121424_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121424_0004
2016-04-12 14:29:19,847 Stage-4 map = 0%,  reduce = 0%
2016-04-12 14:29:28,890 Stage-4 map = 50%,  reduce = 0%
2016-04-12 14:29:37,940 Stage-4 map = 100%,  reduce = 17%
2016-04-12 14:29:46,990 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121424_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121424_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121424_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121424_0005
2016-04-12 14:29:55,605 Stage-5 map = 0%,  reduce = 0%
2016-04-12 14:30:01,634 Stage-5 map = 100%,  reduce = 0%
2016-04-12 14:30:10,683 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604121424_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121424_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121424_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121424_0006
2016-04-12 14:30:20,188 Stage-6 map = 0%,  reduce = 0%
2016-04-12 14:30:23,205 Stage-6 map = 100%,  reduce = 0%
2016-04-12 14:30:32,256 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604121424_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 334.507 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121431_596948463.txt
OK
Time taken: 3.448 seconds
OK
Time taken: 0.073 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.066 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q10_returned_item. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604121431_377094996.txt
OK
Time taken: 3.038 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.426 seconds
OK
Time taken: 0.053 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 6
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121431_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121431_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121431_0001
2016-04-12 14:31:19,125 Stage-1 map = 0%,  reduce = 0%
2016-04-12 14:31:28,190 Stage-1 map = 17%,  reduce = 0%
2016-04-12 14:31:31,219 Stage-1 map = 38%,  reduce = 0%
2016-04-12 14:31:34,254 Stage-1 map = 54%,  reduce = 0%
2016-04-12 14:31:37,289 Stage-1 map = 64%,  reduce = 0%
2016-04-12 14:31:40,323 Stage-1 map = 73%,  reduce = 0%
2016-04-12 14:31:43,359 Stage-1 map = 80%,  reduce = 0%
2016-04-12 14:31:46,392 Stage-1 map = 83%,  reduce = 13%
2016-04-12 14:31:49,425 Stage-1 map = 87%,  reduce = 13%
2016-04-12 14:31:52,460 Stage-1 map = 100%,  reduce = 13%
2016-04-12 14:31:55,495 Stage-1 map = 100%,  reduce = 20%
2016-04-12 14:32:00,548 Stage-1 map = 100%,  reduce = 53%
2016-04-12 14:32:01,575 Stage-1 map = 100%,  reduce = 93%
2016-04-12 14:32:03,605 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604121431_0001
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121431_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121431_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121431_0002
2016-04-12 14:32:13,255 Stage-2 map = 0%,  reduce = 0%
2016-04-12 14:32:16,279 Stage-2 map = 33%,  reduce = 0%
2016-04-12 14:32:22,329 Stage-2 map = 100%,  reduce = 0%
2016-04-12 14:32:25,356 Stage-2 map = 100%,  reduce = 33%
2016-04-12 14:32:28,383 Stage-2 map = 100%,  reduce = 79%
2016-04-12 14:32:31,413 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604121431_0002
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121431_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121431_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121431_0003
2016-04-12 14:32:40,030 Stage-3 map = 0%,  reduce = 0%
2016-04-12 14:32:49,081 Stage-3 map = 7%,  reduce = 0%
2016-04-12 14:32:52,101 Stage-3 map = 12%,  reduce = 0%
2016-04-12 14:32:55,120 Stage-3 map = 18%,  reduce = 0%
2016-04-12 14:32:58,144 Stage-3 map = 20%,  reduce = 0%
2016-04-12 14:33:01,173 Stage-3 map = 22%,  reduce = 0%
2016-04-12 14:33:04,202 Stage-3 map = 23%,  reduce = 0%
2016-04-12 14:33:07,244 Stage-3 map = 26%,  reduce = 1%
2016-04-12 14:33:10,273 Stage-3 map = 28%,  reduce = 2%
2016-04-12 14:33:13,302 Stage-3 map = 31%,  reduce = 2%
2016-04-12 14:33:16,332 Stage-3 map = 32%,  reduce = 2%
2016-04-12 14:33:19,362 Stage-3 map = 35%,  reduce = 3%
2016-04-12 14:33:22,390 Stage-3 map = 39%,  reduce = 4%
2016-04-12 14:33:25,416 Stage-3 map = 42%,  reduce = 4%
2016-04-12 14:33:28,442 Stage-3 map = 44%,  reduce = 4%
2016-04-12 14:33:31,466 Stage-3 map = 46%,  reduce = 4%
2016-04-12 14:33:34,491 Stage-3 map = 48%,  reduce = 5%
2016-04-12 14:33:37,514 Stage-3 map = 51%,  reduce = 6%
2016-04-12 14:33:40,537 Stage-3 map = 53%,  reduce = 7%
2016-04-12 14:33:43,560 Stage-3 map = 55%,  reduce = 7%
2016-04-12 14:33:46,584 Stage-3 map = 58%,  reduce = 8%
2016-04-12 14:33:49,609 Stage-3 map = 61%,  reduce = 8%
2016-04-12 14:33:52,631 Stage-3 map = 65%,  reduce = 8%
2016-04-12 14:33:55,652 Stage-3 map = 68%,  reduce = 8%
2016-04-12 14:33:58,676 Stage-3 map = 71%,  reduce = 8%
2016-04-12 14:34:00,693 Stage-3 map = 72%,  reduce = 8%
2016-04-12 14:34:01,702 Stage-3 map = 73%,  reduce = 8%
2016-04-12 14:34:03,718 Stage-3 map = 74%,  reduce = 9%
2016-04-12 14:34:06,739 Stage-3 map = 76%,  reduce = 10%
2016-04-12 14:34:07,749 Stage-3 map = 77%,  reduce = 10%
2016-04-12 14:34:09,764 Stage-3 map = 79%,  reduce = 11%
2016-04-12 14:34:12,785 Stage-3 map = 82%,  reduce = 12%
2016-04-12 14:34:15,808 Stage-3 map = 85%,  reduce = 12%
2016-04-12 14:34:18,829 Stage-3 map = 89%,  reduce = 12%
2016-04-12 14:34:21,851 Stage-3 map = 93%,  reduce = 12%
2016-04-12 14:34:24,873 Stage-3 map = 97%,  reduce = 12%
2016-04-12 14:34:27,894 Stage-3 map = 99%,  reduce = 12%
2016-04-12 14:34:30,915 Stage-3 map = 100%,  reduce = 12%
2016-04-12 14:34:33,935 Stage-3 map = 100%,  reduce = 13%
2016-04-12 14:34:36,956 Stage-3 map = 100%,  reduce = 19%
2016-04-12 14:34:39,978 Stage-3 map = 100%,  reduce = 32%
2016-04-12 14:34:43,000 Stage-3 map = 100%,  reduce = 40%
2016-04-12 14:34:46,021 Stage-3 map = 100%,  reduce = 46%
2016-04-12 14:34:49,043 Stage-3 map = 100%,  reduce = 50%
2016-04-12 14:34:52,064 Stage-3 map = 100%,  reduce = 54%
2016-04-12 14:34:55,085 Stage-3 map = 100%,  reduce = 58%
2016-04-12 14:34:58,106 Stage-3 map = 100%,  reduce = 67%
2016-04-12 14:35:01,131 Stage-3 map = 100%,  reduce = 75%
2016-04-12 14:35:04,152 Stage-3 map = 100%,  reduce = 89%
2016-04-12 14:35:07,172 Stage-3 map = 100%,  reduce = 94%
2016-04-12 14:35:10,192 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604121431_0003
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121431_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121431_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121431_0004
2016-04-12 14:35:19,661 Stage-4 map = 0%,  reduce = 0%
2016-04-12 14:35:28,701 Stage-4 map = 50%,  reduce = 0%
2016-04-12 14:35:37,749 Stage-4 map = 100%,  reduce = 17%
2016-04-12 14:35:46,798 Stage-4 map = 100%,  reduce = 99%
2016-04-12 14:35:49,816 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604121431_0004
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121431_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121431_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121431_0005
2016-04-12 14:35:58,282 Stage-5 map = 0%,  reduce = 0%
2016-04-12 14:36:07,325 Stage-5 map = 100%,  reduce = 0%
2016-04-12 14:36:16,385 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201604121431_0005
Launching Job 6 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604121431_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604121431_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604121431_0006
2016-04-12 14:36:25,819 Stage-6 map = 0%,  reduce = 0%
2016-04-12 14:36:28,835 Stage-6 map = 100%,  reduce = 0%
2016-04-12 14:36:37,883 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201604121431_0006
Loading data to table q10_returned_item
20 Rows loaded to q10_returned_item
OK
Time taken: 328.339 seconds
