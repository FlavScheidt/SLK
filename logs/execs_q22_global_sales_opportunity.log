Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131920_2061953270.txt
OK
Time taken: 3.419 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.015 seconds
OK
Time taken: 0.017 seconds
OK
Time taken: 0.013 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.285 seconds
OK
Time taken: 0.033 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/hive/warehouse/q22_customer_tmp. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:1761)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:1735)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.mkdirs(NameNode.java:542)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131920_1599787219.txt
OK
Time taken: 3.622 seconds
OK
Time taken: 0.08 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.252 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.066 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131920_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131920_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131920_0001
2016-04-13 19:20:42,809 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:20:51,877 Stage-1 map = 19%,  reduce = 0%
2016-04-13 19:20:54,914 Stage-1 map = 45%,  reduce = 0%
2016-04-13 19:20:57,951 Stage-1 map = 83%,  reduce = 0%
2016-04-13 19:21:00,990 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:21:04,022 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131920_0001
Ended Job = 558840427, job is filtered out (removed at runtime).
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131920_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131920_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131920_0002
2016-04-13 19:21:09,617 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:21:15,661 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:21:18,686 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131920_0002
Loading data to table q22_customer_tmp
419974 Rows loaded to q22_customer_tmp
OK
Time taken: 41.196 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131920_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131920_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131920_0003
2016-04-13 19:21:25,376 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:21:28,399 Stage-1 map = 50%,  reduce = 0%
2016-04-13 19:21:31,429 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:21:37,486 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131920_0003
Loading data to table q22_customer_tmp1
1 Rows loaded to q22_customer_tmp1
OK
Time taken: 21.885 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131920_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131920_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131920_0004
2016-04-13 19:21:46,199 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:21:55,249 Stage-1 map = 9%,  reduce = 0%
2016-04-13 19:21:58,267 Stage-1 map = 15%,  reduce = 0%
2016-04-13 19:22:10,362 Stage-1 map = 15%,  reduce = 3%
2016-04-13 19:22:13,391 Stage-1 map = 22%,  reduce = 5%
2016-04-13 19:22:16,420 Stage-1 map = 31%,  reduce = 5%
2016-04-13 19:22:25,500 Stage-1 map = 39%,  reduce = 8%
2016-04-13 19:22:28,529 Stage-1 map = 46%,  reduce = 10%
2016-04-13 19:22:40,626 Stage-1 map = 54%,  reduce = 13%
2016-04-13 19:22:43,653 Stage-1 map = 62%,  reduce = 15%
2016-04-13 19:22:55,745 Stage-1 map = 70%,  reduce = 18%
2016-04-13 19:22:58,772 Stage-1 map = 77%,  reduce = 21%
2016-04-13 19:23:10,864 Stage-1 map = 84%,  reduce = 23%
2016-04-13 19:23:12,882 Stage-1 map = 89%,  reduce = 26%
2016-04-13 19:23:15,905 Stage-1 map = 92%,  reduce = 26%
2016-04-13 19:23:21,945 Stage-1 map = 92%,  reduce = 28%
2016-04-13 19:23:24,967 Stage-1 map = 98%,  reduce = 28%
2016-04-13 19:23:27,989 Stage-1 map = 100%,  reduce = 31%
2016-04-13 19:23:31,010 Stage-1 map = 100%,  reduce = 32%
2016-04-13 19:23:34,030 Stage-1 map = 100%,  reduce = 50%
2016-04-13 19:23:37,051 Stage-1 map = 100%,  reduce = 98%
2016-04-13 19:23:40,072 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131920_0004
Loading data to table q22_orders_tmp
999982 Rows loaded to q22_orders_tmp
OK
Time taken: 122.578 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131920_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131920_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131920_0005
2016-04-13 19:23:49,847 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:23:55,879 Stage-1 map = 33%,  reduce = 0%
2016-04-13 19:23:58,899 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:24:04,937 Stage-1 map = 100%,  reduce = 33%
2016-04-13 19:24:07,960 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131920_0005
Launching Job 2 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131920_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131920_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131920_0006
2016-04-13 19:24:16,495 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:24:19,513 Stage-2 map = 50%,  reduce = 0%
2016-04-13 19:24:22,532 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:24:28,571 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131920_0006
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131920_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131920_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131920_0007
2016-04-13 19:24:37,042 Stage-3 map = 0%,  reduce = 0%
2016-04-13 19:24:40,059 Stage-3 map = 100%,  reduce = 0%
2016-04-13 19:24:49,111 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604131920_0007
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131920_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131920_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131920_0008
2016-04-13 19:24:58,604 Stage-4 map = 0%,  reduce = 0%
2016-04-13 19:25:01,620 Stage-4 map = 100%,  reduce = 0%
2016-04-13 19:25:10,671 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604131920_0008
Loading data to table q22_global_sales_opportunity
7 Rows loaded to q22_global_sales_opportunity
OK
Time taken: 90.617 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131925_114690421.txt
OK
Time taken: 3.428 seconds
OK
Time taken: 0.088 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q22_customer_tmp. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131925_1634992785.txt
OK
Time taken: 2.804 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.661 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.177 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.067 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131925_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131925_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131925_0001
2016-04-13 19:25:54,671 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:26:03,743 Stage-1 map = 19%,  reduce = 0%
2016-04-13 19:26:06,765 Stage-1 map = 63%,  reduce = 0%
2016-04-13 19:26:09,804 Stage-1 map = 99%,  reduce = 0%
2016-04-13 19:26:12,843 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:26:15,876 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131925_0001
Ended Job = -1469507460, job is filtered out (removed at runtime).
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131925_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131925_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131925_0002
2016-04-13 19:26:21,482 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:26:27,520 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:26:30,545 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131925_0002
Loading data to table q22_customer_tmp
419974 Rows loaded to q22_customer_tmp
OK
Time taken: 42.169 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131925_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131925_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131925_0003
2016-04-13 19:26:36,264 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:26:39,288 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:26:48,372 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131925_0003
Loading data to table q22_customer_tmp1
1 Rows loaded to q22_customer_tmp1
OK
Time taken: 20.868 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131925_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131925_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131925_0004
2016-04-13 19:26:57,988 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:27:07,038 Stage-1 map = 8%,  reduce = 0%
2016-04-13 19:27:10,057 Stage-1 map = 15%,  reduce = 0%
2016-04-13 19:27:22,149 Stage-1 map = 15%,  reduce = 3%
2016-04-13 19:27:24,169 Stage-1 map = 23%,  reduce = 5%
2016-04-13 19:27:27,200 Stage-1 map = 31%,  reduce = 5%
2016-04-13 19:27:36,280 Stage-1 map = 39%,  reduce = 10%
2016-04-13 19:27:39,309 Stage-1 map = 46%,  reduce = 10%
2016-04-13 19:27:51,402 Stage-1 map = 54%,  reduce = 15%
2016-04-13 19:27:54,427 Stage-1 map = 61%,  reduce = 15%
2016-04-13 19:27:57,453 Stage-1 map = 62%,  reduce = 15%
2016-04-13 19:28:06,520 Stage-1 map = 70%,  reduce = 21%
2016-04-13 19:28:09,545 Stage-1 map = 77%,  reduce = 21%
2016-04-13 19:28:21,638 Stage-1 map = 84%,  reduce = 26%
2016-04-13 19:28:24,660 Stage-1 map = 87%,  reduce = 26%
2016-04-13 19:28:27,685 Stage-1 map = 92%,  reduce = 26%
2016-04-13 19:28:33,726 Stage-1 map = 92%,  reduce = 31%
2016-04-13 19:28:36,749 Stage-1 map = 96%,  reduce = 31%
2016-04-13 19:28:39,773 Stage-1 map = 100%,  reduce = 31%
2016-04-13 19:28:48,843 Stage-1 map = 100%,  reduce = 33%
2016-04-13 19:28:51,862 Stage-1 map = 100%,  reduce = 75%
2016-04-13 19:28:54,882 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131925_0004
Loading data to table q22_orders_tmp
999982 Rows loaded to q22_orders_tmp
OK
Time taken: 126.528 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131925_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131925_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131925_0005
2016-04-13 19:29:03,577 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:29:09,612 Stage-1 map = 33%,  reduce = 0%
2016-04-13 19:29:12,633 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:29:18,670 Stage-1 map = 100%,  reduce = 33%
2016-04-13 19:29:21,693 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131925_0005
Launching Job 2 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131925_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131925_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131925_0006
2016-04-13 19:29:31,190 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:29:34,208 Stage-2 map = 50%,  reduce = 0%
2016-04-13 19:29:37,229 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:29:43,266 Stage-2 map = 100%,  reduce = 17%
2016-04-13 19:29:49,301 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131925_0006
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131925_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131925_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131925_0007
2016-04-13 19:29:57,749 Stage-3 map = 0%,  reduce = 0%
2016-04-13 19:30:00,765 Stage-3 map = 100%,  reduce = 0%
2016-04-13 19:30:09,815 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604131925_0007
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131925_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131925_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131925_0008
2016-04-13 19:30:19,368 Stage-4 map = 0%,  reduce = 0%
2016-04-13 19:30:22,385 Stage-4 map = 100%,  reduce = 0%
2016-04-13 19:30:31,436 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604131925_0008
Loading data to table q22_global_sales_opportunity
7 Rows loaded to q22_global_sales_opportunity
OK
Time taken: 96.501 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131931_1221835411.txt
OK
Time taken: 3.57 seconds
OK
Time taken: 0.081 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q22_customer_tmp. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131931_1278539884.txt
OK
Time taken: 2.926 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.608 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.167 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.066 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131930_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131930_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131930_0001
2016-04-13 19:31:15,811 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:31:24,896 Stage-1 map = 19%,  reduce = 0%
2016-04-13 19:31:27,919 Stage-1 map = 65%,  reduce = 0%
2016-04-13 19:31:30,957 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:31:33,991 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131930_0001
Ended Job = 966525164, job is filtered out (removed at runtime).
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131930_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131930_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131930_0002
2016-04-13 19:31:39,558 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:31:45,596 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:31:48,621 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131930_0002
Loading data to table q22_customer_tmp
419974 Rows loaded to q22_customer_tmp
OK
Time taken: 38.573 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131930_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131930_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131930_0003
2016-04-13 19:31:55,360 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:31:58,383 Stage-1 map = 50%,  reduce = 0%
2016-04-13 19:32:01,413 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:32:07,471 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131930_0003
Loading data to table q22_customer_tmp1
1 Rows loaded to q22_customer_tmp1
OK
Time taken: 21.869 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131930_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131930_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131930_0004
2016-04-13 19:32:16,105 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:32:25,155 Stage-1 map = 9%,  reduce = 0%
2016-04-13 19:32:28,173 Stage-1 map = 15%,  reduce = 0%
2016-04-13 19:32:40,263 Stage-1 map = 17%,  reduce = 3%
2016-04-13 19:32:43,290 Stage-1 map = 24%,  reduce = 5%
2016-04-13 19:32:46,317 Stage-1 map = 31%,  reduce = 5%
2016-04-13 19:32:55,394 Stage-1 map = 40%,  reduce = 8%
2016-04-13 19:32:58,422 Stage-1 map = 46%,  reduce = 10%
2016-04-13 19:33:10,513 Stage-1 map = 55%,  reduce = 13%
2016-04-13 19:33:13,539 Stage-1 map = 62%,  reduce = 15%
2016-04-13 19:33:25,630 Stage-1 map = 70%,  reduce = 18%
2016-04-13 19:33:28,657 Stage-1 map = 77%,  reduce = 21%
2016-04-13 19:33:31,683 Stage-1 map = 77%,  reduce = 22%
2016-04-13 19:33:34,709 Stage-1 map = 77%,  reduce = 23%
2016-04-13 19:33:37,734 Stage-1 map = 81%,  reduce = 23%
2016-04-13 19:33:40,762 Stage-1 map = 88%,  reduce = 24%
2016-04-13 19:33:43,784 Stage-1 map = 91%,  reduce = 26%
2016-04-13 19:33:46,805 Stage-1 map = 92%,  reduce = 27%
2016-04-13 19:33:49,826 Stage-1 map = 92%,  reduce = 28%
2016-04-13 19:33:51,842 Stage-1 map = 96%,  reduce = 28%
2016-04-13 19:33:52,850 Stage-1 map = 96%,  reduce = 29%
2016-04-13 19:33:54,865 Stage-1 map = 100%,  reduce = 29%
2016-04-13 19:33:57,886 Stage-1 map = 100%,  reduce = 31%
2016-04-13 19:34:00,906 Stage-1 map = 100%,  reduce = 32%
2016-04-13 19:34:03,927 Stage-1 map = 100%,  reduce = 50%
2016-04-13 19:34:06,962 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131930_0004
Loading data to table q22_orders_tmp
999982 Rows loaded to q22_orders_tmp
OK
Time taken: 119.52 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131930_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131930_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131930_0005
2016-04-13 19:34:16,653 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:34:22,688 Stage-1 map = 33%,  reduce = 0%
2016-04-13 19:34:25,708 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:34:31,746 Stage-1 map = 100%,  reduce = 33%
2016-04-13 19:34:34,769 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131930_0005
Launching Job 2 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131930_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131930_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131930_0006
2016-04-13 19:34:43,264 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:34:46,282 Stage-2 map = 50%,  reduce = 0%
2016-04-13 19:34:49,302 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:34:55,340 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131930_0006
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131930_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131930_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131930_0007
2016-04-13 19:35:04,775 Stage-3 map = 0%,  reduce = 0%
2016-04-13 19:35:07,792 Stage-3 map = 100%,  reduce = 0%
2016-04-13 19:35:16,845 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604131930_0007
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131930_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131930_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131930_0008
2016-04-13 19:35:25,296 Stage-4 map = 0%,  reduce = 0%
2016-04-13 19:35:28,313 Stage-4 map = 100%,  reduce = 0%
2016-04-13 19:35:37,364 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604131930_0008
Loading data to table q22_global_sales_opportunity
7 Rows loaded to q22_global_sales_opportunity
OK
Time taken: 90.392 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131936_1627494769.txt
OK
Time taken: 3.488 seconds
OK
Time taken: 0.073 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q22_customer_tmp. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131936_185906580.txt
OK
Time taken: 2.972 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.631 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.415 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.065 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131936_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131936_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131936_0001
2016-04-13 19:36:20,408 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:36:29,475 Stage-1 map = 18%,  reduce = 0%
2016-04-13 19:36:32,498 Stage-1 map = 62%,  reduce = 0%
2016-04-13 19:36:35,535 Stage-1 map = 99%,  reduce = 0%
2016-04-13 19:36:38,570 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:36:41,603 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131936_0001
Ended Job = 1905946167, job is filtered out (removed at runtime).
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131936_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131936_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131936_0002
2016-04-13 19:36:48,207 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:36:54,245 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:36:57,271 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131936_0002
Loading data to table q22_customer_tmp
419974 Rows loaded to q22_customer_tmp
OK
Time taken: 42.178 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131936_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131936_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131936_0003
2016-04-13 19:37:03,007 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:37:06,029 Stage-1 map = 50%,  reduce = 0%
2016-04-13 19:37:09,059 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:37:15,115 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131936_0003
Loading data to table q22_customer_tmp1
1 Rows loaded to q22_customer_tmp1
OK
Time taken: 20.893 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131936_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131936_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131936_0004
2016-04-13 19:37:23,829 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:37:32,878 Stage-1 map = 9%,  reduce = 0%
2016-04-13 19:37:35,898 Stage-1 map = 15%,  reduce = 0%
2016-04-13 19:37:51,014 Stage-1 map = 23%,  reduce = 5%
2016-04-13 19:37:54,043 Stage-1 map = 31%,  reduce = 5%
2016-04-13 19:38:03,121 Stage-1 map = 39%,  reduce = 5%
2016-04-13 19:38:06,150 Stage-1 map = 46%,  reduce = 10%
2016-04-13 19:38:18,244 Stage-1 map = 54%,  reduce = 15%
2016-04-13 19:38:21,270 Stage-1 map = 61%,  reduce = 15%
2016-04-13 19:38:24,297 Stage-1 map = 62%,  reduce = 15%
2016-04-13 19:38:33,366 Stage-1 map = 70%,  reduce = 21%
2016-04-13 19:38:36,392 Stage-1 map = 77%,  reduce = 21%
2016-04-13 19:38:48,486 Stage-1 map = 84%,  reduce = 26%
2016-04-13 19:38:51,508 Stage-1 map = 89%,  reduce = 26%
2016-04-13 19:38:54,533 Stage-1 map = 92%,  reduce = 26%
2016-04-13 19:39:00,574 Stage-1 map = 92%,  reduce = 28%
2016-04-13 19:39:03,600 Stage-1 map = 98%,  reduce = 31%
2016-04-13 19:39:06,623 Stage-1 map = 100%,  reduce = 31%
2016-04-13 19:39:15,679 Stage-1 map = 100%,  reduce = 33%
2016-04-13 19:39:17,693 Stage-1 map = 100%,  reduce = 51%
2016-04-13 19:39:18,702 Stage-1 map = 100%,  reduce = 77%
2016-04-13 19:39:20,717 Stage-1 map = 100%,  reduce = 92%
2016-04-13 19:39:21,727 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131936_0004
Loading data to table q22_orders_tmp
999982 Rows loaded to q22_orders_tmp
OK
Time taken: 125.629 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131936_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131936_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131936_0005
2016-04-13 19:39:30,432 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:39:36,462 Stage-1 map = 33%,  reduce = 0%
2016-04-13 19:39:39,482 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:39:45,519 Stage-1 map = 100%,  reduce = 33%
2016-04-13 19:39:48,541 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131936_0005
Launching Job 2 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131936_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131936_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131936_0006
2016-04-13 19:39:57,132 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:40:00,150 Stage-2 map = 50%,  reduce = 0%
2016-04-13 19:40:03,173 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:40:09,208 Stage-2 map = 100%,  reduce = 17%
2016-04-13 19:40:15,246 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131936_0006
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131936_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131936_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131936_0007
2016-04-13 19:40:24,702 Stage-3 map = 0%,  reduce = 0%
2016-04-13 19:40:27,719 Stage-3 map = 100%,  reduce = 0%
2016-04-13 19:40:36,770 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604131936_0007
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131936_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131936_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131936_0008
2016-04-13 19:40:45,247 Stage-4 map = 0%,  reduce = 0%
2016-04-13 19:40:48,265 Stage-4 map = 100%,  reduce = 0%
2016-04-13 19:40:57,315 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604131936_0008
Loading data to table q22_global_sales_opportunity
7 Rows loaded to q22_global_sales_opportunity
OK
Time taken: 96.609 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131941_734331957.txt
OK
Time taken: 3.476 seconds
OK
Time taken: 0.082 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q22_customer_tmp. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131941_909816861.txt
OK
Time taken: 2.82 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.619 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.156 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.074 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131941_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131941_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131941_0001
2016-04-13 19:41:42,089 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:41:51,160 Stage-1 map = 18%,  reduce = 0%
2016-04-13 19:41:54,183 Stage-1 map = 66%,  reduce = 0%
2016-04-13 19:41:57,223 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:42:00,262 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131941_0001
Ended Job = 329923413, job is filtered out (removed at runtime).
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131941_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131941_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131941_0002
2016-04-13 19:42:05,810 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:42:11,850 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:42:14,877 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131941_0002
Loading data to table q22_customer_tmp
419974 Rows loaded to q22_customer_tmp
OK
Time taken: 39.186 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131941_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131941_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131941_0003
2016-04-13 19:42:20,571 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:42:23,595 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:42:32,679 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131941_0003
Loading data to table q22_customer_tmp1
1 Rows loaded to q22_customer_tmp1
OK
Time taken: 20.843 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131941_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131941_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131941_0004
2016-04-13 19:42:42,358 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:42:51,410 Stage-1 map = 4%,  reduce = 0%
2016-04-13 19:42:54,432 Stage-1 map = 11%,  reduce = 0%
2016-04-13 19:42:57,460 Stage-1 map = 15%,  reduce = 0%
2016-04-13 19:43:03,513 Stage-1 map = 18%,  reduce = 0%
2016-04-13 19:43:05,534 Stage-1 map = 19%,  reduce = 0%
2016-04-13 19:43:08,563 Stage-1 map = 21%,  reduce = 3%
2016-04-13 19:43:11,592 Stage-1 map = 26%,  reduce = 6%
2016-04-13 19:43:14,622 Stage-1 map = 33%,  reduce = 6%
2016-04-13 19:43:17,652 Stage-1 map = 35%,  reduce = 6%
2016-04-13 19:43:20,681 Stage-1 map = 37%,  reduce = 7%
2016-04-13 19:43:23,712 Stage-1 map = 43%,  reduce = 10%
2016-04-13 19:43:26,740 Stage-1 map = 50%,  reduce = 12%
2016-04-13 19:43:35,811 Stage-1 map = 53%,  reduce = 12%
2016-04-13 19:43:38,836 Stage-1 map = 60%,  reduce = 16%
2016-04-13 19:43:41,863 Stage-1 map = 65%,  reduce = 17%
2016-04-13 19:43:47,911 Stage-1 map = 68%,  reduce = 19%
2016-04-13 19:43:50,937 Stage-1 map = 71%,  reduce = 19%
2016-04-13 19:43:53,963 Stage-1 map = 77%,  reduce = 21%
2016-04-13 19:43:56,989 Stage-1 map = 81%,  reduce = 21%
2016-04-13 19:44:00,014 Stage-1 map = 83%,  reduce = 24%
2016-04-13 19:44:03,038 Stage-1 map = 86%,  reduce = 24%
2016-04-13 19:44:06,064 Stage-1 map = 88%,  reduce = 27%
2016-04-13 19:44:09,086 Stage-1 map = 90%,  reduce = 27%
2016-04-13 19:44:12,109 Stage-1 map = 92%,  reduce = 28%
2016-04-13 19:44:15,133 Stage-1 map = 97%,  reduce = 29%
2016-04-13 19:44:18,156 Stage-1 map = 100%,  reduce = 29%
2016-04-13 19:44:24,210 Stage-1 map = 100%,  reduce = 31%
2016-04-13 19:44:30,247 Stage-1 map = 100%,  reduce = 66%
2016-04-13 19:44:33,268 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131941_0004
Loading data to table q22_orders_tmp
999982 Rows loaded to q22_orders_tmp
OK
Time taken: 120.604 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131941_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131941_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131941_0005
2016-04-13 19:44:41,989 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:44:48,022 Stage-1 map = 33%,  reduce = 0%
2016-04-13 19:44:51,042 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:44:57,079 Stage-1 map = 100%,  reduce = 33%
2016-04-13 19:45:00,103 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131941_0005
Launching Job 2 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131941_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131941_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131941_0006
2016-04-13 19:45:09,600 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:45:12,618 Stage-2 map = 50%,  reduce = 0%
2016-04-13 19:45:15,638 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:45:21,675 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131941_0006
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131941_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131941_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131941_0007
2016-04-13 19:45:30,546 Stage-3 map = 0%,  reduce = 0%
2016-04-13 19:45:33,564 Stage-3 map = 100%,  reduce = 0%
2016-04-13 19:45:42,617 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604131941_0007
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131941_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131941_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131941_0008
2016-04-13 19:45:51,110 Stage-4 map = 0%,  reduce = 0%
2016-04-13 19:45:54,127 Stage-4 map = 100%,  reduce = 0%
2016-04-13 19:46:03,179 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604131941_0008
Loading data to table q22_global_sales_opportunity
7 Rows loaded to q22_global_sales_opportunity
OK
Time taken: 89.901 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131946_210797903.txt
OK
Time taken: 3.4 seconds
OK
Time taken: 0.065 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q22_customer_tmp. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131946_2092659539.txt
OK
Time taken: 2.957 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.6 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.165 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.048 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.073 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131946_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131946_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131946_0001
2016-04-13 19:46:46,988 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:46:56,056 Stage-1 map = 18%,  reduce = 0%
2016-04-13 19:46:59,093 Stage-1 map = 66%,  reduce = 0%
2016-04-13 19:47:02,129 Stage-1 map = 99%,  reduce = 0%
2016-04-13 19:47:05,164 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:47:08,194 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131946_0001
Ended Job = 1926099927, job is filtered out (removed at runtime).
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131946_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131946_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131946_0002
2016-04-13 19:47:14,746 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:47:20,782 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:47:23,808 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131946_0002
Loading data to table q22_customer_tmp
419974 Rows loaded to q22_customer_tmp
OK
Time taken: 42.486 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131946_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131946_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131946_0003
2016-04-13 19:47:29,545 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:47:32,568 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:47:41,647 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131946_0003
Loading data to table q22_customer_tmp1
1 Rows loaded to q22_customer_tmp1
OK
Time taken: 20.869 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131946_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131946_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131946_0004
2016-04-13 19:47:50,253 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:47:59,301 Stage-1 map = 9%,  reduce = 0%
2016-04-13 19:48:02,323 Stage-1 map = 15%,  reduce = 0%
2016-04-13 19:48:11,397 Stage-1 map = 17%,  reduce = 5%
2016-04-13 19:48:14,424 Stage-1 map = 24%,  reduce = 5%
2016-04-13 19:48:17,453 Stage-1 map = 30%,  reduce = 5%
2016-04-13 19:48:20,481 Stage-1 map = 31%,  reduce = 5%
2016-04-13 19:48:26,532 Stage-1 map = 35%,  reduce = 10%
2016-04-13 19:48:29,560 Stage-1 map = 43%,  reduce = 10%
2016-04-13 19:48:32,588 Stage-1 map = 46%,  reduce = 10%
2016-04-13 19:48:41,659 Stage-1 map = 50%,  reduce = 15%
2016-04-13 19:48:44,684 Stage-1 map = 58%,  reduce = 15%
2016-04-13 19:48:47,709 Stage-1 map = 62%,  reduce = 15%
2016-04-13 19:48:56,776 Stage-1 map = 66%,  reduce = 18%
2016-04-13 19:48:59,802 Stage-1 map = 74%,  reduce = 19%
2016-04-13 19:49:02,827 Stage-1 map = 77%,  reduce = 21%
2016-04-13 19:49:11,901 Stage-1 map = 85%,  reduce = 23%
2016-04-13 19:49:14,922 Stage-1 map = 89%,  reduce = 23%
2016-04-13 19:49:17,947 Stage-1 map = 92%,  reduce = 26%
2016-04-13 19:49:23,988 Stage-1 map = 92%,  reduce = 27%
2016-04-13 19:49:27,009 Stage-1 map = 96%,  reduce = 28%
2016-04-13 19:49:30,031 Stage-1 map = 100%,  reduce = 28%
2016-04-13 19:49:33,053 Stage-1 map = 100%,  reduce = 31%
2016-04-13 19:49:42,110 Stage-1 map = 100%,  reduce = 94%
2016-04-13 19:49:45,137 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131946_0004
Loading data to table q22_orders_tmp
999982 Rows loaded to q22_orders_tmp
OK
Time taken: 123.529 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131946_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131946_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131946_0005
2016-04-13 19:49:53,846 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:49:59,877 Stage-1 map = 33%,  reduce = 0%
2016-04-13 19:50:02,897 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:50:08,933 Stage-1 map = 100%,  reduce = 33%
2016-04-13 19:50:11,956 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131946_0005
Launching Job 2 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131946_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131946_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131946_0006
2016-04-13 19:50:20,466 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:50:23,483 Stage-2 map = 50%,  reduce = 0%
2016-04-13 19:50:26,501 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:50:32,535 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131946_0006
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131946_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131946_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131946_0007
2016-04-13 19:50:41,963 Stage-3 map = 0%,  reduce = 0%
2016-04-13 19:50:44,981 Stage-3 map = 100%,  reduce = 0%
2016-04-13 19:50:54,030 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604131946_0007
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131946_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131946_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131946_0008
2016-04-13 19:51:02,451 Stage-4 map = 0%,  reduce = 0%
2016-04-13 19:51:05,468 Stage-4 map = 100%,  reduce = 0%
2016-04-13 19:51:14,515 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604131946_0008
Loading data to table q22_global_sales_opportunity
7 Rows loaded to q22_global_sales_opportunity
OK
Time taken: 89.326 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131951_1267151002.txt
OK
Time taken: 3.394 seconds
OK
Time taken: 0.065 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q22_customer_tmp. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131951_1244782328.txt
OK
Time taken: 2.914 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.621 seconds
OK
Time taken: 0.074 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.156 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.074 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131951_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131951_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131951_0001
2016-04-13 19:51:57,898 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:52:06,966 Stage-1 map = 18%,  reduce = 0%
2016-04-13 19:52:09,988 Stage-1 map = 62%,  reduce = 0%
2016-04-13 19:52:13,025 Stage-1 map = 98%,  reduce = 0%
2016-04-13 19:52:16,064 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:52:19,096 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131951_0001
Ended Job = 471408492, job is filtered out (removed at runtime).
Launching Job 2 out of 2
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201604131951_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131951_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131951_0002
2016-04-13 19:52:24,671 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:52:30,711 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:52:33,736 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131951_0002
Loading data to table q22_customer_tmp
419974 Rows loaded to q22_customer_tmp
OK
Time taken: 42.095 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131951_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131951_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131951_0003
2016-04-13 19:52:39,510 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:52:42,533 Stage-1 map = 50%,  reduce = 0%
2016-04-13 19:52:48,589 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:52:51,620 Stage-1 map = 100%,  reduce = 17%
2016-04-13 19:52:57,677 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131951_0003
Loading data to table q22_customer_tmp1
1 Rows loaded to q22_customer_tmp1
OK
Time taken: 27.011 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131951_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131951_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131951_0004
2016-04-13 19:53:07,319 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:53:16,368 Stage-1 map = 8%,  reduce = 0%
2016-04-13 19:53:19,389 Stage-1 map = 15%,  reduce = 0%
2016-04-13 19:53:28,464 Stage-1 map = 15%,  reduce = 3%
2016-04-13 19:53:31,491 Stage-1 map = 19%,  reduce = 3%
2016-04-13 19:53:34,520 Stage-1 map = 27%,  reduce = 5%
2016-04-13 19:53:36,541 Stage-1 map = 31%,  reduce = 5%
2016-04-13 19:53:42,591 Stage-1 map = 35%,  reduce = 9%
2016-04-13 19:53:45,620 Stage-1 map = 43%,  reduce = 9%
2016-04-13 19:53:48,648 Stage-1 map = 46%,  reduce = 10%
2016-04-13 19:53:57,722 Stage-1 map = 50%,  reduce = 13%
2016-04-13 19:54:00,748 Stage-1 map = 58%,  reduce = 14%
2016-04-13 19:54:03,775 Stage-1 map = 61%,  reduce = 15%
2016-04-13 19:54:06,801 Stage-1 map = 62%,  reduce = 15%
2016-04-13 19:54:09,826 Stage-1 map = 62%,  reduce = 17%
2016-04-13 19:54:12,851 Stage-1 map = 66%,  reduce = 19%
2016-04-13 19:54:15,877 Stage-1 map = 73%,  reduce = 21%
2016-04-13 19:54:18,906 Stage-1 map = 77%,  reduce = 21%
2016-04-13 19:54:21,930 Stage-1 map = 77%,  reduce = 22%
2016-04-13 19:54:27,973 Stage-1 map = 81%,  reduce = 24%
2016-04-13 19:54:30,996 Stage-1 map = 86%,  reduce = 24%
2016-04-13 19:54:34,018 Stage-1 map = 90%,  reduce = 26%
2016-04-13 19:54:37,039 Stage-1 map = 92%,  reduce = 26%
2016-04-13 19:54:40,060 Stage-1 map = 92%,  reduce = 29%
2016-04-13 19:54:43,081 Stage-1 map = 96%,  reduce = 29%
2016-04-13 19:54:46,102 Stage-1 map = 100%,  reduce = 31%
2016-04-13 19:54:58,190 Stage-1 map = 100%,  reduce = 90%
2016-04-13 19:55:01,210 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131951_0004
Loading data to table q22_orders_tmp
999982 Rows loaded to q22_orders_tmp
OK
Time taken: 123.512 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131951_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131951_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131951_0005
2016-04-13 19:55:09,883 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:55:15,914 Stage-1 map = 33%,  reduce = 0%
2016-04-13 19:55:18,934 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:55:24,972 Stage-1 map = 100%,  reduce = 33%
2016-04-13 19:55:27,995 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131951_0005
Launching Job 2 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131951_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131951_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131951_0006
2016-04-13 19:55:37,516 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:55:40,534 Stage-2 map = 50%,  reduce = 0%
2016-04-13 19:55:43,554 Stage-2 map = 100%,  reduce = 0%
2016-04-13 19:55:49,591 Stage-2 map = 100%,  reduce = 17%
2016-04-13 19:55:55,629 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131951_0006
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131951_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131951_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131951_0007
2016-04-13 19:56:04,068 Stage-3 map = 0%,  reduce = 0%
2016-04-13 19:56:07,084 Stage-3 map = 100%,  reduce = 0%
2016-04-13 19:56:16,136 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604131951_0007
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131951_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131951_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131951_0008
2016-04-13 19:56:25,568 Stage-4 map = 0%,  reduce = 0%
2016-04-13 19:56:28,584 Stage-4 map = 100%,  reduce = 0%
2016-04-13 19:56:37,635 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604131951_0008
Loading data to table q22_global_sales_opportunity
7 Rows loaded to q22_global_sales_opportunity
OK
Time taken: 96.464 seconds
