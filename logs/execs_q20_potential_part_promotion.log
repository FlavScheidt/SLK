Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122228_646457156.txt
OK
Time taken: 3.47 seconds
OK
Time taken: 0.104 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.005 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.244 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/hive/warehouse/q20_tmp1. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:1761)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:1735)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.mkdirs(NameNode.java:542)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122228_1267111808.txt
OK
Time taken: 3.602 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.242 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.043 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.1 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122228_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122228_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122228_0001
2016-04-12 22:28:35,183 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:28:44,260 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:28:53,354 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122228_0001
Loading data to table q20_tmp1
21551 Rows loaded to q20_tmp1
OK
Time taken: 26.346 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122228_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122228_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122228_0002
2016-04-12 22:29:02,104 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:29:11,157 Stage-1 map = 1%,  reduce = 0%
2016-04-12 22:29:14,176 Stage-1 map = 4%,  reduce = 0%
2016-04-12 22:29:17,195 Stage-1 map = 7%,  reduce = 0%
2016-04-12 22:29:20,215 Stage-1 map = 10%,  reduce = 0%
2016-04-12 22:29:23,234 Stage-1 map = 14%,  reduce = 0%
2016-04-12 22:29:26,257 Stage-1 map = 18%,  reduce = 0%
2016-04-12 22:29:29,287 Stage-1 map = 22%,  reduce = 0%
2016-04-12 22:29:32,321 Stage-1 map = 24%,  reduce = 0%
2016-04-12 22:29:35,353 Stage-1 map = 27%,  reduce = 0%
2016-04-12 22:29:38,387 Stage-1 map = 28%,  reduce = 1%
2016-04-12 22:29:41,419 Stage-1 map = 28%,  reduce = 2%
2016-04-12 22:29:44,451 Stage-1 map = 29%,  reduce = 2%
2016-04-12 22:29:47,481 Stage-1 map = 32%,  reduce = 3%
2016-04-12 22:29:50,511 Stage-1 map = 34%,  reduce = 4%
2016-04-12 22:29:53,541 Stage-1 map = 36%,  reduce = 4%
2016-04-12 22:29:56,571 Stage-1 map = 39%,  reduce = 4%
2016-04-12 22:29:59,605 Stage-1 map = 43%,  reduce = 4%
2016-04-12 22:30:02,635 Stage-1 map = 46%,  reduce = 4%
2016-04-12 22:30:05,665 Stage-1 map = 48%,  reduce = 5%
2016-04-12 22:30:08,692 Stage-1 map = 52%,  reduce = 5%
2016-04-12 22:30:11,720 Stage-1 map = 54%,  reduce = 6%
2016-04-12 22:30:14,747 Stage-1 map = 56%,  reduce = 6%
2016-04-12 22:30:17,778 Stage-1 map = 57%,  reduce = 6%
2016-04-12 22:30:20,805 Stage-1 map = 58%,  reduce = 7%
2016-04-12 22:30:23,830 Stage-1 map = 61%,  reduce = 8%
2016-04-12 22:30:26,856 Stage-1 map = 64%,  reduce = 9%
2016-04-12 22:30:29,882 Stage-1 map = 66%,  reduce = 9%
2016-04-12 22:30:32,906 Stage-1 map = 70%,  reduce = 9%
2016-04-12 22:30:35,928 Stage-1 map = 71%,  reduce = 9%
2016-04-12 22:30:38,950 Stage-1 map = 75%,  reduce = 9%
2016-04-12 22:30:41,972 Stage-1 map = 78%,  reduce = 10%
2016-04-12 22:30:44,994 Stage-1 map = 81%,  reduce = 10%
2016-04-12 22:30:48,017 Stage-1 map = 84%,  reduce = 10%
2016-04-12 22:30:51,040 Stage-1 map = 84%,  reduce = 11%
2016-04-12 22:30:54,062 Stage-1 map = 86%,  reduce = 12%
2016-04-12 22:30:57,082 Stage-1 map = 88%,  reduce = 13%
2016-04-12 22:31:00,103 Stage-1 map = 90%,  reduce = 13%
2016-04-12 22:31:03,123 Stage-1 map = 92%,  reduce = 13%
2016-04-12 22:31:06,143 Stage-1 map = 95%,  reduce = 14%
2016-04-12 22:31:09,163 Stage-1 map = 96%,  reduce = 14%
2016-04-12 22:31:12,183 Stage-1 map = 98%,  reduce = 14%
2016-04-12 22:31:15,204 Stage-1 map = 100%,  reduce = 15%
2016-04-12 22:31:20,235 Stage-1 map = 100%,  reduce = 16%
2016-04-12 22:31:23,257 Stage-1 map = 100%,  reduce = 25%
2016-04-12 22:31:26,278 Stage-1 map = 100%,  reduce = 35%
2016-04-12 22:31:29,299 Stage-1 map = 100%,  reduce = 47%
2016-04-12 22:31:32,320 Stage-1 map = 100%,  reduce = 50%
2016-04-12 22:31:35,341 Stage-1 map = 100%,  reduce = 54%
2016-04-12 22:31:38,362 Stage-1 map = 100%,  reduce = 63%
2016-04-12 22:31:41,384 Stage-1 map = 100%,  reduce = 73%
2016-04-12 22:31:44,406 Stage-1 map = 100%,  reduce = 92%
2016-04-12 22:31:47,427 Stage-1 map = 100%,  reduce = 96%
2016-04-12 22:31:50,448 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122228_0002
Loading data to table q20_tmp2
5441669 Rows loaded to q20_tmp2
OK
Time taken: 177.167 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122228_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122228_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122228_0003
2016-04-12 22:32:00,261 Stage-2 map = 0%,  reduce = 0%
2016-04-12 22:32:05,290 Stage-2 map = 25%,  reduce = 0%
2016-04-12 22:32:08,308 Stage-2 map = 38%,  reduce = 0%
2016-04-12 22:32:09,316 Stage-2 map = 42%,  reduce = 0%
2016-04-12 22:32:11,330 Stage-2 map = 58%,  reduce = 0%
2016-04-12 22:32:14,349 Stage-2 map = 65%,  reduce = 8%
2016-04-12 22:32:17,369 Stage-2 map = 71%,  reduce = 17%
2016-04-12 22:32:20,389 Stage-2 map = 77%,  reduce = 17%
2016-04-12 22:32:23,408 Stage-2 map = 84%,  reduce = 17%
2016-04-12 22:32:26,429 Stage-2 map = 91%,  reduce = 17%
2016-04-12 22:32:29,447 Stage-2 map = 98%,  reduce = 17%
2016-04-12 22:32:32,465 Stage-2 map = 100%,  reduce = 17%
2016-04-12 22:32:44,529 Stage-2 map = 100%,  reduce = 21%
2016-04-12 22:32:47,548 Stage-2 map = 100%,  reduce = 47%
2016-04-12 22:32:50,569 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122228_0003
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122228_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122228_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122228_0004
2016-04-12 22:33:00,019 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:33:09,063 Stage-1 map = 30%,  reduce = 0%
2016-04-12 22:33:12,081 Stage-1 map = 36%,  reduce = 0%
2016-04-12 22:33:15,100 Stage-1 map = 39%,  reduce = 0%
2016-04-12 22:33:18,119 Stage-1 map = 50%,  reduce = 10%
2016-04-12 22:33:21,138 Stage-1 map = 70%,  reduce = 10%
2016-04-12 22:33:24,157 Stage-1 map = 80%,  reduce = 10%
2016-04-12 22:33:27,175 Stage-1 map = 100%,  reduce = 13%
2016-04-12 22:33:33,209 Stage-1 map = 100%,  reduce = 27%
2016-04-12 22:33:36,227 Stage-1 map = 100%,  reduce = 67%
2016-04-12 22:33:39,245 Stage-1 map = 100%,  reduce = 83%
2016-04-12 22:33:42,264 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122228_0004
Loading data to table q20_tmp3
58782 Rows loaded to q20_tmp3
OK
Time taken: 111.718 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122228_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122228_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122228_0005
2016-04-12 22:33:50,868 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:33:53,897 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:34:02,948 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122228_0005
Loading data to table q20_tmp4
44482 Rows loaded to q20_tmp4
OK
Time taken: 20.717 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122228_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122228_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122228_0006
2016-04-12 22:34:11,644 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:34:14,660 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:34:23,709 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122228_0006
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122228_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122228_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122228_0007
2016-04-12 22:34:33,126 Stage-2 map = 0%,  reduce = 0%
2016-04-12 22:34:36,142 Stage-2 map = 100%,  reduce = 0%
2016-04-12 22:34:45,201 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122228_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122228_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122228_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122228_0008
2016-04-12 22:34:53,691 Stage-3 map = 0%,  reduce = 0%
2016-04-12 22:34:56,706 Stage-3 map = 100%,  reduce = 0%
2016-04-12 22:35:05,752 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604122228_0008
Loading data to table q20_potential_part_promotion
1804 Rows loaded to q20_potential_part_promotion
OK
Time taken: 62.748 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122235_679805112.txt
OK
Time taken: 3.487 seconds
OK
Time taken: 0.073 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.058 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q20_tmp1. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122235_1317279426.txt
OK
Time taken: 2.952 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.622 seconds
OK
Time taken: 0.074 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.156 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.025 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122235_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122235_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122235_0001
2016-04-12 22:35:55,777 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:36:04,853 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:36:13,944 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122235_0001
Loading data to table q20_tmp1
21551 Rows loaded to q20_tmp1
OK
Time taken: 26.829 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122235_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122235_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122235_0002
2016-04-12 22:36:22,727 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:36:31,780 Stage-1 map = 3%,  reduce = 0%
2016-04-12 22:36:34,798 Stage-1 map = 5%,  reduce = 0%
2016-04-12 22:36:37,817 Stage-1 map = 8%,  reduce = 0%
2016-04-12 22:36:40,835 Stage-1 map = 13%,  reduce = 0%
2016-04-12 22:36:43,854 Stage-1 map = 16%,  reduce = 0%
2016-04-12 22:36:46,876 Stage-1 map = 20%,  reduce = 0%
2016-04-12 22:36:49,905 Stage-1 map = 23%,  reduce = 0%
2016-04-12 22:36:52,939 Stage-1 map = 24%,  reduce = 0%
2016-04-12 22:36:55,969 Stage-1 map = 26%,  reduce = 0%
2016-04-12 22:36:59,002 Stage-1 map = 28%,  reduce = 1%
2016-04-12 22:37:02,034 Stage-1 map = 29%,  reduce = 2%
2016-04-12 22:37:05,065 Stage-1 map = 30%,  reduce = 2%
2016-04-12 22:37:08,096 Stage-1 map = 31%,  reduce = 2%
2016-04-12 22:37:11,125 Stage-1 map = 34%,  reduce = 4%
2016-04-12 22:37:14,156 Stage-1 map = 36%,  reduce = 4%
2016-04-12 22:37:17,185 Stage-1 map = 39%,  reduce = 4%
2016-04-12 22:37:20,218 Stage-1 map = 43%,  reduce = 4%
2016-04-12 22:37:23,247 Stage-1 map = 45%,  reduce = 4%
2016-04-12 22:37:26,276 Stage-1 map = 48%,  reduce = 5%
2016-04-12 22:37:29,302 Stage-1 map = 52%,  reduce = 5%
2016-04-12 22:37:32,330 Stage-1 map = 54%,  reduce = 6%
2016-04-12 22:37:35,359 Stage-1 map = 56%,  reduce = 6%
2016-04-12 22:37:41,412 Stage-1 map = 58%,  reduce = 8%
2016-04-12 22:37:44,441 Stage-1 map = 61%,  reduce = 8%
2016-04-12 22:37:47,467 Stage-1 map = 63%,  reduce = 9%
2016-04-12 22:37:50,501 Stage-1 map = 66%,  reduce = 9%
2016-04-12 22:37:53,526 Stage-1 map = 70%,  reduce = 9%
2016-04-12 22:37:56,550 Stage-1 map = 72%,  reduce = 9%
2016-04-12 22:37:59,572 Stage-1 map = 75%,  reduce = 9%
2016-04-12 22:38:02,595 Stage-1 map = 78%,  reduce = 10%
2016-04-12 22:38:05,618 Stage-1 map = 80%,  reduce = 10%
2016-04-12 22:38:08,641 Stage-1 map = 82%,  reduce = 10%
2016-04-12 22:38:10,661 Stage-1 map = 82%,  reduce = 11%
2016-04-12 22:38:11,670 Stage-1 map = 83%,  reduce = 12%
2016-04-12 22:38:14,690 Stage-1 map = 84%,  reduce = 12%
2016-04-12 22:38:16,705 Stage-1 map = 87%,  reduce = 13%
2016-04-12 22:38:19,725 Stage-1 map = 90%,  reduce = 13%
2016-04-12 22:38:22,745 Stage-1 map = 92%,  reduce = 13%
2016-04-12 22:38:25,765 Stage-1 map = 95%,  reduce = 13%
2016-04-12 22:38:28,786 Stage-1 map = 97%,  reduce = 13%
2016-04-12 22:38:31,807 Stage-1 map = 98%,  reduce = 14%
2016-04-12 22:38:37,844 Stage-1 map = 100%,  reduce = 15%
2016-04-12 22:38:40,864 Stage-1 map = 100%,  reduce = 16%
2016-04-12 22:38:49,921 Stage-1 map = 100%,  reduce = 30%
2016-04-12 22:38:52,943 Stage-1 map = 100%,  reduce = 44%
2016-04-12 22:38:55,964 Stage-1 map = 100%,  reduce = 49%
2016-04-12 22:38:58,985 Stage-1 map = 100%,  reduce = 50%
2016-04-12 22:39:02,005 Stage-1 map = 100%,  reduce = 54%
2016-04-12 22:39:05,026 Stage-1 map = 100%,  reduce = 67%
2016-04-12 22:39:08,047 Stage-1 map = 100%,  reduce = 88%
2016-04-12 22:39:11,068 Stage-1 map = 100%,  reduce = 94%
2016-04-12 22:39:14,089 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122235_0002
Loading data to table q20_tmp2
5441669 Rows loaded to q20_tmp2
OK
Time taken: 180.209 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122235_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122235_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122235_0003
2016-04-12 22:39:22,876 Stage-2 map = 0%,  reduce = 0%
2016-04-12 22:39:28,906 Stage-2 map = 25%,  reduce = 0%
2016-04-12 22:39:31,926 Stage-2 map = 43%,  reduce = 0%
2016-04-12 22:39:34,945 Stage-2 map = 59%,  reduce = 0%
2016-04-12 22:39:37,964 Stage-2 map = 65%,  reduce = 8%
2016-04-12 22:39:40,983 Stage-2 map = 72%,  reduce = 17%
2016-04-12 22:39:44,001 Stage-2 map = 78%,  reduce = 17%
2016-04-12 22:39:47,019 Stage-2 map = 84%,  reduce = 17%
2016-04-12 22:39:50,037 Stage-2 map = 91%,  reduce = 17%
2016-04-12 22:39:53,055 Stage-2 map = 98%,  reduce = 17%
2016-04-12 22:39:56,073 Stage-2 map = 100%,  reduce = 17%
2016-04-12 22:40:08,138 Stage-2 map = 100%,  reduce = 51%
2016-04-12 22:40:11,161 Stage-2 map = 100%,  reduce = 99%
2016-04-12 22:40:14,180 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122235_0003
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122235_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122235_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122235_0004
2016-04-12 22:40:23,658 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:40:32,701 Stage-1 map = 31%,  reduce = 0%
2016-04-12 22:40:35,719 Stage-1 map = 36%,  reduce = 0%
2016-04-12 22:40:38,738 Stage-1 map = 40%,  reduce = 0%
2016-04-12 22:40:41,758 Stage-1 map = 60%,  reduce = 10%
2016-04-12 22:40:44,777 Stage-1 map = 70%,  reduce = 10%
2016-04-12 22:40:47,796 Stage-1 map = 80%,  reduce = 10%
2016-04-12 22:40:50,815 Stage-1 map = 100%,  reduce = 17%
2016-04-12 22:40:56,849 Stage-1 map = 100%,  reduce = 27%
2016-04-12 22:41:02,883 Stage-1 map = 100%,  reduce = 72%
2016-04-12 22:41:04,896 Stage-1 map = 100%,  reduce = 98%
2016-04-12 22:41:07,916 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122235_0004
Loading data to table q20_tmp3
58782 Rows loaded to q20_tmp3
OK
Time taken: 113.721 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122235_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122235_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122235_0005
2016-04-12 22:41:17,498 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:41:20,514 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:41:29,564 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122235_0005
Loading data to table q20_tmp4
44482 Rows loaded to q20_tmp4
OK
Time taken: 21.693 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122235_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122235_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122235_0006
2016-04-12 22:41:38,265 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:41:41,281 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:41:50,332 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122235_0006
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122235_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122235_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122235_0007
2016-04-12 22:41:59,745 Stage-2 map = 0%,  reduce = 0%
2016-04-12 22:42:02,760 Stage-2 map = 100%,  reduce = 0%
2016-04-12 22:42:11,816 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122235_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122235_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122235_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122235_0008
2016-04-12 22:42:20,320 Stage-3 map = 0%,  reduce = 0%
2016-04-12 22:42:23,335 Stage-3 map = 100%,  reduce = 0%
2016-04-12 22:42:32,379 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604122235_0008
Loading data to table q20_potential_part_promotion
1804 Rows loaded to q20_potential_part_promotion
OK
Time taken: 62.765 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122243_1546912595.txt
OK
Time taken: 3.616 seconds
OK
Time taken: 0.073 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.059 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q20_tmp1. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122243_1788278737.txt
OK
Time taken: 2.891 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.63 seconds
OK
Time taken: 0.09 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.41 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122243_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122243_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122243_0001
2016-04-12 22:43:22,210 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:43:31,288 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:43:40,380 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122243_0001
Loading data to table q20_tmp1
21551 Rows loaded to q20_tmp1
OK
Time taken: 26.429 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122243_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122243_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122243_0002
2016-04-12 22:43:49,125 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:43:58,177 Stage-1 map = 3%,  reduce = 0%
2016-04-12 22:44:01,196 Stage-1 map = 5%,  reduce = 0%
2016-04-12 22:44:04,216 Stage-1 map = 8%,  reduce = 0%
2016-04-12 22:44:07,235 Stage-1 map = 12%,  reduce = 0%
2016-04-12 22:44:10,254 Stage-1 map = 16%,  reduce = 0%
2016-04-12 22:44:13,276 Stage-1 map = 19%,  reduce = 0%
2016-04-12 22:44:16,306 Stage-1 map = 21%,  reduce = 0%
2016-04-12 22:44:19,340 Stage-1 map = 23%,  reduce = 0%
2016-04-12 22:44:22,370 Stage-1 map = 25%,  reduce = 1%
2016-04-12 22:44:25,403 Stage-1 map = 28%,  reduce = 1%
2016-04-12 22:44:31,461 Stage-1 map = 29%,  reduce = 2%
2016-04-12 22:44:34,492 Stage-1 map = 31%,  reduce = 2%
2016-04-12 22:44:37,522 Stage-1 map = 33%,  reduce = 4%
2016-04-12 22:44:40,552 Stage-1 map = 36%,  reduce = 4%
2016-04-12 22:44:43,585 Stage-1 map = 39%,  reduce = 4%
2016-04-12 22:44:46,617 Stage-1 map = 42%,  reduce = 4%
2016-04-12 22:44:49,647 Stage-1 map = 46%,  reduce = 4%
2016-04-12 22:44:52,675 Stage-1 map = 48%,  reduce = 5%
2016-04-12 22:44:55,702 Stage-1 map = 51%,  reduce = 5%
2016-04-12 22:44:58,731 Stage-1 map = 53%,  reduce = 6%
2016-04-12 22:45:01,759 Stage-1 map = 54%,  reduce = 6%
2016-04-12 22:45:04,786 Stage-1 map = 56%,  reduce = 6%
2016-04-12 22:45:07,812 Stage-1 map = 58%,  reduce = 8%
2016-04-12 22:45:10,839 Stage-1 map = 60%,  reduce = 9%
2016-04-12 22:45:13,864 Stage-1 map = 63%,  reduce = 9%
2016-04-12 22:45:16,895 Stage-1 map = 67%,  reduce = 9%
2016-04-12 22:45:19,923 Stage-1 map = 70%,  reduce = 9%
2016-04-12 22:45:22,947 Stage-1 map = 73%,  reduce = 9%
2016-04-12 22:45:25,971 Stage-1 map = 76%,  reduce = 9%
2016-04-12 22:45:28,995 Stage-1 map = 78%,  reduce = 10%
2016-04-12 22:45:32,019 Stage-1 map = 80%,  reduce = 10%
2016-04-12 22:45:35,041 Stage-1 map = 81%,  reduce = 10%
2016-04-12 22:45:38,064 Stage-1 map = 83%,  reduce = 11%
2016-04-12 22:45:41,085 Stage-1 map = 85%,  reduce = 12%
2016-04-12 22:45:44,106 Stage-1 map = 88%,  reduce = 13%
2016-04-12 22:45:47,127 Stage-1 map = 90%,  reduce = 13%
2016-04-12 22:45:50,148 Stage-1 map = 93%,  reduce = 13%
2016-04-12 22:45:52,163 Stage-1 map = 95%,  reduce = 13%
2016-04-12 22:45:55,184 Stage-1 map = 97%,  reduce = 13%
2016-04-12 22:45:56,193 Stage-1 map = 98%,  reduce = 13%
2016-04-12 22:45:58,209 Stage-1 map = 99%,  reduce = 14%
2016-04-12 22:46:01,230 Stage-1 map = 100%,  reduce = 14%
2016-04-12 22:46:07,268 Stage-1 map = 100%,  reduce = 16%
2016-04-12 22:46:10,289 Stage-1 map = 100%,  reduce = 21%
2016-04-12 22:46:13,310 Stage-1 map = 100%,  reduce = 34%
2016-04-12 22:46:16,332 Stage-1 map = 100%,  reduce = 45%
2016-04-12 22:46:19,353 Stage-1 map = 100%,  reduce = 49%
2016-04-12 22:46:22,373 Stage-1 map = 100%,  reduce = 54%
2016-04-12 22:46:25,394 Stage-1 map = 100%,  reduce = 63%
2016-04-12 22:46:28,414 Stage-1 map = 100%,  reduce = 74%
2016-04-12 22:46:31,435 Stage-1 map = 100%,  reduce = 88%
2016-04-12 22:46:34,456 Stage-1 map = 100%,  reduce = 96%
2016-04-12 22:46:37,477 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122243_0002
Loading data to table q20_tmp2
5441669 Rows loaded to q20_tmp2
OK
Time taken: 177.16 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122243_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122243_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122243_0003
2016-04-12 22:46:46,349 Stage-2 map = 0%,  reduce = 0%
2016-04-12 22:46:52,382 Stage-2 map = 25%,  reduce = 0%
2016-04-12 22:46:55,401 Stage-2 map = 40%,  reduce = 0%
2016-04-12 22:46:58,420 Stage-2 map = 58%,  reduce = 0%
2016-04-12 22:47:01,439 Stage-2 map = 72%,  reduce = 4%
2016-04-12 22:47:04,458 Stage-2 map = 83%,  reduce = 13%
2016-04-12 22:47:07,477 Stage-2 map = 93%,  reduce = 13%
2016-04-12 22:47:10,496 Stage-2 map = 99%,  reduce = 17%
2016-04-12 22:47:13,514 Stage-2 map = 100%,  reduce = 17%
2016-04-12 22:47:16,532 Stage-2 map = 100%,  reduce = 25%
2016-04-12 22:47:19,551 Stage-2 map = 100%,  reduce = 55%
2016-04-12 22:47:22,572 Stage-2 map = 100%,  reduce = 88%
2016-04-12 22:47:25,591 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122243_0003
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122243_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122243_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122243_0004
2016-04-12 22:47:35,055 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:47:44,098 Stage-1 map = 30%,  reduce = 0%
2016-04-12 22:47:47,116 Stage-1 map = 36%,  reduce = 0%
2016-04-12 22:47:50,135 Stage-1 map = 40%,  reduce = 0%
2016-04-12 22:47:53,153 Stage-1 map = 50%,  reduce = 10%
2016-04-12 22:47:56,172 Stage-1 map = 80%,  reduce = 10%
2016-04-12 22:48:02,207 Stage-1 map = 90%,  reduce = 13%
2016-04-12 22:48:05,224 Stage-1 map = 100%,  reduce = 13%
2016-04-12 22:48:08,242 Stage-1 map = 100%,  reduce = 27%
2016-04-12 22:48:11,261 Stage-1 map = 100%,  reduce = 68%
2016-04-12 22:48:14,279 Stage-1 map = 100%,  reduce = 86%
2016-04-12 22:48:17,299 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122243_0004
Loading data to table q20_tmp3
58782 Rows loaded to q20_tmp3
OK
Time taken: 99.712 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122243_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122243_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122243_0005
2016-04-12 22:48:25,907 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:48:28,923 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:48:37,974 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122243_0005
Loading data to table q20_tmp4
44482 Rows loaded to q20_tmp4
OK
Time taken: 20.721 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122243_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122243_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122243_0006
2016-04-12 22:48:46,694 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:48:49,711 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:48:58,762 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122243_0006
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122243_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122243_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122243_0007
2016-04-12 22:49:08,234 Stage-2 map = 0%,  reduce = 0%
2016-04-12 22:49:11,250 Stage-2 map = 100%,  reduce = 0%
2016-04-12 22:49:20,299 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122243_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122243_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122243_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122243_0008
2016-04-12 22:49:28,731 Stage-3 map = 0%,  reduce = 0%
2016-04-12 22:49:31,746 Stage-3 map = 100%,  reduce = 0%
2016-04-12 22:49:40,793 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604122243_0008
Loading data to table q20_potential_part_promotion
1804 Rows loaded to q20_potential_part_promotion
OK
Time taken: 62.759 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122250_1112297986.txt
OK
Time taken: 3.31 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.058 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q20_tmp1. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122250_511833045.txt
OK
Time taken: 2.845 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.606 seconds
OK
Time taken: 0.074 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.059 seconds
OK
Time taken: 0.188 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122250_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122250_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122250_0001
2016-04-12 22:50:30,101 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:50:39,181 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:50:48,273 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122250_0001
Loading data to table q20_tmp1
21551 Rows loaded to q20_tmp1
OK
Time taken: 27.513 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122250_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122250_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122250_0002
2016-04-12 22:50:57,123 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:51:06,177 Stage-1 map = 3%,  reduce = 0%
2016-04-12 22:51:09,196 Stage-1 map = 5%,  reduce = 0%
2016-04-12 22:51:12,215 Stage-1 map = 7%,  reduce = 0%
2016-04-12 22:51:15,234 Stage-1 map = 12%,  reduce = 0%
2016-04-12 22:51:18,253 Stage-1 map = 15%,  reduce = 0%
2016-04-12 22:51:21,275 Stage-1 map = 18%,  reduce = 0%
2016-04-12 22:51:24,305 Stage-1 map = 21%,  reduce = 0%
2016-04-12 22:51:27,340 Stage-1 map = 23%,  reduce = 0%
2016-04-12 22:51:30,372 Stage-1 map = 25%,  reduce = 0%
2016-04-12 22:51:33,405 Stage-1 map = 27%,  reduce = 1%
2016-04-12 22:51:36,438 Stage-1 map = 28%,  reduce = 1%
2016-04-12 22:51:39,470 Stage-1 map = 29%,  reduce = 1%
2016-04-12 22:51:42,503 Stage-1 map = 30%,  reduce = 2%
2016-04-12 22:51:45,532 Stage-1 map = 33%,  reduce = 3%
2016-04-12 22:51:48,561 Stage-1 map = 35%,  reduce = 4%
2016-04-12 22:51:51,591 Stage-1 map = 38%,  reduce = 4%
2016-04-12 22:51:54,623 Stage-1 map = 41%,  reduce = 4%
2016-04-12 22:51:56,644 Stage-1 map = 42%,  reduce = 4%
2016-04-12 22:51:57,657 Stage-1 map = 44%,  reduce = 4%
2016-04-12 22:51:59,677 Stage-1 map = 46%,  reduce = 5%
2016-04-12 22:52:02,704 Stage-1 map = 49%,  reduce = 5%
2016-04-12 22:52:05,731 Stage-1 map = 52%,  reduce = 6%
2016-04-12 22:52:08,758 Stage-1 map = 55%,  reduce = 6%
2016-04-12 22:52:11,786 Stage-1 map = 56%,  reduce = 6%
2016-04-12 22:52:14,813 Stage-1 map = 58%,  reduce = 7%
2016-04-12 22:52:17,839 Stage-1 map = 60%,  reduce = 8%
2016-04-12 22:52:20,865 Stage-1 map = 63%,  reduce = 9%
2016-04-12 22:52:23,898 Stage-1 map = 65%,  reduce = 9%
2016-04-12 22:52:26,923 Stage-1 map = 69%,  reduce = 9%
2016-04-12 22:52:29,943 Stage-1 map = 72%,  reduce = 9%
2016-04-12 22:52:32,965 Stage-1 map = 74%,  reduce = 10%
2016-04-12 22:52:35,986 Stage-1 map = 77%,  reduce = 10%
2016-04-12 22:52:39,007 Stage-1 map = 79%,  reduce = 10%
2016-04-12 22:52:42,029 Stage-1 map = 82%,  reduce = 10%
2016-04-12 22:52:45,050 Stage-1 map = 83%,  reduce = 10%
2016-04-12 22:52:48,072 Stage-1 map = 84%,  reduce = 11%
2016-04-12 22:52:51,093 Stage-1 map = 87%,  reduce = 13%
2016-04-12 22:52:54,114 Stage-1 map = 89%,  reduce = 13%
2016-04-12 22:52:57,135 Stage-1 map = 91%,  reduce = 13%
2016-04-12 22:53:00,156 Stage-1 map = 93%,  reduce = 13%
2016-04-12 22:53:03,177 Stage-1 map = 96%,  reduce = 14%
2016-04-12 22:53:06,198 Stage-1 map = 99%,  reduce = 14%
2016-04-12 22:53:09,218 Stage-1 map = 100%,  reduce = 14%
2016-04-12 22:53:12,238 Stage-1 map = 100%,  reduce = 15%
2016-04-12 22:53:18,277 Stage-1 map = 100%,  reduce = 16%
2016-04-12 22:53:21,297 Stage-1 map = 100%,  reduce = 34%
2016-04-12 22:53:24,318 Stage-1 map = 100%,  reduce = 40%
2016-04-12 22:53:27,339 Stage-1 map = 100%,  reduce = 49%
2016-04-12 22:53:30,359 Stage-1 map = 100%,  reduce = 54%
2016-04-12 22:53:33,379 Stage-1 map = 100%,  reduce = 60%
2016-04-12 22:53:36,400 Stage-1 map = 100%,  reduce = 71%
2016-04-12 22:53:39,421 Stage-1 map = 100%,  reduce = 84%
2016-04-12 22:53:42,441 Stage-1 map = 100%,  reduce = 96%
2016-04-12 22:53:45,462 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122250_0002
Loading data to table q20_tmp2
5441669 Rows loaded to q20_tmp2
OK
Time taken: 177.192 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122250_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122250_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122250_0003
2016-04-12 22:53:54,298 Stage-2 map = 0%,  reduce = 0%
2016-04-12 22:54:00,328 Stage-2 map = 25%,  reduce = 0%
2016-04-12 22:54:03,347 Stage-2 map = 40%,  reduce = 0%
2016-04-12 22:54:06,365 Stage-2 map = 57%,  reduce = 0%
2016-04-12 22:54:09,383 Stage-2 map = 72%,  reduce = 4%
2016-04-12 22:54:12,402 Stage-2 map = 83%,  reduce = 4%
2016-04-12 22:54:15,420 Stage-2 map = 93%,  reduce = 13%
2016-04-12 22:54:18,438 Stage-2 map = 99%,  reduce = 17%
2016-04-12 22:54:21,457 Stage-2 map = 100%,  reduce = 17%
2016-04-12 22:54:24,474 Stage-2 map = 100%,  reduce = 25%
2016-04-12 22:54:27,493 Stage-2 map = 100%,  reduce = 46%
2016-04-12 22:54:30,514 Stage-2 map = 100%,  reduce = 63%
2016-04-12 22:54:33,533 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122250_0003
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122250_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122250_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122250_0004
2016-04-12 22:54:42,038 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:54:51,081 Stage-1 map = 31%,  reduce = 0%
2016-04-12 22:54:54,099 Stage-1 map = 36%,  reduce = 0%
2016-04-12 22:54:57,117 Stage-1 map = 40%,  reduce = 0%
2016-04-12 22:55:00,136 Stage-1 map = 50%,  reduce = 10%
2016-04-12 22:55:03,155 Stage-1 map = 70%,  reduce = 10%
2016-04-12 22:55:06,174 Stage-1 map = 80%,  reduce = 10%
2016-04-12 22:55:09,193 Stage-1 map = 90%,  reduce = 10%
2016-04-12 22:55:12,212 Stage-1 map = 100%,  reduce = 10%
2016-04-12 22:55:15,230 Stage-1 map = 100%,  reduce = 27%
2016-04-12 22:55:18,249 Stage-1 map = 100%,  reduce = 68%
2016-04-12 22:55:21,267 Stage-1 map = 100%,  reduce = 88%
2016-04-12 22:55:24,286 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122250_0004
Loading data to table q20_tmp3
58782 Rows loaded to q20_tmp3
OK
Time taken: 98.759 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122250_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122250_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122250_0005
2016-04-12 22:55:33,905 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:55:36,921 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:55:44,967 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122250_0005
Loading data to table q20_tmp4
44482 Rows loaded to q20_tmp4
OK
Time taken: 20.634 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122250_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122250_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122250_0006
2016-04-12 22:55:54,574 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:55:57,591 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:56:06,641 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122250_0006
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122250_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122250_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122250_0007
2016-04-12 22:56:15,093 Stage-2 map = 0%,  reduce = 0%
2016-04-12 22:56:18,109 Stage-2 map = 100%,  reduce = 0%
2016-04-12 22:56:27,155 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122250_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122250_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122250_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122250_0008
2016-04-12 22:56:36,550 Stage-3 map = 0%,  reduce = 0%
2016-04-12 22:56:39,565 Stage-3 map = 100%,  reduce = 0%
2016-04-12 22:56:48,612 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604122250_0008
Loading data to table q20_potential_part_promotion
1804 Rows loaded to q20_potential_part_promotion
OK
Time taken: 63.732 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122257_1049180405.txt
OK
Time taken: 3.581 seconds
OK
Time taken: 0.073 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.066 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q20_tmp1. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122257_1408459407.txt
OK
Time taken: 3.009 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.635 seconds
OK
Time taken: 0.074 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.174 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.025 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.091 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122257_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122257_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122257_0001
2016-04-12 22:57:38,119 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:57:47,198 Stage-1 map = 100%,  reduce = 0%
2016-04-12 22:57:56,296 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122257_0001
Loading data to table q20_tmp1
21551 Rows loaded to q20_tmp1
OK
Time taken: 26.46 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122257_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122257_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122257_0002
2016-04-12 22:58:05,125 Stage-1 map = 0%,  reduce = 0%
2016-04-12 22:58:14,179 Stage-1 map = 2%,  reduce = 0%
2016-04-12 22:58:17,199 Stage-1 map = 5%,  reduce = 0%
2016-04-12 22:58:20,218 Stage-1 map = 8%,  reduce = 0%
2016-04-12 22:58:23,237 Stage-1 map = 12%,  reduce = 0%
2016-04-12 22:58:26,256 Stage-1 map = 15%,  reduce = 0%
2016-04-12 22:58:29,278 Stage-1 map = 18%,  reduce = 0%
2016-04-12 22:58:32,308 Stage-1 map = 21%,  reduce = 0%
2016-04-12 22:58:35,342 Stage-1 map = 23%,  reduce = 0%
2016-04-12 22:58:38,373 Stage-1 map = 24%,  reduce = 0%
2016-04-12 22:58:41,404 Stage-1 map = 27%,  reduce = 1%
2016-04-12 22:58:44,437 Stage-1 map = 28%,  reduce = 1%
2016-04-12 22:58:47,468 Stage-1 map = 29%,  reduce = 1%
2016-04-12 22:58:50,499 Stage-1 map = 30%,  reduce = 2%
2016-04-12 22:58:53,529 Stage-1 map = 32%,  reduce = 4%
2016-04-12 22:58:56,560 Stage-1 map = 35%,  reduce = 4%
2016-04-12 22:58:59,591 Stage-1 map = 38%,  reduce = 4%
2016-04-12 22:59:02,623 Stage-1 map = 42%,  reduce = 4%
2016-04-12 22:59:05,654 Stage-1 map = 44%,  reduce = 4%
2016-04-12 22:59:08,683 Stage-1 map = 47%,  reduce = 5%
2016-04-12 22:59:11,709 Stage-1 map = 50%,  reduce = 5%
2016-04-12 22:59:14,737 Stage-1 map = 53%,  reduce = 6%
2016-04-12 22:59:17,763 Stage-1 map = 56%,  reduce = 6%
2016-04-12 22:59:23,812 Stage-1 map = 58%,  reduce = 6%
2016-04-12 22:59:26,838 Stage-1 map = 59%,  reduce = 8%
2016-04-12 22:59:29,864 Stage-1 map = 62%,  reduce = 9%
2016-04-12 22:59:32,892 Stage-1 map = 65%,  reduce = 9%
2016-04-12 22:59:35,919 Stage-1 map = 67%,  reduce = 9%
2016-04-12 22:59:38,941 Stage-1 map = 70%,  reduce = 9%
2016-04-12 22:59:41,966 Stage-1 map = 73%,  reduce = 9%
2016-04-12 22:59:44,989 Stage-1 map = 75%,  reduce = 10%
2016-04-12 22:59:48,012 Stage-1 map = 78%,  reduce = 10%
2016-04-12 22:59:51,035 Stage-1 map = 80%,  reduce = 10%
2016-04-12 22:59:54,059 Stage-1 map = 83%,  reduce = 11%
2016-04-12 22:59:57,080 Stage-1 map = 84%,  reduce = 12%
2016-04-12 23:00:00,101 Stage-1 map = 87%,  reduce = 13%
2016-04-12 23:00:03,123 Stage-1 map = 89%,  reduce = 13%
2016-04-12 23:00:06,143 Stage-1 map = 91%,  reduce = 13%
2016-04-12 23:00:09,163 Stage-1 map = 93%,  reduce = 13%
2016-04-12 23:00:12,184 Stage-1 map = 96%,  reduce = 14%
2016-04-12 23:00:15,205 Stage-1 map = 97%,  reduce = 14%
2016-04-12 23:00:18,226 Stage-1 map = 98%,  reduce = 14%
2016-04-12 23:00:20,240 Stage-1 map = 98%,  reduce = 15%
2016-04-12 23:00:21,249 Stage-1 map = 99%,  reduce = 15%
2016-04-12 23:00:23,264 Stage-1 map = 100%,  reduce = 16%
2016-04-12 23:00:29,303 Stage-1 map = 100%,  reduce = 20%
2016-04-12 23:00:32,326 Stage-1 map = 100%,  reduce = 27%
2016-04-12 23:00:35,350 Stage-1 map = 100%,  reduce = 47%
2016-04-12 23:00:38,371 Stage-1 map = 100%,  reduce = 49%
2016-04-12 23:00:41,393 Stage-1 map = 100%,  reduce = 50%
2016-04-12 23:00:44,415 Stage-1 map = 100%,  reduce = 58%
2016-04-12 23:00:47,437 Stage-1 map = 100%,  reduce = 71%
2016-04-12 23:00:50,459 Stage-1 map = 100%,  reduce = 92%
2016-04-12 23:00:53,480 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122257_0002
Loading data to table q20_tmp2
5441669 Rows loaded to q20_tmp2
OK
Time taken: 177.317 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122257_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122257_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122257_0003
2016-04-12 23:01:02,378 Stage-2 map = 0%,  reduce = 0%
2016-04-12 23:01:08,410 Stage-2 map = 25%,  reduce = 0%
2016-04-12 23:01:11,430 Stage-2 map = 40%,  reduce = 0%
2016-04-12 23:01:14,450 Stage-2 map = 57%,  reduce = 0%
2016-04-12 23:01:17,470 Stage-2 map = 72%,  reduce = 4%
2016-04-12 23:01:20,490 Stage-2 map = 82%,  reduce = 4%
2016-04-12 23:01:23,510 Stage-2 map = 93%,  reduce = 8%
2016-04-12 23:01:26,547 Stage-2 map = 97%,  reduce = 8%
2016-04-12 23:01:29,565 Stage-2 map = 100%,  reduce = 8%
2016-04-12 23:01:32,583 Stage-2 map = 100%,  reduce = 21%
2016-04-12 23:01:38,618 Stage-2 map = 100%,  reduce = 25%
2016-04-12 23:01:41,639 Stage-2 map = 100%,  reduce = 89%
2016-04-12 23:01:44,659 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122257_0003
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122257_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122257_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122257_0004
2016-04-12 23:01:54,160 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:02:03,205 Stage-1 map = 11%,  reduce = 0%
2016-04-12 23:02:06,222 Stage-1 map = 36%,  reduce = 0%
2016-04-12 23:02:09,241 Stage-1 map = 40%,  reduce = 0%
2016-04-12 23:02:15,276 Stage-1 map = 70%,  reduce = 10%
2016-04-12 23:02:18,294 Stage-1 map = 80%,  reduce = 10%
2016-04-12 23:02:24,330 Stage-1 map = 100%,  reduce = 27%
2016-04-12 23:02:36,397 Stage-1 map = 100%,  reduce = 74%
2016-04-12 23:02:39,418 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122257_0004
Loading data to table q20_tmp3
58782 Rows loaded to q20_tmp3
OK
Time taken: 105.731 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122257_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122257_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122257_0005
2016-04-12 23:02:48,024 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:02:51,040 Stage-1 map = 100%,  reduce = 0%
2016-04-12 23:03:00,092 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122257_0005
Loading data to table q20_tmp4
44482 Rows loaded to q20_tmp4
OK
Time taken: 20.69 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122257_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122257_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122257_0006
2016-04-12 23:03:08,754 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:03:11,771 Stage-1 map = 100%,  reduce = 0%
2016-04-12 23:03:20,823 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122257_0006
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122257_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122257_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122257_0007
2016-04-12 23:03:30,291 Stage-2 map = 0%,  reduce = 0%
2016-04-12 23:03:33,308 Stage-2 map = 50%,  reduce = 0%
2016-04-12 23:03:36,327 Stage-2 map = 100%,  reduce = 0%
2016-04-12 23:03:42,362 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122257_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122257_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122257_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122257_0008
2016-04-12 23:03:50,805 Stage-3 map = 0%,  reduce = 0%
2016-04-12 23:03:53,821 Stage-3 map = 100%,  reduce = 0%
2016-04-12 23:04:02,869 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604122257_0008
Loading data to table q20_potential_part_promotion
1804 Rows loaded to q20_potential_part_promotion
OK
Time taken: 62.773 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122304_1481015810.txt
OK
Time taken: 3.547 seconds
OK
Time taken: 0.073 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.083 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q20_tmp1. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122304_1559059145.txt
OK
Time taken: 2.903 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.586 seconds
OK
Time taken: 0.074 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.163 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122304_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122304_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122304_0001
2016-04-12 23:04:53,517 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:05:02,593 Stage-1 map = 100%,  reduce = 0%
2016-04-12 23:05:11,686 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122304_0001
Loading data to table q20_tmp1
21551 Rows loaded to q20_tmp1
OK
Time taken: 26.917 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122304_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122304_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122304_0002
2016-04-12 23:05:20,509 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:05:29,563 Stage-1 map = 3%,  reduce = 0%
2016-04-12 23:05:32,582 Stage-1 map = 4%,  reduce = 0%
2016-04-12 23:05:35,601 Stage-1 map = 7%,  reduce = 0%
2016-04-12 23:05:38,619 Stage-1 map = 11%,  reduce = 0%
2016-04-12 23:05:41,638 Stage-1 map = 15%,  reduce = 0%
2016-04-12 23:05:44,661 Stage-1 map = 20%,  reduce = 0%
2016-04-12 23:05:47,690 Stage-1 map = 23%,  reduce = 0%
2016-04-12 23:05:50,725 Stage-1 map = 24%,  reduce = 0%
2016-04-12 23:05:53,756 Stage-1 map = 26%,  reduce = 0%
2016-04-12 23:05:56,789 Stage-1 map = 27%,  reduce = 1%
2016-04-12 23:05:59,823 Stage-1 map = 28%,  reduce = 2%
2016-04-12 23:06:02,854 Stage-1 map = 30%,  reduce = 2%
2016-04-12 23:06:05,885 Stage-1 map = 30%,  reduce = 3%
2016-04-12 23:06:08,915 Stage-1 map = 33%,  reduce = 4%
2016-04-12 23:06:11,947 Stage-1 map = 35%,  reduce = 4%
2016-04-12 23:06:14,977 Stage-1 map = 38%,  reduce = 4%
2016-04-12 23:06:18,009 Stage-1 map = 42%,  reduce = 4%
2016-04-12 23:06:21,040 Stage-1 map = 45%,  reduce = 4%
2016-04-12 23:06:24,070 Stage-1 map = 47%,  reduce = 5%
2016-04-12 23:06:27,096 Stage-1 map = 51%,  reduce = 5%
2016-04-12 23:06:30,125 Stage-1 map = 53%,  reduce = 6%
2016-04-12 23:06:33,154 Stage-1 map = 55%,  reduce = 6%
2016-04-12 23:06:36,180 Stage-1 map = 56%,  reduce = 6%
2016-04-12 23:06:39,207 Stage-1 map = 58%,  reduce = 8%
2016-04-12 23:06:42,233 Stage-1 map = 60%,  reduce = 8%
2016-04-12 23:06:45,262 Stage-1 map = 63%,  reduce = 9%
2016-04-12 23:06:48,290 Stage-1 map = 66%,  reduce = 9%
2016-04-12 23:06:50,309 Stage-1 map = 68%,  reduce = 9%
2016-04-12 23:06:51,320 Stage-1 map = 69%,  reduce = 9%
2016-04-12 23:06:53,337 Stage-1 map = 71%,  reduce = 9%
2016-04-12 23:06:54,349 Stage-1 map = 72%,  reduce = 9%
2016-04-12 23:06:56,366 Stage-1 map = 75%,  reduce = 9%
2016-04-12 23:06:59,391 Stage-1 map = 78%,  reduce = 10%
2016-04-12 23:07:02,414 Stage-1 map = 80%,  reduce = 10%
2016-04-12 23:07:05,438 Stage-1 map = 82%,  reduce = 11%
2016-04-12 23:07:08,461 Stage-1 map = 84%,  reduce = 11%
2016-04-12 23:07:11,483 Stage-1 map = 85%,  reduce = 12%
2016-04-12 23:07:14,504 Stage-1 map = 87%,  reduce = 12%
2016-04-12 23:07:17,525 Stage-1 map = 89%,  reduce = 12%
2016-04-12 23:07:20,545 Stage-1 map = 92%,  reduce = 13%
2016-04-12 23:07:23,566 Stage-1 map = 93%,  reduce = 13%
2016-04-12 23:07:26,588 Stage-1 map = 96%,  reduce = 14%
2016-04-12 23:07:29,610 Stage-1 map = 97%,  reduce = 14%
2016-04-12 23:07:35,649 Stage-1 map = 99%,  reduce = 15%
2016-04-12 23:07:38,670 Stage-1 map = 100%,  reduce = 16%
2016-04-12 23:07:44,709 Stage-1 map = 100%,  reduce = 20%
2016-04-12 23:07:47,731 Stage-1 map = 100%,  reduce = 26%
2016-04-12 23:07:50,753 Stage-1 map = 100%,  reduce = 42%
2016-04-12 23:07:53,777 Stage-1 map = 100%,  reduce = 47%
2016-04-12 23:07:56,799 Stage-1 map = 100%,  reduce = 50%
2016-04-12 23:07:59,820 Stage-1 map = 100%,  reduce = 54%
2016-04-12 23:08:02,842 Stage-1 map = 100%,  reduce = 67%
2016-04-12 23:08:05,864 Stage-1 map = 100%,  reduce = 88%
2016-04-12 23:08:08,885 Stage-1 map = 100%,  reduce = 94%
2016-04-12 23:08:11,907 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122304_0002
Loading data to table q20_tmp2
5441669 Rows loaded to q20_tmp2
OK
Time taken: 180.226 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122304_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122304_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122304_0003
2016-04-12 23:08:20,714 Stage-2 map = 0%,  reduce = 0%
2016-04-12 23:08:26,744 Stage-2 map = 25%,  reduce = 0%
2016-04-12 23:08:29,763 Stage-2 map = 43%,  reduce = 0%
2016-04-12 23:08:32,783 Stage-2 map = 59%,  reduce = 0%
2016-04-12 23:08:35,802 Stage-2 map = 65%,  reduce = 8%
2016-04-12 23:08:38,822 Stage-2 map = 72%,  reduce = 17%
2016-04-12 23:08:41,841 Stage-2 map = 78%,  reduce = 17%
2016-04-12 23:08:44,859 Stage-2 map = 84%,  reduce = 17%
2016-04-12 23:08:47,877 Stage-2 map = 91%,  reduce = 17%
2016-04-12 23:08:50,896 Stage-2 map = 99%,  reduce = 17%
2016-04-12 23:08:53,914 Stage-2 map = 100%,  reduce = 17%
2016-04-12 23:09:05,981 Stage-2 map = 100%,  reduce = 51%
2016-04-12 23:09:09,002 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122304_0003
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122304_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122304_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122304_0004
2016-04-12 23:09:18,484 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:09:26,522 Stage-1 map = 11%,  reduce = 0%
2016-04-12 23:09:27,530 Stage-1 map = 31%,  reduce = 0%
2016-04-12 23:09:29,544 Stage-1 map = 36%,  reduce = 0%
2016-04-12 23:09:32,563 Stage-1 map = 40%,  reduce = 0%
2016-04-12 23:09:35,581 Stage-1 map = 60%,  reduce = 10%
2016-04-12 23:09:38,598 Stage-1 map = 70%,  reduce = 10%
2016-04-12 23:09:44,635 Stage-1 map = 100%,  reduce = 20%
2016-04-12 23:09:50,670 Stage-1 map = 100%,  reduce = 23%
2016-04-12 23:09:56,706 Stage-1 map = 100%,  reduce = 70%
2016-04-12 23:09:59,725 Stage-1 map = 100%,  reduce = 95%
2016-04-12 23:10:02,745 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122304_0004
Loading data to table q20_tmp3
58782 Rows loaded to q20_tmp3
OK
Time taken: 110.738 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122304_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122304_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122304_0005
2016-04-12 23:10:12,339 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:10:15,355 Stage-1 map = 100%,  reduce = 0%
2016-04-12 23:10:24,406 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122304_0005
Loading data to table q20_tmp4
44482 Rows loaded to q20_tmp4
OK
Time taken: 21.684 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122304_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122304_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122304_0006
2016-04-12 23:10:33,096 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:10:36,114 Stage-1 map = 100%,  reduce = 0%
2016-04-12 23:10:45,168 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122304_0006
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122304_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122304_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122304_0007
2016-04-12 23:10:54,648 Stage-2 map = 0%,  reduce = 0%
2016-04-12 23:10:56,660 Stage-2 map = 100%,  reduce = 0%
2016-04-12 23:11:05,719 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122304_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122304_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122304_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122304_0008
2016-04-12 23:11:15,109 Stage-3 map = 0%,  reduce = 0%
2016-04-12 23:11:18,124 Stage-3 map = 100%,  reduce = 0%
2016-04-12 23:11:27,170 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604122304_0008
Loading data to table q20_potential_part_promotion
1804 Rows loaded to q20_potential_part_promotion
OK
Time taken: 62.74 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122312_55253980.txt
OK
Time taken: 3.446 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.058 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q20_tmp1. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604122312_272194605.txt
OK
Time taken: 2.861 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.619 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.158 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.066 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122312_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122312_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122312_0001
2016-04-12 23:12:17,568 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:12:23,623 Stage-1 map = 100%,  reduce = 0%
2016-04-12 23:12:32,732 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122312_0001
Loading data to table q20_tmp1
21551 Rows loaded to q20_tmp1
OK
Time taken: 23.927 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122312_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122312_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122312_0002
2016-04-12 23:12:41,527 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:12:50,580 Stage-1 map = 2%,  reduce = 0%
2016-04-12 23:12:53,599 Stage-1 map = 4%,  reduce = 0%
2016-04-12 23:12:56,618 Stage-1 map = 7%,  reduce = 0%
2016-04-12 23:12:59,637 Stage-1 map = 11%,  reduce = 0%
2016-04-12 23:13:02,656 Stage-1 map = 15%,  reduce = 0%
2016-04-12 23:13:05,678 Stage-1 map = 20%,  reduce = 0%
2016-04-12 23:13:08,707 Stage-1 map = 22%,  reduce = 0%
2016-04-12 23:13:11,738 Stage-1 map = 24%,  reduce = 0%
2016-04-12 23:13:14,771 Stage-1 map = 26%,  reduce = 0%
2016-04-12 23:13:17,802 Stage-1 map = 27%,  reduce = 0%
2016-04-12 23:13:20,835 Stage-1 map = 28%,  reduce = 2%
2016-04-12 23:13:23,866 Stage-1 map = 29%,  reduce = 2%
2016-04-12 23:13:26,896 Stage-1 map = 31%,  reduce = 2%
2016-04-12 23:13:29,927 Stage-1 map = 33%,  reduce = 4%
2016-04-12 23:13:32,957 Stage-1 map = 35%,  reduce = 4%
2016-04-12 23:13:35,987 Stage-1 map = 39%,  reduce = 4%
2016-04-12 23:13:39,020 Stage-1 map = 42%,  reduce = 4%
2016-04-12 23:13:42,050 Stage-1 map = 45%,  reduce = 4%
2016-04-12 23:13:45,081 Stage-1 map = 49%,  reduce = 5%
2016-04-12 23:13:48,108 Stage-1 map = 52%,  reduce = 5%
2016-04-12 23:13:51,136 Stage-1 map = 55%,  reduce = 6%
2016-04-12 23:13:54,162 Stage-1 map = 56%,  reduce = 6%
2016-04-12 23:13:57,189 Stage-1 map = 57%,  reduce = 6%
2016-04-12 23:14:00,215 Stage-1 map = 58%,  reduce = 7%
2016-04-12 23:14:03,241 Stage-1 map = 61%,  reduce = 8%
2016-04-12 23:14:06,270 Stage-1 map = 63%,  reduce = 9%
2016-04-12 23:14:09,301 Stage-1 map = 66%,  reduce = 9%
2016-04-12 23:14:12,326 Stage-1 map = 69%,  reduce = 9%
2016-04-12 23:14:15,350 Stage-1 map = 71%,  reduce = 9%
2016-04-12 23:14:18,373 Stage-1 map = 75%,  reduce = 10%
2016-04-12 23:14:21,395 Stage-1 map = 78%,  reduce = 10%
2016-04-12 23:14:24,418 Stage-1 map = 81%,  reduce = 10%
2016-04-12 23:14:27,441 Stage-1 map = 83%,  reduce = 10%
2016-04-12 23:14:30,463 Stage-1 map = 84%,  reduce = 11%
2016-04-12 23:14:33,484 Stage-1 map = 85%,  reduce = 12%
2016-04-12 23:14:36,505 Stage-1 map = 87%,  reduce = 13%
2016-04-12 23:14:39,525 Stage-1 map = 90%,  reduce = 13%
2016-04-12 23:14:42,546 Stage-1 map = 91%,  reduce = 13%
2016-04-12 23:14:45,566 Stage-1 map = 95%,  reduce = 14%
2016-04-12 23:14:48,587 Stage-1 map = 97%,  reduce = 14%
2016-04-12 23:14:51,608 Stage-1 map = 99%,  reduce = 14%
2016-04-12 23:14:53,624 Stage-1 map = 100%,  reduce = 14%
2016-04-12 23:14:56,644 Stage-1 map = 100%,  reduce = 16%
2016-04-12 23:14:59,691 Stage-1 map = 100%,  reduce = 21%
2016-04-12 23:15:02,712 Stage-1 map = 100%,  reduce = 34%
2016-04-12 23:15:05,734 Stage-1 map = 100%,  reduce = 42%
2016-04-12 23:15:08,755 Stage-1 map = 100%,  reduce = 48%
2016-04-12 23:15:11,777 Stage-1 map = 100%,  reduce = 54%
2016-04-12 23:15:14,799 Stage-1 map = 100%,  reduce = 63%
2016-04-12 23:15:17,821 Stage-1 map = 100%,  reduce = 75%
2016-04-12 23:15:20,842 Stage-1 map = 100%,  reduce = 88%
2016-04-12 23:15:23,862 Stage-1 map = 100%,  reduce = 96%
2016-04-12 23:15:26,883 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122312_0002
Loading data to table q20_tmp2
5441669 Rows loaded to q20_tmp2
OK
Time taken: 174.167 seconds
Total MapReduce jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122312_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122312_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122312_0003
2016-04-12 23:15:35,692 Stage-2 map = 0%,  reduce = 0%
2016-04-12 23:15:41,724 Stage-2 map = 25%,  reduce = 0%
2016-04-12 23:15:44,743 Stage-2 map = 45%,  reduce = 0%
2016-04-12 23:15:47,763 Stage-2 map = 58%,  reduce = 0%
2016-04-12 23:15:50,782 Stage-2 map = 65%,  reduce = 8%
2016-04-12 23:15:53,802 Stage-2 map = 71%,  reduce = 17%
2016-04-12 23:15:56,821 Stage-2 map = 78%,  reduce = 17%
2016-04-12 23:15:59,841 Stage-2 map = 84%,  reduce = 17%
2016-04-12 23:16:02,861 Stage-2 map = 91%,  reduce = 17%
2016-04-12 23:16:05,879 Stage-2 map = 99%,  reduce = 17%
2016-04-12 23:16:08,898 Stage-2 map = 100%,  reduce = 17%
2016-04-12 23:16:20,967 Stage-2 map = 100%,  reduce = 51%
2016-04-12 23:16:23,988 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122312_0003
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122312_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122312_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122312_0004
2016-04-12 23:16:33,460 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:16:42,502 Stage-1 map = 31%,  reduce = 0%
2016-04-12 23:16:45,521 Stage-1 map = 36%,  reduce = 0%
2016-04-12 23:16:48,540 Stage-1 map = 40%,  reduce = 0%
2016-04-12 23:16:51,558 Stage-1 map = 60%,  reduce = 10%
2016-04-12 23:16:54,577 Stage-1 map = 70%,  reduce = 10%
2016-04-12 23:17:00,613 Stage-1 map = 99%,  reduce = 10%
2016-04-12 23:17:03,632 Stage-1 map = 100%,  reduce = 23%
2016-04-12 23:17:06,651 Stage-1 map = 100%,  reduce = 30%
2016-04-12 23:17:15,702 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122312_0004
Loading data to table q20_tmp3
58782 Rows loaded to q20_tmp3
OK
Time taken: 108.772 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122312_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122312_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122312_0005
2016-04-12 23:17:24,343 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:17:27,359 Stage-1 map = 100%,  reduce = 0%
2016-04-12 23:17:36,410 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122312_0005
Loading data to table q20_tmp4
44482 Rows loaded to q20_tmp4
OK
Time taken: 20.724 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122312_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122312_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122312_0006
2016-04-12 23:17:45,182 Stage-1 map = 0%,  reduce = 0%
2016-04-12 23:17:48,198 Stage-1 map = 100%,  reduce = 0%
2016-04-12 23:17:57,250 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604122312_0006
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122312_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122312_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122312_0007
2016-04-12 23:18:06,715 Stage-2 map = 0%,  reduce = 0%
2016-04-12 23:18:09,731 Stage-2 map = 50%,  reduce = 0%
2016-04-12 23:18:12,749 Stage-2 map = 100%,  reduce = 0%
2016-04-12 23:18:18,781 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604122312_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604122312_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604122312_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604122312_0008
2016-04-12 23:18:27,630 Stage-3 map = 0%,  reduce = 0%
2016-04-12 23:18:30,645 Stage-3 map = 100%,  reduce = 0%
2016-04-12 23:18:39,691 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604122312_0008
Loading data to table q20_potential_part_promotion
1804 Rows loaded to q20_potential_part_promotion
OK
Time taken: 63.232 seconds
