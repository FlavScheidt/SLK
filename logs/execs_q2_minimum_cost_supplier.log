Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131957_2133089272.txt
OK
Time taken: 3.67 seconds
OK
Time taken: 0.072 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.059 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.239 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.025 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/hive/warehouse/q2_minimum_cost_supplier_tmp1. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:1761)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:1735)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.mkdirs(NameNode.java:542)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604131957_1638475128.txt
OK
Time taken: 3.707 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.059 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.24 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.084 seconds
OK
Time taken: 0.032 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131957_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131957_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131957_0001
2016-04-13 19:57:21,842 Stage-4 map = 0%,  reduce = 0%
2016-04-13 19:57:24,877 Stage-4 map = 50%,  reduce = 0%
2016-04-13 19:57:27,918 Stage-4 map = 100%,  reduce = 0%
2016-04-13 19:57:33,988 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604131957_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131957_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131957_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131957_0002
2016-04-13 19:57:42,592 Stage-1 map = 0%,  reduce = 0%
2016-04-13 19:57:45,614 Stage-1 map = 50%,  reduce = 0%
2016-04-13 19:57:48,645 Stage-1 map = 100%,  reduce = 0%
2016-04-13 19:57:54,697 Stage-1 map = 100%,  reduce = 33%
2016-04-13 19:57:57,727 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131957_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131957_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131957_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131957_0003
2016-04-13 19:58:06,330 Stage-2 map = 0%,  reduce = 0%
2016-04-13 19:58:15,381 Stage-2 map = 15%,  reduce = 0%
2016-04-13 19:58:18,404 Stage-2 map = 21%,  reduce = 0%
2016-04-13 19:58:27,478 Stage-2 map = 21%,  reduce = 4%
2016-04-13 19:58:30,506 Stage-2 map = 36%,  reduce = 7%
2016-04-13 19:58:33,535 Stage-2 map = 42%,  reduce = 7%
2016-04-13 19:58:42,613 Stage-2 map = 57%,  reduce = 11%
2016-04-13 19:58:45,643 Stage-2 map = 63%,  reduce = 14%
2016-04-13 19:58:48,673 Stage-2 map = 63%,  reduce = 18%
2016-04-13 19:58:51,703 Stage-2 map = 63%,  reduce = 21%
2016-04-13 19:58:54,734 Stage-2 map = 78%,  reduce = 21%
2016-04-13 19:58:57,760 Stage-2 map = 84%,  reduce = 21%
2016-04-13 19:59:00,785 Stage-2 map = 89%,  reduce = 21%
2016-04-13 19:59:03,810 Stage-2 map = 89%,  reduce = 25%
2016-04-13 19:59:06,835 Stage-2 map = 97%,  reduce = 26%
2016-04-13 19:59:09,859 Stage-2 map = 100%,  reduce = 30%
2016-04-13 19:59:18,927 Stage-2 map = 100%,  reduce = 48%
2016-04-13 19:59:21,957 Stage-2 map = 100%,  reduce = 54%
2016-04-13 19:59:24,985 Stage-2 map = 100%,  reduce = 91%
2016-04-13 19:59:28,011 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131957_0003
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131957_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131957_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131957_0004
2016-04-13 19:59:36,627 Stage-3 map = 0%,  reduce = 0%
2016-04-13 19:59:45,672 Stage-3 map = 33%,  reduce = 0%
2016-04-13 19:59:48,690 Stage-3 map = 50%,  reduce = 0%
2016-04-13 20:00:00,760 Stage-3 map = 86%,  reduce = 0%
2016-04-13 20:00:03,781 Stage-3 map = 100%,  reduce = 25%
2016-04-13 20:00:12,840 Stage-3 map = 100%,  reduce = 94%
2016-04-13 20:00:15,863 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604131957_0004
Loading data to table q2_minimum_cost_supplier_tmp1
6351 Rows loaded to q2_minimum_cost_supplier_tmp1
OK
Time taken: 182.401 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131957_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131957_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131957_0005
2016-04-13 20:00:24,533 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:00:27,549 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:00:36,599 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131957_0005
Loading data to table q2_minimum_cost_supplier_tmp2
4667 Rows loaded to q2_minimum_cost_supplier_tmp2
OK
Time taken: 20.75 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131957_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131957_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131957_0006
2016-04-13 20:00:46,240 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:00:49,257 Stage-1 map = 50%,  reduce = 0%
2016-04-13 20:00:52,275 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:00:58,310 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604131957_0006
Launching Job 2 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131957_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131957_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131957_0007
2016-04-13 20:01:06,717 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:01:09,734 Stage-2 map = 100%,  reduce = 0%
2016-04-13 20:01:18,801 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604131957_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604131957_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604131957_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604131957_0008
2016-04-13 20:01:28,266 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:01:31,282 Stage-3 map = 100%,  reduce = 0%
2016-04-13 20:01:40,331 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604131957_0008
Loading data to table q2_minimum_cost_supplier
100 Rows loaded to q2_minimum_cost_supplier
OK
Time taken: 63.69 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132002_884522806.txt
OK
Time taken: 3.495 seconds
OK
Time taken: 0.057 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.05 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q2_minimum_cost_supplier. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132002_1853052078.txt
OK
Time taken: 3.044 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.609 seconds
OK
Time taken: 0.073 seconds
OK
Time taken: 0.18 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132002_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132002_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132002_0001
2016-04-13 20:02:23,360 Stage-4 map = 0%,  reduce = 0%
2016-04-13 20:02:26,395 Stage-4 map = 50%,  reduce = 0%
2016-04-13 20:02:29,437 Stage-4 map = 100%,  reduce = 0%
2016-04-13 20:02:35,523 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604132002_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132002_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132002_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132002_0002
2016-04-13 20:02:45,171 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:02:48,195 Stage-1 map = 50%,  reduce = 0%
2016-04-13 20:02:50,217 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:02:56,268 Stage-1 map = 100%,  reduce = 33%
2016-04-13 20:02:59,299 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132002_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132002_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132002_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132002_0003
2016-04-13 20:03:08,882 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:03:17,933 Stage-2 map = 15%,  reduce = 0%
2016-04-13 20:03:20,957 Stage-2 map = 21%,  reduce = 0%
2016-04-13 20:03:30,032 Stage-2 map = 21%,  reduce = 7%
2016-04-13 20:03:33,060 Stage-2 map = 36%,  reduce = 7%
2016-04-13 20:03:36,090 Stage-2 map = 42%,  reduce = 7%
2016-04-13 20:03:45,166 Stage-2 map = 57%,  reduce = 14%
2016-04-13 20:03:48,196 Stage-2 map = 63%,  reduce = 14%
2016-04-13 20:03:51,226 Stage-2 map = 63%,  reduce = 21%
2016-04-13 20:03:57,277 Stage-2 map = 81%,  reduce = 21%
2016-04-13 20:04:00,304 Stage-2 map = 84%,  reduce = 21%
2016-04-13 20:04:06,351 Stage-2 map = 92%,  reduce = 26%
2016-04-13 20:04:08,371 Stage-2 map = 97%,  reduce = 27%
2016-04-13 20:04:09,382 Stage-2 map = 100%,  reduce = 28%
2016-04-13 20:04:11,400 Stage-2 map = 100%,  reduce = 31%
2016-04-13 20:04:12,412 Stage-2 map = 100%,  reduce = 33%
2016-04-13 20:04:14,432 Stage-2 map = 100%,  reduce = 69%
2016-04-13 20:04:17,457 Stage-2 map = 100%,  reduce = 85%
2016-04-13 20:04:20,484 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132002_0003
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132002_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132002_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132002_0004
2016-04-13 20:04:30,045 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:04:39,091 Stage-3 map = 33%,  reduce = 0%
2016-04-13 20:04:42,109 Stage-3 map = 50%,  reduce = 0%
2016-04-13 20:04:54,180 Stage-3 map = 75%,  reduce = 0%
2016-04-13 20:04:57,205 Stage-3 map = 100%,  reduce = 25%
2016-04-13 20:05:06,266 Stage-3 map = 100%,  reduce = 95%
2016-04-13 20:05:09,291 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132002_0004
Loading data to table q2_minimum_cost_supplier_tmp1
6351 Rows loaded to q2_minimum_cost_supplier_tmp1
OK
Time taken: 174.35 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132002_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132002_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132002_0005
2016-04-13 20:05:17,918 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:05:20,934 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:05:29,985 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132002_0005
Loading data to table q2_minimum_cost_supplier_tmp2
4667 Rows loaded to q2_minimum_cost_supplier_tmp2
OK
Time taken: 20.666 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132002_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132002_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132002_0006
2016-04-13 20:05:38,559 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:05:41,575 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:05:50,626 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132002_0006
Launching Job 2 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132002_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132002_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132002_0007
2016-04-13 20:06:00,073 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:06:03,089 Stage-2 map = 100%,  reduce = 0%
2016-04-13 20:06:12,139 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132002_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132002_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132002_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132002_0008
2016-04-13 20:06:20,638 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:06:23,654 Stage-3 map = 100%,  reduce = 0%
2016-04-13 20:06:32,703 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132002_0008
Loading data to table q2_minimum_cost_supplier
100 Rows loaded to q2_minimum_cost_supplier
OK
Time taken: 62.69 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132006_817029455.txt
OK
Time taken: 3.608 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.059 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.05 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q2_minimum_cost_supplier. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132007_1905219707.txt
OK
Time taken: 2.973 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.623 seconds
OK
Time taken: 0.073 seconds
OK
Time taken: 0.181 seconds
OK
Time taken: 0.051 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.033 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132006_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132006_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132006_0001
2016-04-13 20:07:15,145 Stage-4 map = 0%,  reduce = 0%
2016-04-13 20:07:18,180 Stage-4 map = 50%,  reduce = 0%
2016-04-13 20:07:21,222 Stage-4 map = 100%,  reduce = 0%
2016-04-13 20:07:27,305 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604132006_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132006_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132006_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132006_0002
2016-04-13 20:07:36,368 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:07:39,392 Stage-1 map = 50%,  reduce = 0%
2016-04-13 20:07:42,421 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:07:48,473 Stage-1 map = 100%,  reduce = 33%
2016-04-13 20:07:51,505 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132006_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132006_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132006_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132006_0003
2016-04-13 20:08:00,121 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:08:09,171 Stage-2 map = 15%,  reduce = 0%
2016-04-13 20:08:12,194 Stage-2 map = 21%,  reduce = 0%
2016-04-13 20:08:21,268 Stage-2 map = 21%,  reduce = 4%
2016-04-13 20:08:24,297 Stage-2 map = 36%,  reduce = 7%
2016-04-13 20:08:27,331 Stage-2 map = 42%,  reduce = 7%
2016-04-13 20:08:36,408 Stage-2 map = 57%,  reduce = 11%
2016-04-13 20:08:39,439 Stage-2 map = 63%,  reduce = 14%
2016-04-13 20:08:42,468 Stage-2 map = 63%,  reduce = 18%
2016-04-13 20:08:45,498 Stage-2 map = 63%,  reduce = 21%
2016-04-13 20:08:48,529 Stage-2 map = 81%,  reduce = 21%
2016-04-13 20:08:51,557 Stage-2 map = 84%,  reduce = 21%
2016-04-13 20:08:57,604 Stage-2 map = 97%,  reduce = 23%
2016-04-13 20:08:59,623 Stage-2 map = 97%,  reduce = 26%
2016-04-13 20:09:00,634 Stage-2 map = 100%,  reduce = 28%
2016-04-13 20:09:02,654 Stage-2 map = 100%,  reduce = 30%
2016-04-13 20:09:05,680 Stage-2 map = 100%,  reduce = 51%
2016-04-13 20:09:08,705 Stage-2 map = 100%,  reduce = 85%
2016-04-13 20:09:11,732 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132006_0003
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132006_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132006_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132006_0004
2016-04-13 20:09:21,556 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:09:30,605 Stage-3 map = 32%,  reduce = 0%
2016-04-13 20:09:33,622 Stage-3 map = 49%,  reduce = 0%
2016-04-13 20:09:36,641 Stage-3 map = 50%,  reduce = 0%
2016-04-13 20:09:45,704 Stage-3 map = 75%,  reduce = 0%
2016-04-13 20:09:47,721 Stage-3 map = 100%,  reduce = 0%
2016-04-13 20:09:50,743 Stage-3 map = 100%,  reduce = 25%
2016-04-13 20:09:59,806 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132006_0004
Loading data to table q2_minimum_cost_supplier_tmp1
6351 Rows loaded to q2_minimum_cost_supplier_tmp1
OK
Time taken: 173.533 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132006_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132006_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132006_0005
2016-04-13 20:10:09,500 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:10:12,517 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:10:21,567 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132006_0005
Loading data to table q2_minimum_cost_supplier_tmp2
4667 Rows loaded to q2_minimum_cost_supplier_tmp2
OK
Time taken: 21.734 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132006_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132006_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132006_0006
2016-04-13 20:10:30,178 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:10:33,194 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:10:42,246 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132006_0006
Launching Job 2 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132006_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132006_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132006_0007
2016-04-13 20:10:51,726 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:10:54,743 Stage-2 map = 100%,  reduce = 0%
2016-04-13 20:11:03,793 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132006_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132006_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132006_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132006_0008
2016-04-13 20:11:12,243 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:11:15,259 Stage-3 map = 50%,  reduce = 0%
2016-04-13 20:11:18,277 Stage-3 map = 100%,  reduce = 0%
2016-04-13 20:11:24,310 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132006_0008
Loading data to table q2_minimum_cost_supplier
100 Rows loaded to q2_minimum_cost_supplier
OK
Time taken: 62.698 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132011_550777054.txt
OK
Time taken: 3.573 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.059 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.05 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q2_minimum_cost_supplier. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132011_305907940.txt
OK
Time taken: 2.985 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.618 seconds
OK
Time taken: 0.073 seconds
OK
Time taken: 0.189 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.05 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.034 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132011_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132011_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132011_0001
2016-04-13 20:12:07,223 Stage-4 map = 0%,  reduce = 0%
2016-04-13 20:12:10,260 Stage-4 map = 100%,  reduce = 0%
2016-04-13 20:12:19,363 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604132011_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132011_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132011_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132011_0002
2016-04-13 20:12:28,013 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:12:31,036 Stage-1 map = 50%,  reduce = 0%
2016-04-13 20:12:34,065 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:12:40,117 Stage-1 map = 100%,  reduce = 33%
2016-04-13 20:12:43,147 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132011_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132011_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132011_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132011_0003
2016-04-13 20:12:52,742 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:13:01,793 Stage-2 map = 15%,  reduce = 0%
2016-04-13 20:13:04,816 Stage-2 map = 21%,  reduce = 0%
2016-04-13 20:13:16,913 Stage-2 map = 35%,  reduce = 7%
2016-04-13 20:13:19,943 Stage-2 map = 42%,  reduce = 7%
2016-04-13 20:13:29,021 Stage-2 map = 57%,  reduce = 11%
2016-04-13 20:13:32,051 Stage-2 map = 63%,  reduce = 14%
2016-04-13 20:13:35,082 Stage-2 map = 63%,  reduce = 21%
2016-04-13 20:13:41,135 Stage-2 map = 79%,  reduce = 21%
2016-04-13 20:13:44,167 Stage-2 map = 84%,  reduce = 21%
2016-04-13 20:13:50,213 Stage-2 map = 89%,  reduce = 21%
2016-04-13 20:13:52,231 Stage-2 map = 97%,  reduce = 25%
2016-04-13 20:13:55,257 Stage-2 map = 100%,  reduce = 28%
2016-04-13 20:13:58,282 Stage-2 map = 100%,  reduce = 30%
2016-04-13 20:14:13,394 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132011_0003
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132011_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132011_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132011_0004
2016-04-13 20:14:22,949 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:14:31,993 Stage-3 map = 31%,  reduce = 0%
2016-04-13 20:14:35,010 Stage-3 map = 50%,  reduce = 0%
2016-04-13 20:14:47,080 Stage-3 map = 85%,  reduce = 0%
2016-04-13 20:14:50,101 Stage-3 map = 100%,  reduce = 0%
2016-04-13 20:14:53,123 Stage-3 map = 100%,  reduce = 25%
2016-04-13 20:15:02,183 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132011_0004
Loading data to table q2_minimum_cost_supplier_tmp1
6351 Rows loaded to q2_minimum_cost_supplier_tmp1
OK
Time taken: 183.376 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132011_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132011_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132011_0005
2016-04-13 20:15:10,822 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:15:13,839 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:15:22,890 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132011_0005
Loading data to table q2_minimum_cost_supplier_tmp2
4667 Rows loaded to q2_minimum_cost_supplier_tmp2
OK
Time taken: 20.742 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132011_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132011_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132011_0006
2016-04-13 20:15:31,565 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:15:34,582 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:15:43,632 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132011_0006
Launching Job 2 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132011_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132011_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132011_0007
2016-04-13 20:15:53,079 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:15:56,094 Stage-2 map = 100%,  reduce = 0%
2016-04-13 20:16:05,143 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132011_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132011_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132011_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132011_0008
2016-04-13 20:16:13,590 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:16:16,606 Stage-3 map = 100%,  reduce = 0%
2016-04-13 20:16:25,654 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132011_0008
Loading data to table q2_minimum_cost_supplier
100 Rows loaded to q2_minimum_cost_supplier
OK
Time taken: 62.69 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132016_544982907.txt
OK
Time taken: 3.532 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.049 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q2_minimum_cost_supplier. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132016_468419070.txt
OK
Time taken: 2.963 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.596 seconds
OK
Time taken: 0.073 seconds
OK
Time taken: 0.164 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.083 seconds
OK
Time taken: 0.075 seconds
OK
Time taken: 0.04 seconds
OK
Time taken: 0.05 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132016_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132016_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132016_0001
2016-04-13 20:17:08,421 Stage-4 map = 0%,  reduce = 0%
2016-04-13 20:17:11,458 Stage-4 map = 50%,  reduce = 0%
2016-04-13 20:17:14,499 Stage-4 map = 100%,  reduce = 0%
2016-04-13 20:17:20,569 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604132016_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132016_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132016_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132016_0002
2016-04-13 20:17:29,653 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:17:32,676 Stage-1 map = 50%,  reduce = 0%
2016-04-13 20:17:35,704 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:17:41,756 Stage-1 map = 100%,  reduce = 33%
2016-04-13 20:17:44,787 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132016_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132016_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132016_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132016_0003
2016-04-13 20:17:53,418 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:18:02,470 Stage-2 map = 15%,  reduce = 0%
2016-04-13 20:18:05,494 Stage-2 map = 21%,  reduce = 0%
2016-04-13 20:18:14,571 Stage-2 map = 21%,  reduce = 4%
2016-04-13 20:18:17,601 Stage-2 map = 36%,  reduce = 7%
2016-04-13 20:18:20,632 Stage-2 map = 42%,  reduce = 7%
2016-04-13 20:18:29,713 Stage-2 map = 58%,  reduce = 11%
2016-04-13 20:18:32,745 Stage-2 map = 63%,  reduce = 14%
2016-04-13 20:18:35,777 Stage-2 map = 63%,  reduce = 18%
2016-04-13 20:18:38,808 Stage-2 map = 63%,  reduce = 21%
2016-04-13 20:18:41,837 Stage-2 map = 79%,  reduce = 21%
2016-04-13 20:18:44,864 Stage-2 map = 84%,  reduce = 21%
2016-04-13 20:18:47,891 Stage-2 map = 89%,  reduce = 21%
2016-04-13 20:18:50,917 Stage-2 map = 89%,  reduce = 25%
2016-04-13 20:18:53,944 Stage-2 map = 96%,  reduce = 30%
2016-04-13 20:18:56,973 Stage-2 map = 100%,  reduce = 30%
2016-04-13 20:19:06,040 Stage-2 map = 100%,  reduce = 31%
2016-04-13 20:19:09,066 Stage-2 map = 100%,  reduce = 68%
2016-04-13 20:19:12,092 Stage-2 map = 100%,  reduce = 88%
2016-04-13 20:19:15,117 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132016_0003
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132016_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132016_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132016_0004
2016-04-13 20:19:23,688 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:19:32,732 Stage-3 map = 33%,  reduce = 0%
2016-04-13 20:19:35,748 Stage-3 map = 50%,  reduce = 0%
2016-04-13 20:19:47,818 Stage-3 map = 75%,  reduce = 0%
2016-04-13 20:19:50,840 Stage-3 map = 100%,  reduce = 0%
2016-04-13 20:19:53,861 Stage-3 map = 100%,  reduce = 25%
2016-04-13 20:20:02,918 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132016_0004
Loading data to table q2_minimum_cost_supplier_tmp1
6351 Rows loaded to q2_minimum_cost_supplier_tmp1
OK
Time taken: 182.93 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132016_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132016_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132016_0005
2016-04-13 20:20:11,590 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:20:14,607 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:20:23,659 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132016_0005
Loading data to table q2_minimum_cost_supplier_tmp2
4667 Rows loaded to q2_minimum_cost_supplier_tmp2
OK
Time taken: 20.727 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132016_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132016_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132016_0006
2016-04-13 20:20:33,318 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:20:36,336 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:20:45,388 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132016_0006
Launching Job 2 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132016_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132016_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132016_0007
2016-04-13 20:20:53,940 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:20:56,956 Stage-2 map = 100%,  reduce = 0%
2016-04-13 20:21:06,004 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132016_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132016_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132016_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132016_0008
2016-04-13 20:21:15,430 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:21:18,446 Stage-3 map = 50%,  reduce = 0%
2016-04-13 20:21:21,465 Stage-3 map = 100%,  reduce = 0%
2016-04-13 20:21:27,498 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132016_0008
Loading data to table q2_minimum_cost_supplier
100 Rows loaded to q2_minimum_cost_supplier
OK
Time taken: 63.815 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132021_268669608.txt
OK
Time taken: 3.419 seconds
OK
Time taken: 0.056 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.05 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q2_minimum_cost_supplier. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132022_99612348.txt
OK
Time taken: 2.99 seconds
OK
Time taken: 0.009 seconds
OK
Time taken: 0.01 seconds
OK
Time taken: 0.012 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.011 seconds
OK
Time taken: 0.577 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.19 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.034 seconds
OK
Time taken: 0.049 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.076 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.034 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132021_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132021_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132021_0001
2016-04-13 20:22:10,815 Stage-4 map = 0%,  reduce = 0%
2016-04-13 20:22:13,850 Stage-4 map = 50%,  reduce = 0%
2016-04-13 20:22:16,894 Stage-4 map = 100%,  reduce = 0%
2016-04-13 20:22:22,964 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604132021_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132021_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132021_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132021_0002
2016-04-13 20:22:32,059 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:22:35,081 Stage-1 map = 50%,  reduce = 0%
2016-04-13 20:22:38,110 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:22:44,162 Stage-1 map = 100%,  reduce = 33%
2016-04-13 20:22:47,193 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132021_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132021_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132021_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132021_0003
2016-04-13 20:22:55,783 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:23:04,833 Stage-2 map = 15%,  reduce = 0%
2016-04-13 20:23:07,856 Stage-2 map = 21%,  reduce = 0%
2016-04-13 20:23:16,932 Stage-2 map = 25%,  reduce = 4%
2016-04-13 20:23:19,960 Stage-2 map = 37%,  reduce = 7%
2016-04-13 20:23:22,991 Stage-2 map = 42%,  reduce = 7%
2016-04-13 20:23:32,070 Stage-2 map = 57%,  reduce = 11%
2016-04-13 20:23:35,102 Stage-2 map = 63%,  reduce = 14%
2016-04-13 20:23:38,133 Stage-2 map = 63%,  reduce = 18%
2016-04-13 20:23:41,164 Stage-2 map = 63%,  reduce = 21%
2016-04-13 20:23:44,194 Stage-2 map = 79%,  reduce = 21%
2016-04-13 20:23:47,221 Stage-2 map = 84%,  reduce = 21%
2016-04-13 20:23:50,247 Stage-2 map = 89%,  reduce = 21%
2016-04-13 20:23:53,273 Stage-2 map = 89%,  reduce = 25%
2016-04-13 20:23:56,299 Stage-2 map = 97%,  reduce = 30%
2016-04-13 20:23:59,325 Stage-2 map = 100%,  reduce = 30%
2016-04-13 20:24:08,392 Stage-2 map = 100%,  reduce = 32%
2016-04-13 20:24:13,433 Stage-2 map = 100%,  reduce = 51%
2016-04-13 20:24:14,445 Stage-2 map = 100%,  reduce = 77%
2016-04-13 20:24:16,467 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132021_0003
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132021_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132021_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132021_0004
2016-04-13 20:24:26,011 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:24:35,055 Stage-3 map = 33%,  reduce = 0%
2016-04-13 20:24:38,071 Stage-3 map = 50%,  reduce = 0%
2016-04-13 20:24:50,143 Stage-3 map = 86%,  reduce = 0%
2016-04-13 20:24:53,165 Stage-3 map = 100%,  reduce = 25%
2016-04-13 20:25:02,227 Stage-3 map = 100%,  reduce = 95%
2016-04-13 20:25:05,251 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132021_0004
Loading data to table q2_minimum_cost_supplier_tmp1
6351 Rows loaded to q2_minimum_cost_supplier_tmp1
OK
Time taken: 183.399 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132021_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132021_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132021_0005
2016-04-13 20:25:13,887 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:25:16,903 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:25:25,954 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132021_0005
Loading data to table q2_minimum_cost_supplier_tmp2
4667 Rows loaded to q2_minimum_cost_supplier_tmp2
OK
Time taken: 20.7 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132021_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132021_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132021_0006
2016-04-13 20:25:34,631 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:25:37,648 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:25:46,699 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132021_0006
Launching Job 2 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132021_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132021_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132021_0007
2016-04-13 20:25:56,150 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:25:59,167 Stage-2 map = 100%,  reduce = 0%
2016-04-13 20:26:08,217 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132021_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132021_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132021_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132021_0008
2016-04-13 20:26:16,661 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:26:19,677 Stage-3 map = 100%,  reduce = 0%
2016-04-13 20:26:28,724 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132021_0008
Loading data to table q2_minimum_cost_supplier
100 Rows loaded to q2_minimum_cost_supplier
OK
Time taken: 62.706 seconds
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132026_2098056259.txt
OK
Time taken: 3.49 seconds
OK
Time taken: 0.065 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.058 seconds
OK
Time taken: 0.05 seconds
FAILED: Error in metadata: MetaException(message:Got exception: org.apache.hadoop.ipc.RemoteException org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/hive/warehouse/q2_minimum_cost_supplier. Name node is in safe mode.
The ratio of reported blocks 1.0000 has reached the threshold 0.9990. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1700)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1680)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.delete(NameNode.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
Hive history file=/tmp/hadoop/hive_job_log_hadoop_201604132027_2082140209.txt
OK
Time taken: 2.962 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.008 seconds
OK
Time taken: 0.007 seconds
OK
Time taken: 0.576 seconds
OK
Time taken: 0.064 seconds
OK
Time taken: 0.183 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.033 seconds
OK
Time taken: 0.042 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.067 seconds
OK
Time taken: 0.041 seconds
OK
Time taken: 0.041 seconds
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132026_0001, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132026_0001
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132026_0001
2016-04-13 20:27:11,545 Stage-4 map = 0%,  reduce = 0%
2016-04-13 20:27:14,584 Stage-4 map = 50%,  reduce = 0%
2016-04-13 20:27:17,626 Stage-4 map = 100%,  reduce = 0%
2016-04-13 20:27:23,699 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201604132026_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132026_0002, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132026_0002
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132026_0002
2016-04-13 20:27:32,539 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:27:35,562 Stage-1 map = 50%,  reduce = 0%
2016-04-13 20:27:38,592 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:27:44,648 Stage-1 map = 100%,  reduce = 33%
2016-04-13 20:27:47,679 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132026_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132026_0003, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132026_0003
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132026_0003
2016-04-13 20:27:56,286 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:28:05,337 Stage-2 map = 15%,  reduce = 0%
2016-04-13 20:28:08,360 Stage-2 map = 21%,  reduce = 0%
2016-04-13 20:28:17,436 Stage-2 map = 25%,  reduce = 4%
2016-04-13 20:28:20,466 Stage-2 map = 37%,  reduce = 7%
2016-04-13 20:28:23,497 Stage-2 map = 42%,  reduce = 7%
2016-04-13 20:28:26,529 Stage-2 map = 42%,  reduce = 8%
2016-04-13 20:28:29,560 Stage-2 map = 47%,  reduce = 9%
2016-04-13 20:28:32,589 Stage-2 map = 60%,  reduce = 11%
2016-04-13 20:28:35,619 Stage-2 map = 63%,  reduce = 14%
2016-04-13 20:28:38,649 Stage-2 map = 68%,  reduce = 18%
2016-04-13 20:28:41,679 Stage-2 map = 68%,  reduce = 21%
2016-04-13 20:28:44,704 Stage-2 map = 80%,  reduce = 21%
2016-04-13 20:28:47,730 Stage-2 map = 88%,  reduce = 22%
2016-04-13 20:28:50,757 Stage-2 map = 89%,  reduce = 23%
2016-04-13 20:28:53,783 Stage-2 map = 95%,  reduce = 26%
2016-04-13 20:28:56,811 Stage-2 map = 99%,  reduce = 28%
2016-04-13 20:28:59,837 Stage-2 map = 100%,  reduce = 31%
2016-04-13 20:29:02,864 Stage-2 map = 100%,  reduce = 32%
2016-04-13 20:29:08,911 Stage-2 map = 100%,  reduce = 56%
2016-04-13 20:29:11,937 Stage-2 map = 100%,  reduce = 88%
2016-04-13 20:29:14,966 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132026_0003
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132026_0004, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132026_0004
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132026_0004
2016-04-13 20:29:23,519 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:29:32,564 Stage-3 map = 32%,  reduce = 0%
2016-04-13 20:29:35,580 Stage-3 map = 50%,  reduce = 0%
2016-04-13 20:29:47,649 Stage-3 map = 75%,  reduce = 0%
2016-04-13 20:29:50,672 Stage-3 map = 100%,  reduce = 0%
2016-04-13 20:29:53,693 Stage-3 map = 100%,  reduce = 25%
2016-04-13 20:30:02,752 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132026_0004
Loading data to table q2_minimum_cost_supplier_tmp1
6351 Rows loaded to q2_minimum_cost_supplier_tmp1
OK
Time taken: 180.14 seconds
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132026_0005, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132026_0005
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132026_0005
2016-04-13 20:30:11,406 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:30:14,423 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:30:23,474 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132026_0005
Loading data to table q2_minimum_cost_supplier_tmp2
4667 Rows loaded to q2_minimum_cost_supplier_tmp2
OK
Time taken: 20.709 seconds
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132026_0006, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132026_0006
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132026_0006
2016-04-13 20:30:33,125 Stage-1 map = 0%,  reduce = 0%
2016-04-13 20:30:36,142 Stage-1 map = 100%,  reduce = 0%
2016-04-13 20:30:45,193 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201604132026_0006
Launching Job 2 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132026_0007, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132026_0007
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132026_0007
2016-04-13 20:30:53,647 Stage-2 map = 0%,  reduce = 0%
2016-04-13 20:30:56,664 Stage-2 map = 100%,  reduce = 0%
2016-04-13 20:31:05,714 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201604132026_0007
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201604132026_0008, Tracking URL = http://belona.c3local:50030/jobdetails.jsp?jobid=job_201604132026_0008
Kill Command = /home/hadoop/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=belona.c3local:9001 -kill job_201604132026_0008
2016-04-13 20:31:15,139 Stage-3 map = 0%,  reduce = 0%
2016-04-13 20:31:18,155 Stage-3 map = 100%,  reduce = 0%
2016-04-13 20:31:27,205 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201604132026_0008
Loading data to table q2_minimum_cost_supplier
100 Rows loaded to q2_minimum_cost_supplier
OK
Time taken: 63.69 seconds
